{"./":{"url":"./","title":"目录","keywords":"","body":"目录 目录 工具 Gitbook 安装及使用 Markdown中使用HTML标签 Mac下chrome快捷键 Mac安装mongodb mac上redis环境搭建 Docker与Kubernetes在Mac本地环境搭建与应用部署 Docker使用 centos7下yum安装和配置Nginx 使用kubectl连接远程k8s集群 MacOS上安装DockerDesktop和Kubernetes alinode使用 iPic-Markdown图床、文件上传工具 图片搬家神奇：iPic Mover ffmpeg音视频处理 ffmpeg音视频处理需求升级版 七牛云存储中没有外链域名，无法下载的问题 基于WebAssembly的H265播放 ffprobe与ffplay与ffmpeg常用的命令 使用TravisCI构建Gitbook 持续集成和travisCI Nginx基础 Mac下安装nginx Mac上安装Elasticsearch，基本操作 学习笔记一 React React 总结 React 高级开发 umi-request 返回拦截处理 在隔离中开发组件 React 滚动加载实现 基于ReactCSSTransitionGroup 实现 react-router 过渡动画 文本转换为 markdown 实现 react 项目中 import 文件路径优化 React 组件开发指南 react 通过脚手架创建项目 componentWillReceiveProps 的使用 ant-design-pro-layout 底部 Footer 更改 React 生命周期 React-routerv4 路由配置方法 服务端渲染 SSR href属性使用js代码段在React下warning 问题 图表 三大图标库:ECharts、BizCharts 和 G2，该如何选择 three.js 在 React 中的运用 浏览器 一个 TCP 连接可以发送多少个 HTTP 请求 使用 http-proxy-middleware 代理跨域 跨域方式实现原理 常见 HTTP 请求头，响应头，实体 CORS 跨域详解 SEO小技巧 URL的最大长度是多少？ 深入理解Session和Cookie HTTP切面流程 Node与浏览器之间的区别 JavaScript如何在浏览器和Node中工作 Webpack Webpack5 更新日志 Webpack 中实现静态资源内联 CSS flex 布局 移动端适配 1px 的问题 Node npm 依赖管理之 peerDependencies 如何解决 npm unmet peer dependency 如何发布一个npm包 Node版本管理工具 使用node-clinic快速定位性能问题 cluster：扩展你的node应用 如何保存价值上千万的Node源代码 6个Async、Await优于Promise的方面 npm安装包时的几个命令区别 Moment进入维护状态 package.json中的workspaces 项目开发中的规范 npm库：AJV，JSON模式验证 pm2深入学习 pm2的cluster模式与fork模式的区别 Node原生模块整理 koa-bodyparser中间件 koa-multer实现文件上传并自定义文件名和目录 egg体系 egg应用自定义4xx和5xx的方案 egg的Http请求 怎么开发一个像egg-init的脚手架 结合源码揭秘egg运行原理 脚手架之egg-init egg框架中的参数校验 egg的Controller最佳实践 egg基于egg-validate的定制化升级 egg中cookie与Session的使用笔记 egg-bin源码解析笔记 egg框架与nest框架的对比 egg常见问题笔记 VScode调试Egg Make命令教程 架构 BFF 前端应用层框架 前端 UI 组件库 跨平台 工程智能化 WebAssembly完全入门了解wasm的前世今生 Next与create-react-app 应用程序性能更高 基于NodeWeb框架Chair 淘宝网的后台架构发展 应用场景数据库选型 Git发布部署相关 Mac下配置多个Git账户 删除远程仓库的某次错误提交 使用Github Actions进行版本发布 使用Github Actions进行版本发布(一) Git subtree教程 git分支开发规范 Git规范和Changelog生成 语义化Semantic规范格式 使用AppVeyor和Travis，自动构建和发布Electron应用 跨平台桌面应用Electron 企业级分布式，EDAS模式 Gitlab使用CICD管道配置参考 Linux Linux上部署Node服务-外网无法访问 服务器中启动服务的时候的IP选择 怎样修改CentOS 7 SSH端口 shell入门 设计模式 javaScript设计模式笔记一 javaScript设计模式笔记二 javaScript设计模式笔记三 javaScript设计模式笔记四 javascript设计模式笔记五 javaScript设计模式笔记六 javaScript设计模式笔记七 javaScript设计模式笔记八 javaScript设计模式笔记九 数据库 ORM框架选型 Knexjs笔记 bookshelf笔记 MySQL一：delete和truncate的区别和联系 MySQL二：having与where MySQL三：pluck MySQL四：first MySQL五：references MySQL七：in和exists二者之间的区别和性能影响 TypeScript 基础笔记一 基础笔记二 函数笔记 泛型笔记 枚举笔记 类型兼容性笔记 高级类型笔记 .d.ts和源文件.ts有什么区别? 使用TypeScript装饰器装饰你的代码 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/":{"url":"Tools/","title":"工具","keywords":"","body":"前端工具的安装及使用 Gitbook 安装及使用 Markdown中使用HTML标签 Mac下chrome快捷键 Mac安装mongodb mac上redis环境搭建 Docker与Kubernetes在Mac本地环境搭建与应用部署 Docker使用 centos7下yum安装和配置Nginx 使用kubectl连接远程k8s集群 MacOS上安装DockerDesktop和Kubernetes alinode使用 iPic-Markdown图床、文件上传工具 图片搬家神奇：iPic Mover ffmpeg音视频处理 ffmpeg音视频处理需求升级版 七牛云存储中没有外链域名，无法下载的问题 基于WebAssembly的H265播放 ffprobe与ffplay与ffmpeg常用的命令 使用TravisCI构建Gitbook 持续集成和travisCI By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/GitBook.html":{"url":"Tools/GitBook.html","title":"Gitbook 安装及使用","keywords":"","body":"概述 GitBook是使用Github/Git和Markdown构建漂亮书籍的命令行工具。 GitBook 可以将宁的内容作为网站（可定制和可扩展）或电子书（PDF，ePub和Mobi）输出。 GitBook.com是使用GitBook格式创建和托管图书的在线平台。它提供托管，协作功能和易于使用的编辑器 一、GitBook 安装 本地安装（以Mac系统为例） 通过NPM安装， npm install gitbook-cli -g gitbook-cil 是GitBook的一个命令行工具。它将自动安装所需版本的GitBook来构建一本书。 安装历史版本命令 gitbook fetch beta 列举可以下载的版本 gitbook ls-remote GitBook服务搭建 初始化 gitbook init 构建 gitbook build 执行命令会在项目的目录下生成一个_book目录，里面的内容为静态站点的资源文件： Debugging 您可以使用选项--log=debug 和 --debug来获取更好的错误信息（使用堆栈跟踪器）。例如： gitbook build ./ --log=debug --debug 启动服务 gitbook serve 使用下面命令会运行一个web服务，预览书籍 二、GitBook命令 这里主要介绍一下常用的几个命令 生成指定gitbook的版本，本地没有会先下载 gitbook build --gitbook=2.0.0 列出本地所有gitbook版本 gitbook ls 列出远程所有gitbook版本 gitbook ls-remote 安装对应的gitbook版本 gitbook fetch 标签/版本号 更新到最新的版本 gitbook update 卸载对应的版本 gitbook uninstall 2.0.1 指定log的级别 gitbook build --log=degbug 输出错误信息 gitbook build --debug 三、GitBook 目录结构 GitBook 项目结构 GitBook使用简单的目录结构。在SUMMERY.md文件，中列出的所有Markdown文件将被转换为HTML。多语言书籍结构略有不同。 一个基本的GitBook电子书结构通常如下： . ├── book.json ├── README.md ├── SUMMARY.md ├── chapter-1/ | ├── README.md | └── something.md └── chapter-2/ ├── README.md └── something.md 文件说明： | 文件名 | 描述 | |---|---| | book.json | 配置数据（optional） | | README.md | 电子书的前沿或简介（required） | | SUMMERY.md | 电子书目录（optional） | | GLOSSARY.md | 词汇/注释语列表（optional） | 静态文件和图片 静态文件是在SUMMARY.md中未列出的文件。除非被忽略，否则所有静态文件都将复制到输出路径。 忽略文件或文件夹 GitBook将读取.gitignore, .bookignore和.ignore文件，一获取要过滤的文件和文件夹。这些文件夹中的格式遵循.gitignore的规则： # This is a comment # Ignore the file test.md test.md # Ignore everything in the directory \"bin\" bin/* 项目与子目录集成 对于软件项目，您可以使用子目录（如 /docs）来存储项目文档的图书。您可以配置根选项来指示GitBook可以找到该图书文件的文件夹： . ├── book.json └── docs/ ├── README.md └── SUMMARY.md 在 book.json中配置以下内容： { \"root\": \"./docs\" } 页面 MarkDown 语法 默认情况下，GitBook的大多数文件都使用Markdown语法 Glossary 允许您指定要显示为注释的术语及其各自的定义。根据这些术语，Gitbook将自动构建索引并突出显示这些术语。 四、GitBook 配置 常规设置 变量 描述 root 包含所有图书文件的根文件夹的路径，除了 book.json structure 指定自述文件，摘要，词汇表等的路径，参考 Structure paragraph. title 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 description 书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 author 作者名。在GitBook.com上，这个字段是预填的 isbn 国际标准书号 ISBN direction 文本阅读顺序。可以是 rtl （从右向左）或 ltr （从左向右），默认值依赖于 language 的值。 gitbook 应该使用的GitBook版本。使用 SemVer 规范，并接受类似于 “> = 3.0.0” 的条件。 links 在左侧导航栏添加链接信息 \"links\" : { \"sidebar\" : { \"Home\" : \"https://github.com/dunwu/gitbook-notes\" } } styles 自定义页面样式，默认情况下各generator对应的CSS文件 \"styles\": { \"website\": \"styles/website.css\", \"ebook\": \"styles/ebook.css\", \"pdf\": \"styles/pdf.css\", \"mobi\": \"styles/mobi.css\", \"epub\": \"styles/epub.css\" } plugins 插件及其配置在book.json中制定。有关详细信息。 自3.0.0版本开始，GitBook可以使用主题。有关详细信息，请参阅the theming section 变量 描述 plugins 要加载的插件列表 pluginsConfig 插件的配置 添加插件之后运行gitbook install来安装新的插件 去除自带插件 GitBook默认自带5个插件 highlight search sharing font-settings liverload \"plugins\": { \"-search\" } structure 除了root属性之外，您可以指定Readme，Summary，Glossary和Language的名称（而不是使用默认名称，如README.md）。这些文件必须在项目的根目录下（或root根目录，如果你在book.json中配置了root属性）。不接受的路径，如dir/MY_README.md 变量 描述 structure.readme Readme文件名（默认值是README.md） structure.summary Summary文件名(默认值是SUMMARY.md) structure.glossary Glossary文件名（默认值是GLOSSARY.md） structure.language LANGUAGE文件名(默认值是LANGUAGE.md) PDF 可以使用book.json中的一组选项来定制PDF输出： | 变量 | 描述 | |---|---| | pdf.pageNumbers | 将页码添加到每个页面的底部（默认为true） | | pdf.fontSize | 基本字体大小（默认是12） | | pdf.fontFamiliy | 基本字体样式（默认是Arial） | | pdf.pageSize | 页面尺寸，选项有（'a0', 'a1', 'a2', 'a3','a4', 'a5', 'a6', 'b0', 'b1', 'b2', 'b3', 'b4', '5', 'b6', 'legal', 'letter'）默认值是a4 | | pdf.margin.top | 上边界（默认值是56） | | pdf.margin.bottom | 下边界（默认值是56） | | pdf.margin.right | 右边界（默认值是62） | | pdf.margin.left | 左边界（默认值是62） | 五、生成电子书 GitBook可以生成一个网站，但也可以输出内容过作为电子书（ePub，Mobi,PDF） # Generate a PDF file $ gitbook pdf ./ ./mybook.pdf # Generate an ePub file $ gitbook epub ./ ./mybook.epub # Generate a Mobi file $ gitbook mobi ./ ./mybook.mobi 安装ebook-convert ebook-convert 可以用来生成电子书（epub, mobi, pdf） OSX 下载 Calibre application。将 calibre.app 移动到应用程序文件夹后，创建一个符号链接到eBook-convert工具： sudo ln -s ~/Applications/calibre.app/Contents/MacOS/ebook-convert /usr/bin 你可以使用$PATH中的任何目录替换 /usr/bin 封面 封面用于所有电子书格式，你可以自己提供一个，也可以使用autocover plugin将指定一个较小版本的封面。封面应为JPEG文件。 好的封面应该遵守以下准则： cover.jpg的尺寸为18002360像素，cover_small.jpg 为200262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 GitBook 部署 托管到 gitbook.com GitBook.com是使用GitBook格式创建和托管图书的在线平台。它提供托管，协作功能和抑郁使用的编辑器。 创建组织 设置组织 在组织中新建space 注：免费的GitBook一个组织下只能新建一个space 点击新建好的space->INTEGRATIONS 这里我用的GitHub(在GitHub上选择已经新建好的仓库关联) 只要你指定的GitHub仓库中的文档内容符合GitBook规范，GitBook就会自动根据你的梅西更新区构建生成电子书网站。默认访问地址是 https://新建的组织名.gitbook.io/space名/ 例如：我刚刚新建的组织名为test-coco，space名为test，则访问的路径是 https://test-coco.gitbook.io/test/ 托管到GitHub Pages 也许你以前也了解GitHub的一个功能：GitHub Pages。它允许用户在GitHub仓库托管你的个人、组织或项目的静态页面（自动识别html、css、javascript）。 建立xxx.github.io仓库 要使用这个特性，首先，你必须建立一个严格遵循以下命名要求的仓库：**GitHub用户名.github.io举例，我的GitHub账号CoCoyh，则这个仓库应该叫CoCoyh.github.io。通常，这个仓库被用来作为个人或组织的博客。 建立gh-pages分支 完成第一步后，在任意一个GitHub仓库中建立一个名为gh-pages 的分支。只要gh-pages中的内容符合一个静态站点要求，就可以在如下地址中进行访问：“https://Github用户名.github.io/GitHub 仓库”。例如我的一个GitHub仓库名为notes，则访问路径是\"https://CoCoyh.github.io/notes\" 自动发布到gh-pages 如果每次都手动git push 到远程分支，略有点麻烦。 如果你了解Nodejs，那么简单的发布方式就是使用gh-pages 插件 现在本地安装插件 npm i -D gh-pages 然后，在package.json文件中添加脚本命令： 如下：-d 命令参数 后面是要发布的静态站点内容目录 \"scripts\": { \"deploy\": \"gh-pages -d build\" }, 命令 git add . git commit -am \"Update\" git push git@github.com:CoCoyh/notes.git gh-pages --force\" By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Markdown中使用HTML标签.html":{"url":"Tools/Markdown中使用HTML标签.html","title":"Markdown中使用HTML标签","keywords":"","body":"Markdown中使用HTML标签 在Markdown中写html标签文本 这是一个标题 会被直接解析为字符串，标签不显示， 原因 网站为避免XSS攻击，例如React中无法在jsx中直接插入HTML（innerHTML） // state { html: 'Markdown' } //render {this.state.html} 编译后放在页面时： Markdown 如果是网站本身的在JSX中直接插入HTML，使用的是dangerouslySetInnerHTML，操作Markdown 这是一个标题\" }> // 页面显示为 这是一个标题 解决办法： 方法一：用斜杠引号 `这是一个标题` 方法二：对标签进行转义 &lt;h1&gt;这是一个标题&lt;/h1&gt; By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Mac下chrome快捷键.html":{"url":"Tools/Mac下chrome快捷键.html","title":"Mac下chrome快捷键","keywords":"","body":"Mac 下 chrome 快捷键【转载】 浏览器的刷新快捷键 F5，强制刷新 Ctrl+F5 Mac 系统下浏览器的刷新快捷键 command+R, 强制刷新快捷键为 command+shift+R 一、标签页和窗口快捷键 快捷键 功能描述 ⌘-N 打开新窗口。 ⌘-T 打开新标签页 ⌘-Shift-N 在隐身模式下打开新窗口。 按 ⌘-O，然后选择文件。 在 Google Chrome 浏览器中打开计算机中的文件。 按住 ⌘ 键，然后点击链接。或用鼠标中键（或鼠标滚轮）点击链接。 从后台在新标签页中打开链接。 按住 ⌘-Shift 键，然后点击链接。或按住 Shift 的同时用鼠标中键（或鼠标滚轮）点击链接。 在新标签页中打开链接并切换到刚打开的标签页。 按住 Shift 键，然后点击链接。 在新窗口中打开链接。 ⌘-Shift-T 重新打开上次关闭的标签页。Google Chrome 浏览器可记住最近关闭的 10 个标签页。 将标签页拖出标签栏。 在新窗口中打开标签页。 将标签页从标签栏拖到现有窗口中。 在现有窗口中打开标签页。 同时按 ⌘-Option 和向右箭头键。 切换到下一个标签页。 同时按 ⌘-Option 和向左箭头键。 切换到上一个标签页。 ⌘-W 关闭当前标签页或弹出窗口。 ⌘-Shift-W 关闭当前窗口。 点击并按住浏览器工具栏中的后退或前进箭头。 在新标签页中显示浏览历史记录。 按 Delete 或 ⌘-[ 转到当前标签页的上一页浏览历史记录。 按 Shift-Delete 或 ⌘-] 转到当前标签页的下一页浏览历史记录。 按住 Shift，然后点击窗口左上方的 + 按钮。 最大化窗口。 ⌘-M 最小化窗口。 ⌘-H 隐藏 Chrome 浏览器。 ⌘-Option-H 隐藏其他所有窗口。 ⌘-Q 关闭 Google Chrome 浏览器。 二、Google Chrome 浏览器功能快捷键 快捷键 功能描述 ⌘-Shift-B 打开和关闭书签栏。 ⌘-Option-B 打开书签管理器。 ⌘-, 打开“偏好设置”对话框。 ⌘-Y 打开“历史记录”页面。 ⌘-Shift-J 打开“下载内容”页面。 ⌘-Shift-Delete 打开“清除浏览数据”对话框。 ⌘-Shift-M 在多个用户之间切换。 三、地址栏快捷键 快捷键 功能描述 输入搜索字词，然后按 Enter。 使用默认搜索引擎进行搜索。 输入搜索引擎关键字，按空格键，然后输入搜索字词，再按 Enter。 使用与关键字相关联的搜索引擎进行搜索。 首先输入搜索引擎网址，然后在系统提示时按 Tab，输入搜索字词，再按 Enter。 使用与网址相关联的搜索引擎进行搜索。 输入网址，然后按 ⌘-Enter。 在新后台标签页中打开网址。 ⌘-L 突出显示网址。 ⌘-Option-F 将“?”置于地址栏中。 在问号后输入搜索字词可用默认搜索引擎执行搜索。 同时按 Option 和向左箭头键。 将光标移到地址栏中的前一个关键字词 同时按 Option 和向右箭头键。 在地址栏中将光标移到下一个关键字词 同时按 Shift-Option 和向左箭头键。 在地址栏中突出显示上一关键字词 同时按 Shift-Option 和向右箭头键。 在地址栏中突出显示下一关键字词 ⌘-Delete 在地址栏中删除光标前的关键字词 用键盘上的方向键从地址栏下拉菜单中选择一个条目，然后按 Shift-Fn-Delete。 从浏览历史记录中删除所选条目（如果可以）。 在地址栏菜单中按 Page Up 或 Page Down。 在菜单中选择上一条目或下一条目。 四、网页快捷键 快捷键 功能描述 ⌘-P 打印当前网页。 ⌘-Shift-P 打开“网页设置”对话框。 ⌘-S 保存当前网页。 ⌘-Shift-I 通过电子邮件发送当前网页。 ⌘-R 重新加载当前网页。 ⌘-, 停止加载当前网页。 ⌘-F 打开查找栏。 ⌘-G 在查找栏中查找下一条与输入内容相匹配的内容。 ⌘-Shift-G 或 Shift-Enter 在查找栏中查找上一条与输入内容相匹配的内容。 ⌘-E 使用所选内容查找 ⌘-J 跳到所选内容 ⌘-Option-I 打开开发者工具。 ⌘-Option-J 打开“JavaScript 控制台”。 ⌘-Option-U 打开当前网页的源代码。 按住 Option，然后点击链接。 下载链接目标。 将链接拖到书签栏中。 将链接保存为书签。 ⌘-D 将当前网页保存为书签。 ⌘-Shift-D 将所有打开的标签页以书签的形式保存在新文件夹中。 ⌘-Shift-F 在全屏模式下打开网页。再按一次 ⌘-Shift-F 可退出全屏模式。 ⌘-+ 放大网页上的所有内容。 ⌘ 和 - 缩小网页上的所有内容。 ⌘-0 将网页上的所有内容都恢复到正常大小。 ⌘-Shift-H 在当前标签页中打开主页。 空格键 向下滚动网页。 ⌘-Option-F 搜索网页。 五、文本快捷键 | 快捷键 | 功能描述 | | --------------------------- | --------------------------------------------------------- | | ⌘-C | 将突出显示的内容复制到剪贴板中。| | ⌘-Option-C | 将您正在查看的网页的网址复制到剪贴板中。| | ⌘-V | 从剪贴板中粘贴内容。| | ⌘-Shift-Option-V | 仅粘贴内容，不带源格式。| | ⌘-X 或 Shift-Delete | 删除突出显示的内容并将其复制到剪贴板中。| | ⌘-Z | 撤消最后一步操作。| | ⌘-Shift-Z | 重复最后一步操作。| | ⌘-X | 删除突出显示的内容并将其保存到剪贴板中（剪切）。| | ⌘-A | 选择当前网页上的所有文本。| | ⌘-: | 打开“拼写和语法”对话框。| | ⌘-; | 检查当前网页上的拼写和语法 | By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Mac安装mongodb.html":{"url":"Tools/Mac安装mongodb.html","title":"Mac安装mongodb","keywords":"","body":"一. 更新Homebrew包 brew update 二. 安装Mongodb brew install mongodb 安装需要一些时间，默认安装在/usr/local/Cellar/mongodb/4.0.3_1(我的版本)目录下安装好了，还需要配置一下，否则是无法正常启动服务的 三. 配置mongodb 1. 创建一个db目录，用户mongodb写数据 mkdir -p /data/db 如果出现 permission denied，加上sudo命令： sudo mkdir -p /data/db 2. 给/data/db文件夹赋予权限 sudo chown id -u /data/db 如果出现“illegal user name“的错误提示，这时我们可以查看当前的username并赋予权限： $whoami username $ sudo chown username /data/db 3. 配置mongodb环境配置 打开.zshrc文件 vim ~/.zshrc 添加mongodb的安装目录到path中 export PATH = /usr/local/Cellar/mongodb/4.0.3_1/bin:${PATH} 是配置生效 source ~/.zshrc 修改mongodb配置文件，配置文件默认在/usr/local/etc下的mongod.conf： ``` dbpath = /data/db - 启动mongod服务 mognod ![mongod启动](https://upload-images.jianshu.io/upload_images/9403248-6911e7d0be70456e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 当出现上图红框中的命令时，就表明服务启动成功了，侦听端口为27017，这是mongod的默认端口，在另外的一个窗口中使用mongo就可以打开客户端： mongo ``` 这时候酒客输入数据库命令进行操作了们必入show dbs，可以查看当前的数据库集合 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/mac上redis环境搭建.html":{"url":"Tools/mac上redis环境搭建.html","title":"mac上redis环境搭建","keywords":"","body":"mac上redis环境搭建 下载 下载 解压 sudo tar -zxf redis-5.0.5.tar.gz 安装 cd redis-5.0.5 sudo make test sudo make install 启动服务 redis-server //运行redis服务 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Docker与Kubernetes在Mac本地环境搭建与应用部署.html":{"url":"Tools/Docker与Kubernetes在Mac本地环境搭建与应用部署.html","title":"Docker与Kubernetes在Mac本地环境搭建与应用部署","keywords":"","body":"Docker官网 Homebrew安装 macOS我们可以使用Homebrew来安装docker Homebrew 的 Cask 已经支持 Docker for Mac，因此可以很方便的使用 Homebrew Cask 来进行安装： # 安装命令 brew cask install docker ==> Satisfying dependencies ==> Downloading https://download.docker.com/mac/stable/28905/Docker.dmg ######################################################################## 100.0% ==> Verifying SHA-256 checksum for Cask 'docker'. ==> Installing Cask docker ==> Moving App 'Docker.app' to '/Applications/Docker.app'. docker was successfully installed! 手动安装 下载最新的Docker for Mac或者Edge版本，即可以看到内置的kubernets集群，直接点击安装即可 安装完毕后，如果页勾选了Show system containers选项，那么使用如下的Docker命令，能看到自动安装的kubernetes相关容器： ➜ ~ docker container ls --format \"table{{.Names}}\\t{{.Image }}\\t{{.Command}}\" NAMES IMAGE COMMAND k8s_compose_compose-75f8bb4779-stxv9_docker_3c963862-f9f4-11e7-93cc-025000000001_0 docker/kube-compose-controller \"/compose-controller…\" k8s_POD_compose-75f8bb4779-stxv9_docker_3c963862-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_sidecar_kube-dns-545bc4bfd4-799pr_kube-system_139bf000-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/k8s-dns-sidecar-amd64 \"/sidecar --v=2 --lo…\" k8s_dnsmasq_kube-dns-545bc4bfd4-799pr_kube-system_139bf000-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64 \"/dnsmasq-nanny -v=2…\" k8s_kubedns_kube-dns-545bc4bfd4-799pr_kube-system_139bf000-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/k8s-dns-kube-dns-amd64 \"/kube-dns --domain=…\" k8s_kube-proxy_kube-proxy-rrd8t_kube-system_139b00df-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/kube-proxy-amd64 \"/usr/local/bin/kube…\" k8s_POD_kube-dns-545bc4bfd4-799pr_kube-system_139bf000-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_POD_kube-proxy-rrd8t_kube-system_139b00df-f9f4-11e7-93cc-025000000001_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_kube-scheduler_kube-scheduler-docker-for-desktop_kube-system_972d74c9fc2f4ebd8ab673058e386a65_0 gcr.io/google_containers/kube-scheduler-amd64 \"kube-scheduler --ad…\" k8s_kube-apiserver_kube-apiserver-docker-for-desktop_kube-system_f7a81e8fe624bd46059fc6084e86bb81_0 gcr.io/google_containers/kube-apiserver-amd64 \"kube-apiserver --ad…\" k8s_etcd_etcd-docker-for-desktop_kube-system_56a21c0a5f545c0cca5388c457bb1b3b_0 gcr.io/google_containers/etcd-amd64 \"etcd --advertise-cl…\" k8s_kube-controller-manager_kube-controller-manager-docker-for-desktop_kube-system_8d1848c1e562e35a225e402988eadcd1_0 gcr.io/google_containers/kube-controller-manager-amd64 \"kube-controller-man…\" k8s_POD_kube-apiserver-docker-for-desktop_kube-system_f7a81e8fe624bd46059fc6084e86bb81_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_POD_kube-controller-manager-docker-for-desktop_kube-system_8d1848c1e562e35a225e402988eadcd1_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_POD_kube-scheduler-docker-for-desktop_kube-system_972d74c9fc2f4ebd8ab673058e386a65_0 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" k8s_POD_etcd-docker-for-desktop_kube-system_56a21c0a5f545c0cca5388c457bb1b3b_0 关于各个容器的作用，可以残月这里。在安装过程中，Docker页为我们安装了kubectl控制命令： $ kubectl get namespaces $ kubectl get posts --namespace kube-system 接下来我们可以使用kubectl命令来创建简单的kubernets-dashboard服务： ➜ ~ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created 服务安装完毕后可以查看部署的容器和服务： ➜ ~ kubectl get deployments --namespace kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 22m kubernetes-dashboard 1 1 1 0 26s ➜ ~ kubectl get services --namespace kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP 22m kubernetes-dashboard ClusterIP 10.111.242.95 443/TCP 30s 在Dashboard启动完毕后，可以使用kubectl提供的Proxy服务来访问该面板： $ kubectl proxy # 打开如下地址： # http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 如果访问出错，可以尝试kubernetes-dashboard服务，或者查阅这里 $ kubectl -n kube-system edit service kubernetes-dashboard # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 ... name: kubernetes-dashboard namespace: kube-system resourceVersion: \"343478\" selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard-head uid: 8e48f478-993d-11e7-87e0-901b0e532516 spec: clusterIP: 10.100.124.90 externalTrafficPolicy: Cluster ports: - port: 443 protocol: TCP targetPort: 8443 selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: ClusterIP ->> NodePort status: loadBalancer: {} 访问上述地址，我们可以看到登陆界面： 此时可暂时直接跳过，进入到控制面板中 Docker同样为我们提供了简单的应用示范，可以直接使用如下的Docker Compose配置文件： version: '3.3' services: web: build: web image: dockerdemos/lab-web volumes: - \"./web/static:/static\" ports: - \"80:80\" words: build: words image: dockerdemos/lab-words deploy: replicas: 5 endpoint_mode: dnsrr resources: limits: memory: 16M reservations: memory: 16M db: build: db image: dockerdemos/lab-db 然后使用stack命令创建应用栈： $ docker stack deploy --compose-file stack.yml demo Stack demo was created Waiting for the stack to be stable and running... - Service web has one container running 应用栈创建完毕后，可以使kubectl查看创建的Pods: $ kubectl get pods NAME READY STATUS RESTARTS AGE db-7f99cc64b7-cbd9t 1/1 Running 0 2m web-758c6998f8-tmxfm 1/1 Running 0 2m words-54bf6c5d57-8bxc8 1/1 Running 0 2m words-54bf6c5d57-dzxm8 1/1 Running 0 2m words-54bf6c5d57-k2448 1/1 Running 0 2m words-54bf6c5d57-mhh4p 1/1 Running 0 2m words-54bf6c5d57-w2q82 1/1 Running 0 2m 也可以来查看部署的集群与服务： $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE db 1 1 1 1 3m web 1 1 1 1 3m words 5 5 5 5 3m $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE db ClusterIP None 55555/TCP 3m kubernetes ClusterIP 10.96.0.1 443/TCP 52m web LoadBalancer 10.97.154.28 80:30577/TCP 3m words ClusterIP None 55555/TCP 3m 可以看到这里的web有所谓的LoadBalancer类型，机可以对外提供服务。最后我们可以用stack与kubectl名利ing来删除应用： $ docker stack remove demo $ kubectl delete deployment kubernetes-dashboard --namespace kube-system 镜像加速 鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们需要配置加速期来解决， 官网 网易 阿里云: 阿里云的镜像官方地址是：http://dev.aliyun.com/search.html， 开发这需要开通阿里开发者账户，登陆后https://cr.console.aliyun.com/undefined/instances/mirrors 中查看你的专属加速器地址 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Docker使用.html":{"url":"Tools/Docker使用.html","title":"Docker使用","keywords":"","body":"Docker 常用命令 查看版本信息 # 查看docker相关信息 docker info # 查看版本信息 docker -v 或者 docker --version # 查看Client和Server版本信息 docker version docker run命令 Docker 允许你在容器内运行应用程序，使用docker run命令来在容器内运行一个应用程序。 docker run ubuntu:15.10 /bin/echo \"Hello world\" 各个参数解析： docker：Docker的为紧致执行文件。 ubuntu:15.10指定运行的镜像，Docker首先从本地主机上查找镜像是否存在，如果不存在，Docker就会从镜像仓库Docker Hub下载公共镜像。 /bin/echo \"Hello world\"，然后输出结果。docker run -i -t ubuntu:15.10 /bin/bash 各个参数解析： -t: 在新容器内指定一个伪终端或终端 -i: 允许你对容器内的标准输入（STDIN）进行交互 -d: 让容器在后台运行 -P：将容器内部使用的网络端口映射到我们使用的主机上 可以通过-p参数来奢姿不一样的端口 docker 查看命令 # 查看当前运行的容器 docker ps # 查看所有容器，包括停止的 docker ps -a # 查看最新创建的容器，只列出最后创建的 docker ps -l # 查看网络端口 docker port [容器ID|容器名] # 查看容器内部的标准输出 docker logs [容器ID|容器名] # 查看容器内部运行的进程 docker top [容器ID|容器名] # 查看 Docker 的底层信息。它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息 docker inspect [容器ID|容器名] docker stop 命令 docker stop [容器ID|容器名] 启动命令 已经停止的容器 docker start [容器ID|容器名] 正在运行的容器， docker restart [容器ID|容器名] docker rm命令 # 删除容器，删除容器时，容器必须是停止状态 docker rm [容器ID|容器名] # 删除所有的容器 docker rm $(docker ps -aq) Docker 容器使用 Docker 客户端 直接输入docker命令来查看到Docker客户端的多有命令 通过命令 docker command --help 更深入的了解指定的 Docker 命令使用方法 Docker镜像使用 当运行容器时，使用的镜像如果在本地中不存在，docker 就会自动从 docker 镜像仓库中下载，默认是从 Docker Hub 公共镜像源下载 列出镜像列表 # 列出本地主机上的镜像 ~ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 15.10 9b9cb95443b5 2 years ago 137MB training/webapp latest 6fae60ef3446 3 years ago 349MB 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如ubuntu仓库源里，有15.10、14.04等多个不同的版本，我们使用 REPOSITORY:TAG 来定义不同的镜像 ~ docker run -t -i ubuntu:15.10 /bin/bash root@6939a46db5bb:/# 如果不指定一个镜像的版本标签，例如只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 获取一个新的镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用 docker pull 命令来下载它 查找镜像 # 搜索镜像 docker search 镜像名 资料 阿里云镜像仓库 https://juejin.im/post/5b62d0356fb9a04fb87767f5#heading-3 https://yeasy.gitbooks.io/docker_practice/image/pull.html By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/centos7下yum安装和配置Nginx.html":{"url":"Tools/centos7下yum安装和配置Nginx.html","title":"centos7下yum安装和配置Nginx","keywords":"","body":"前言 Nginx (engine x) 是一个高性能的 HTTP 和反向代理服务器，也是一个 IMAP/POP3/SMTP 服务器。。 本例演示 CentOS 7 下安装和配置 Nginx 的基本步骤。 环境 查看Linux环境 cat /etc/redhat-release CentOS Linux release 7.6.1810 (Core) 步骤 1. 添加yum源 Nginx不在默认的yum源中，可以使用epel或者官网的yum源，本例使用官网的yum源。 rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm Retrieving http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm warning: /var/tmp/rpm-tmp.SsRR7r: Header V4 RSA/SHA1 Signature, key ID 7bd9bf62: NOKEY Preparing... ################################# [100%] Updating / installing... 1:nginx-release-centos-7-0.el7.ngx ################################# [100%] 安装yum源之后，可以查看下。 $ yum repolist Loaded plugins: fastestmirror Repository nodesource is listed more than once in the configuration Repository nodesource-source is listed more than once in the configuration Loading mirror speeds from cached hostfile nginx | 2.9 kB 00:00 nginx/x86_64/primary_db | 46 kB 00:02 repo id repo name status base/7/x86_64 CentOS-7 - Base 10,019 epel/x86_64 Extra Packages for Enterprise Linux 7 - x86_64 13,338 extras/7/x86_64 CentOS-7 - Extras 435 nginx/x86_64 nginx repo 152 nodesource/x86_64 Node.js Packages for Enterprise Linux 7 - x86_64 159 updates/7/x86_64 CentOS-7 - Updates 2,500 repolist: 26,603 可以发现nginx repo已经安装到本机了。 步骤二：安装 yum 安装Nginx，非常简单，一条命令 yum install nginx 步骤三：配置Nginx服务 设置开机启动 systemctl enable nginx 启动服务 systemctl start nginx 停止服务 systemctl restart nginx 重新加载，因为一般重新配置之后，不希望重启服务，这时可以使用重新加载。 systemctl reload nginx 步骤四：打开防火墙端口 默认Centos使用的防火墙firewalld是关闭http服务的（打开80端口） $ sudo firewall-cmd --zone=public --permanent --add-service=http success $ sudo firewall-cmd --reload success 打开之后，可以查看一下防火墙打开的所有服务 firewall-cmd --list-service 可以看到系统已经打开了http服务 反向代理 Nginx 是一个很方便的反向代理，配置反向代理可以参考Module ngx_http_proxy_module。本文不做赘述 需要指出的是 CentOS 7 的 SELinux，使用反向代理需要打开网络访问权限。 setsebool httpd_can_network_connect 1 绑定其他端口 Nginx 默认绑定的端口是 http 协议的默认端口，端口号为：80，如果需要绑定其他端口，需要注意 SELinux 的配置 例如：绑定 8081 端口，但是会发现无法启动，一般的报错如下 YYYY/MM/DD hh:mm:ss [emerg] 46123#0: bind() to 0.0.0.0:8081 failed (13: Permission denied) 此时需要更改 SELinux 的设置。我们使用 SELinux 的管理工具 semanage 进行操作，比较方便。 安装 semanage 使用如下命令 yum install policycoreutils-python 然后查看是否有其他协议类型使用了此端口 semanage port -l | grep 8081 transproxy_port_t tcp 8081 返回了结果，表明已经被其他类型占用了，类型为 transproxy_port_t。 我们还要查看一下 Nginx 的在 SELinux 中的类型 http_port_t 绑定的端口 $ sudo semanage port -l | grep http_port_t http_port_t tcp 80, 81, 443, 488, 8008, 8009, 8443, 9000 pegasus_http_port_t tcp 5988 第一行 http_port_t 中没有包含 8081 这个端口。因此需要修改 8081 端口到 http_port_t 类型中。 $ sudo semanage port -m -p tcp -t http_port_t 8081 如果没有其他协议类型使用想要绑定的端口，如 8001，则我们只要新增到 SELinux 中即可。 $ sudo semanage port -l | grep 8001 $ sudo semanage port -a -p tcp -t http_port_t 8001 此时，重新启动 Nginx 即可。 资料 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/使用kubectl连接远程k8s集群.html":{"url":"Tools/使用kubectl连接远程k8s集群.html","title":"使用kubectl连接远程k8s集群","keywords":"","body":"什么是kubectl kubectl是kubernetes的客户顿啊程序 是用于运行kubernetes集群命令的管理工具 提供了大量子命令可以让用户和集群进行交互 不一定部署在master上，用户可以通过kubectl 连接到master上 安装 macOS下直接使用homebrew 管理工具进行安装 brew install kubernetes-cli 确认是否安装成功： kubectl version 基本语法 kubectl [command] [TYPE] [NAME] [flags] 配置kubectl # 配置集群名称与服务地址 kubectl config --kubeconfig=${HOME}/.kube/config set-cluster cluster-name --server=https://{IP} --inspecure-skip-tls-verify # 设置一个管理用户为admin，并设置访问凭证。此处使用用户名和密码的验证方式 kubectl config --kubeconfig=${HOME}/.kube/config set-credentials admin --username=username --password=pwd # 设置一个名为admin的配置，使用cluster-name集群与admin 用户的上下文 kubectl config --kubeconfig=${HOME}/.kube/config set-context admin --cluster-cluster-name --namespace=test --user=admin # 启用admin为默认上下文 kubectl config --kubeconfig=${HOME}/.kube/config use-context admin 预览配置 $ kubectl config view apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: https://{IP} name: cluster-name contexts: - context: cluster: cluster-name namespace: test user: admin name: admin current-context: admin kind: Config preferences: {} users: - name: admin user: password: pwd username: username 验证配置 $ kubectl cluster-info Kubernetes master is running at https://{IP} GLBCDefaultBackend is running at https://{IP}/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy Heapster is running at https://{IP}/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://{IP}/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://{IP}/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy Metrics-server is running at https://{IP}/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy 查看集群节点： $ kubectl get node NAME STATUS ROLES AGE VERSION gke-ci-cluster-1-default-pool-a7b52541-075v Ready 3d v1.9.7-gke.5 若以上输出正常，证明连接成功 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/MacOS上安装DockerDesktop和Kubernetes.html":{"url":"Tools/MacOS上安装DockerDesktop和Kubernetes.html","title":"MacOS上安装DockerDesktop和Kubernetes","keywords":"","body":"MacOS上安装Docker Desktop和Kubernetes 相关链接： https://www.docker.com/products/docker-desktop https://kitematic.com/ https://github.com/docker/kitematic/releases 安装Docker Desktop 首先要做的就是下载Docker Desktop.dmg文件，将该文件保存到硬盘后，打开Finder并导航到包含下载文件夹，找到并双击Docker.dmg文件。DO乘客人窗口代开后，点击Docker图标并将其拖动到应用程序文件夹 完成Docker Desktop的安装后，你将在顶部面板上看到一个新图标。点击该图标以显示Docker Desktop菜单。 现在你完成准备可继续安装更多组件 安装Kitematic 对于那些不想完全依赖命令行的人来说，有Kitematic GUI共使用。在安装Kitematic之前必须安装Git。要安装Git，需先下载安装程序。下载后找到该文件，双击安装程序，然后按照简单的指引说明进行操作。 完成安装Git后即可安装Kitematic。虽然Docker Desktop菜单中有一个用于Kitematic安装的菜单，但请不要使用它，因为它不能成功安装Kitematic。我们需要从GitHub[5]下载最新版本的Kitematic。下载完成后同样双击安装，然后完成简单的说明。 此时，请确保点击Docker Desktop图标并单击Sign-in菜单。使用Docker Hub用户凭证登陆以将Docker Desktop关联到你的镜像存储库。 安装kubernetes 下一个技巧，我们将安装Kubernetes对Docker Desktop的支持。为此，请点击Docker Desktop图标并选择“Preferences”。在打开的窗口中，点击Kubernetes选项卡。 单击“Enable Kubernetes”复选框。你还可以选择启用Kubernetes作为Docker Stacks的默认部署工具并显示系统容器。点击“Apply”将出现一个弹出窗口，提醒你安装需要网络连接且需要一些时间。 单击“Install”后安装将开始并结束。安装完成后你应该能看到Docker Engine和Kubernetes都处于运行状态。 在我们继续之前，我们先来测试并确保Kubernetes正常运行。打开终端并敲入命令： 这是令人困惑的地方。你实际上并未使用Docker Desktop来处理容器。Docker Desktop让你可以轻松访问这些可以让你开发和部署容器的工具。例如你现在可以像使用Linux一样打开终端窗口并开始使用Docker命令。你可以拉取镜像，修改镜像，推送镜像，部署容器等。 或者你可以选择使用Kitematic。要打开Kitematic，请单击Docker Desktop图标，然后单击Kitematic。首次运行该工具时，你将需要为其提供辅助功能。在“安全和隐私”窗口中，确保单击与Kitematic关联的复选框。 GUI将打开，请求你的Docker Hub登陆凭据。 登录后，你将看到包含存储库的选项卡，以及镜像，推荐镜像等等。 你现在可以从一个方便的GUI工具开始处理镜像和部署容器。下载你想要使用的镜像，按照你的需求修改并进行部署。 至于Kubernetes，它全部通过命令行处理，除非你安装例如Kubernetic的第三方工具。好消息是，既然你已经在Mac上运行了Kubernetes，那么困难的部分就会得到解决。只需安装第三方GUI工具，你就可以开始使用Kubernetes实例了。通过Kubernetic等工具，你可以轻松创建Pod，Service，Ingress，Deployment等，而无需使用终端窗口。当然如果你想认真学习Docker和Kubernetes，你也应该使用命令行且多加练习。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/alinode.html":{"url":"Tools/alinode.html","title":"alinode使用","keywords":"","body":"alinode By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/iPic-Markdown图床、文件上传工具.html":{"url":"Tools/iPic-Markdown图床、文件上传工具.html","title":"iPic-Markdown图床、文件上传工具","keywords":"","body":"iPic-Markdown图床、文件上传工具 https://apps.apple.com/cn/app/id1101244278?mt=12 有了图床神器 iPic，不论屏幕截图、还是复制图片，都可以自动上传、保存 Markdown 格式的链接，直接粘贴插入，够懒人吧？ 使用 Hexo | Heroku 或 WordPress 写博客、在公众号发文章、在知乎讨论、在豆瓣灌水、在论坛发帖、跨境做外贸电商 … iPic 带给你从未有过的插图体验。 当然，除了图片，你可以 上传普通文件，上传方式与图片完全相同。 上传方式 图床工具 iPic 支持多种图片上传方式。下面我们来简单看下各个上传方式、以及分别适合在什么场景下使用。 1. 拖拽图片上传 拖动是比较好玩的一种上传方式。只要将图片拖到菜单栏的 iPic 图标上，松手后就可以自动上传。 可以注意到，上传时菜单栏图标也会显示上传的进度。很简洁、却很实用，不再盲目等待。 2. 使用服务上传图片 在 Finder 中使用 服务 上传也是很高效的方式。只要在图片上右击、然后选择服务中的 使用 iPic 上传 即可。 除了使用菜单，更高效的方式是使用快捷键。只要选中图片，然后按下 Command + U 快捷键，即可自动上传。 如果你觉得默认快捷键 Command + U 不方便，也可以在 系统偏好设置 > 键盘 > 快捷键 服务 中修改 使用 iPic 上传 对应的快捷键。同样，如果你的 Mac 中安装了很多程序、菜单中有很多你不需要的服务，也可以在这里进行关闭。 使用服务上传还有其他便利之处： 可以一次性上传多张图片 即使 iPic 并未运行，系统也会启动 iPic、并自动上传 注意：由于 macOS 系统更新机制的缘故，新安装 iPic 后上传服务可能未出现、或未翻译，可以等几分钟、甚至几小时后再试，iPic 上传服务就会正常显示；也可以在 终端 手动更新服务菜单： /System/Library/CoreServices/pbs -update 3. 复制图片后上传 iPic 会自动监测剪切板的变化，当复制图片后，该图片会出现在 iPic 菜单中 待上传 区域。如果需要上传，点击菜单中该图片即可。手动上传比较适合临时上传少量图片。 除了手动点击菜单，还可以使用快捷键 Command + Shift + U 上传。当然，可以在偏好设置中修改此快捷键。 4. 上传其他 App 中的图片 上述示例中主要介绍了图片文件的上传。另外，iPic 还支持支持其他程序中图片的上传。例如： 其中，对于图片格式，常见的 jpg、png、gif 等格式都是支持。 上传图片相关设置 上传前压缩图片 可以在 iPic 的 偏好设置 中开启「上传前压缩图片」选项，目前支持压缩的图片格式：jpg、png、gif 采用有损压缩，压缩后肉眼几乎无法看出区别，却可明显降低图片尺寸。使用压缩后的图片，既可以节省图片的存储空间，还可以加快图片加载速度、节省流量。 上传后不播放声音 iPic 上传后会使用系统通知来提示。如果不喜欢该通知的声音，可进入 系统偏好设置 > 通知，在左侧列表选择 iPic，然后在右侧取消「播放通知的声音」。 图床 图床也即你选择存放图片的云服务。可以在 iPic 的 偏好设置 中添加你的图床： 添加后，可以在 iPic 的菜单中选择当前使用的图床： 目前 iPic 支持下列图床：微博图床（即默认图床）、七牛云 、又拍云、阿里云 OSS 、腾讯云 COS 、Imgur 、Flickr 、Amazon S3 iPic 菜单 Markdown 链接 这里有个很贴心的功能：切换普通链接、Markdown 格式链接时，如果粘贴板中有上一格式的内容，会转换后重新保存到粘贴板中。 图片上传记录 iPic 会保存最近上传的 15 张图片，其中最后上传的 3 张图片会出现在一级菜单中，其他的则在 更多已上传图片 中。 点击已上传图片，则会复制该图片的链接。 当然，可以在 更多 菜单中清空图片上传记录。 iPic Mover iPic Mover 可以一键将已有 Markdown 文件中所有图片迁移至新图床。批量上传图片、图床搬家，从未如此简单。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/图片搬家神奇：iPicMover.html":{"url":"Tools/图片搬家神奇：iPicMover.html","title":"图片搬家神奇：iPic Mover","keywords":"","body":"图片搬家神奇：iPic Mover 图片搬家神奇：iPic Mover 在 Markdown 中插图很麻烦；还好，iPic 解决了这个问题。 可是，已有 Markdown 文件中的图片该如何迁移呢？ 不怕，现在有了 iPic Mover :) iPic Mover 是什么？我需要吗？ 先来看看看几个场景，你有没有遇到过？ 批量将本地图片上传至图床，并更新链接、删除本地图片 很多Markdown都支持本地图片，但很少支持上传图片至图床。 Typora除外，因为它继承了iPicUploader，在插入本地图片时，可以调用iPic上传。 更换图床 比如，最近流行升级至https，而原来的图床很可能不支持https，这就需要更换图床。 WordPress迁移至Markdown时，迁移图片 Markdown可谓风头正劲，可WordPress也是昔日王者。将WordPress迁移至Markdown时，比较痛苦的时WordPress中上传的图片资源怎么办？ 压缩已有图片 曾经年少轻狂，上传几MB的图片，眼都不眨一下。现在看看流量账单，心疼啊。可是，要把所有文章中的图片下载、压缩、在上传、更新图片链接，麻烦啊：（ 下个版本即将支持。。。 如果遇到过上面的麻烦，恭喜，iPic Mover就是为你准备的。 如何使用iPic Mover 二话不说先上图： 可以看出，只要选择Markdown所在文件夹即可，iPic Mover自动帮你： 找到所有Markdown文件 找到Markdown文件中所有图片 上传图片 更新Markdown文件中的图片链接 当然，还有几个可选项 迁移图片赋值文件夹或文件 如果选定，自仅在复制的Markdown文件中更新图片链接。原有的Markdown文件将不被修改。这个主要是为了测试使用。 注意： 强烈建议在迁移前备份原始文件 仅上传本地图片 默认会将所有图片迁移至新图床。如果选定，则仅上传本地图片、不上传已经在云上的图片。 上传后删除本地图片文件 如果你不希望本地餐后无用的图片，可以勾选。上传是被的本地图片文件不会被删除。 值得注意的是，iPic Mover还可以按图片大小排序，这样可以快速找出网站中比较大的图片，必要时进行替换、或压缩、以减少网站流量、加快页面打开速度。 iPic Mover 是如何工作的？ 除了上面介绍的，核心的，iPic Mover 是通过调用 iPicUploader，将图片上传至 iPic 当前选中的图床。具体可见： 图床神器 iPic 免费开放上传服务：iPicUploader 由于需要调用 iPic 来上传图片，iPic Mover 在运行时会检测 iPic 是否安装。如果未安装，会提示在 Mac App Store 下载。 下载 iPic Mover iPic Mover 说到最后，终于来重点了 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/ffmpeg音视频处理.html":{"url":"Tools/ffmpeg音视频处理.html","title":"ffmpeg音视频处理","keywords":"","body":"ffmpeg音视频处理 去除音视频中的视频 ffmpeg -i ./1573535886182365.mp4 -map 0:0 -vcodec copy out.mp4 去除音视频中的音频 ffmpeg -i 1573535886182365.mp4 -vcodec copy -an 2.mp4 视频拼接 mpg拼接（问题：拼接成功但只显示第一个） ffmpeg -i \"concat:video1.mpg|video2.mpg\" -c copy output.mpg 拼接视频不一致的格式时，需要转码 例子： ffmpeg -i out11.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate1.ts ffmpeg -i 2.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate2.ts ffmpeg -i \"concat:intermediate1.ts|intermediate2.ts\" -c copy -bsf:a aac_adtstoasc output.mp4 截取音频 ffmpeg -i out.mp4 -ss 00:10 -t 00:40 -acodec copy out1.mp4 将图片转换成视频 生成.mpgffmpeg -i image%3d.jpeg output.mpg 或ffmpeg -f image2 -i image00%d.jpeg video.mpg 生成.mp4 简单的生成 ffmpeg -i image%3d.jpeg -pix_fmt yuv420p a.mp4 -pix_fmt yuv420p 设定参数 ffmpeg -r 25 -loop 1 -i image%3d.jpeg -pix_fmt yuv420p -vcodec libx264 -b:v 600k -r:v 25 -preset medium -crf 30 -s 720x576 -vframes 250 -r 25 -t 3 a.mp4 将多张图片合成视频 带音频合成： ffmpeg -threads 2 -y -r 1 -i image%3d.jpeg -i audio.wav -pix_fmt yuv420p image.mp4 -threads 2: 以两个县城进行运行，加快处理的速度 -y: 对输出文件进行覆盖 -r 10: fps设置为10帧/妙 -absf aac_adtstoasc：编码音频格式，使之能在一些设备上能播放 不带音频合成 ffmpeg -loop 1 -f image2 -i image%3d.jpeg -pix_fmt yuv420p -vcodec libx264 -r 10 -t 10 output.mp4 将一张图片做成视频封面 ffmpeg -i 2.mp4 -i image1.png -map 0 -map 1 -c copy -c:v:1 png -disposition:v:1 attached_pic output_video.mp4 在特定的时间显示图片 在0～5秒内显示图片,图片的x坐标为60，否则为-500(移出屏幕)，y坐标一直为50不变 ffmpeg -y -i 1573535886182365.mp4 -i t016e3f183704154203.jpg -filter_complex \"overlay=x='if(between(t,0,5),0,-5000)':y=0\" -f mp4 left3.mp4 音频提前或延迟N秒 ffmpeg -i 1573535886182365.mp4 -filter_complex \"adelay=3000|3000\" revideo.mp4 合并音频视频或替换音频流 如果输入的视频已经包含音频，并且想要替换它，需要告诉ffmpeg采取哪个音频流： ffmpeg -i output1.mp4 -i out.mp4 \\-c:v copy -c:a aac -strict experimental \\-map 0:v:0 -map 1:a:0 output2.mp4 合并音频和视频，给音频重新编码 在这里，我们假定视频文件没有包含任何音频流，并且希望具有与输入格式相同的输出格式(此处为MP4)。 上述命令转码音频，因为MP4不能携带PCM音频流。如果需要，可以使用任何其他所需的音频编解码器 ffmpeg -i out.mp4 -i 2.mp4 \\-c:v copy -c:a aac -strict experimental output.mp4 实例 视频去除音频 ffmpeg -i v4.mp4 -vcodec copy -an dv2.mp4 ffmpeg -i v3.mp4 -vcodec copy -an dv1.mp4 -i : filename输入文件 -vcodec：vcodec是 -codec:v的一个别称，强制使用codec 编解码方式，未设定时使用与输入流相同的编码器。如果用copy表示原始编解码数据必须被拷贝 -an: 不使用音频记录 拼接视频 ffmpeg -i dv1.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate1.ts ffmpeg -i dv2.mp4 -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate2.ts ffmpeg -i \"concat:intermediate1.ts|intermediate2.ts\" -c:a copy -bsf:a aac_adtstoasc dv12.mp4 -c[:stream_specifier] codec (input/output, per-stream) 插入封面图片 ffmpeg -i dv12.mp4 -i image1.png -map 0 -map 1 -c copy -c:v:1 png -disposition:v:1 attached_pic idv12.mp4 合成音频 ffmpeg -i idv12.mp4 -i audio.wav \\-c:v copy -c:a aac -strict experimental chen.mp4 管道pipe ffmpeg -f image2 -loop 1 -i img.jpg \\ -f s16le -ac 1 -ar 16k -i wd.pcm -t 6 \\ -f nut pipe: | \\ ffmpeg -i audio.mp3 \\ -i pipe: -i 1.mp4 \\ -i 2.mp4 -i 3.mp4 \\ -i 4.mp4 \\ -filter_complex '[1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [3:v]scale=720:1280[in3]; [4:v]scale=720:1280[in4]; [5:v]scale=720:1280[in5]; [in1][in2][in3][in4][in5] concat=n=5:v=1:a=0 [v]; [0:a][1:a] amix=inputs=2 [a]' \\ -map '[v]' -map '[a]' -c:v libx264 -t 30 -f nut pipe: | \\ ffmpeg -i pipe: -i f.jpg \\ -map 0 -map 1 -c:v libx264 -c:a aac -c:v:1 png -disposition:v:1 attached_pic result.mp4 相关资料： 官网 https://vimsky.com/article/3687.html https://www.longqi.cf/tools/2015/02/13/ffmpegcn/ http://einverne.github.io/post/2015/12/ffmpeg-first.html#ffmpeg-usage https://www.twblogs.net/a/5c9a26bfbd9eee434fc6c81b/zh-cn https://wklchris.github.io/FFmpeg.html#%E8%A7%86%E9%A2%91%E5%8F%82%E6%95%B0 https://github.com/tonydeng/fmj/blob/master/ffmpeg.md#%e6%b7%bb%e5%8a%a0%e5%ad%97%e5%b9%95 https://www.ancii.com/arzmnzdzz/ https://www.jianshu.com/p/d6480c6ea6bd http://trac.ffmpeg.org/wiki/Concatenate https://www.linmao.dev/joy/868/#i-3 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/ffmpeg音视频处理需求升级版.html":{"url":"Tools/ffmpeg音视频处理需求升级版.html","title":"ffmpeg音视频处理需求升级版","keywords":"","body":"ffmpeg音视频处理需求升级版 需求 图片转视频、视屏拼接、音频合并、音视频合成，加视频封面（目前这条还没合并在一条命令） 刚开始单命令实现，但是这样会产生多个中间文件，前面的文章已经介绍过了。优化，希望合并为一条命令搞定 在次研究实现过程中踩了几个坑，记录一下：合成过程采用渐进式的方式加入每个小功能点 图片转视频加背景音乐 ffmpeg -f image2 \\ -loop 1 \\ -i img.jpg \\ -f s16le \\ -ac 1 \\ -ar 16k \\ -i wd.pcm \\ -t 5 \\ -vcodec mpeg4 output.mp4 视频拼接 ffmpeg -i v4.mp4 -i v2.mp4 \\ -filter_complex '[0:v]scale=720:1280[in1]; [1:v]scale=720:1280[in2]; [in1][0:a][in2][1:a] concat=n=2:v=1:a=1 [v] [a]' \\ -map [v] -map [a] -c:v libx264 d1.mp4 注意： scale=720:1280指定统一拼接视屏的尺寸，否则 不同尺寸的的视频拼接错误 视频拼接并去除声音 ffmpeg -i v4.mp4 -i v2.mp4 \\ -filter_complex '[0:v]scale=720:1280[in1]; [1:v]scale=720:1280[in2]; [in1][in2] concat=n=2:v=1:a=0 [v]' \\ -map [v] -c:v libx264 dc1.mp4 视频（去除声音）拼接、音频合成 ffmpeg -i v4.mp4 -i v2.mp4 -i audio.wav \\ -filter_complex '[0:v]scale=720:1280[in1]; [1:v]scale=720:1280[in2]; [in1][in2] concat=n=2:v=1:a=0 [v]; [1:a][2:a] concat=n=2:v=0:a=1 [a]' \\ -map [v] -map [a] -c:v libx264 dca1.mp4 图片转视频封面、加背景音乐、拼接视频、加音频合成 ffmpeg -i WechatIMG612.png \\ -i v4.mp4 \\ -i v2.mp4 \\ -i audio1.wav \\ -i t.wav \\ -filter_complex '[0]scale=720:1280[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [4:a][3:a] amix=inputs=2:duration=longest [a]' \\ -map [v] -map [a] -c:v libx264 -pix_fmt yuv420p -c:a aac cc1.mp4 注意： -pix_fmt yuv420p指定可以支持 不同格式的图片转视频，否则，只支持png的格式 amix默认是2，所以可以只写amix, duration=longest,按最长音频输出 但这样写，存在问题，音频比视频长，则视频播放完成，音频会一直播放下去，需求中是按视频播放总时长 尝试解决方案： 方案一：amerge：但音频会按最短音频输出 ffmpeg -i t.wav \\ -i audio.mp3 \\ -filter_complex '[0:a][1:a] amerge [a]' \\ -map [a] -c:a aac -b:a 256k vv.mp3 方案二：join,和merge同样的 ffmpeg -i t.wav \\ -i audio.mp3 \\ -filter_complex '[0:a][1:a] join [a]' \\ -map [a] -c:a aac -b:a 256k vv.mp3 方案三：-shortest：测试结果只支持 单音频时，可以这样实现 ffmpeg -i v1.mp4 \\ -i audio.mp3 \\ -filter_complex \"[0:a][1:a]amerge=inputs=2[a]\" \\ -map 0:v -map \"[a]\" -c:v copy -c:a libvorbis -ac 2 -shortest out.mp4 方案四：-t在最后指定播放总时长(目前只能使用这种方式) ffmpeg -i WechatIMG612.png \\ -i v4.mp4 \\ -i v2.mp4 \\ -i audio1.wav \\ -i t.wav \\ -filter_complex '[0]scale=720:1280[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [4:a][3:a] amix=inputs=2:duration=longest [a]' \\ -map [v] -map [a] -c:v libx264 -pix_fmt yuv420p -c:a aac -t 23 cc1.mp4 图片转视频默认只有一帧，设定持续时长 ffmpeg -i f.png \\ -i 1.mp4 \\ -i 2.mp4 \\ -i audio.mp3 \\ -i t.wav \\ -filter_complex '[0]scale=720:1280,setpts=PTS+7/TB[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [3:a][4:a] amix [a]' \\ -map [v] -map [a] -c:v libx264 -pix_fmt yuv420p -c:a aac -t 23 cc1.mp4 setpts=PTS+5/TB 为输出视频帧设置PTS。 TB: 输入时间戳的时基。 PTS: 输入的时间戳 音频延迟播放 ffmpeg -i f.png \\ -i 1.mp4 \\ -i 2.mp4 \\ -i audio.mp3 \\ -i t.wav \\ -filter_complex '[0]scale=720:1280,setpts=PTS+6/TB[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [3:a][4:a] amix,adelay=1000|1000 [a]' \\ -map [v] -map [a] -c:v libx264 -pix_fmt yuv420p -c:a aac -t 23 cc1.mp4 setpts偏移时间-adelay时间=图片视频持续时间 adelay=1000|1000：两个音频延迟播放时间，单位为ms 推流 ffmpeg -i f.jpg \\ -i 1.mp4 \\ -i long.mp4 \\ -i audio.mp3 \\ -i t.wav \\ -filter_complex '[0]scale=720:1280,setpts=PTS+6/TB[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [3:a]adelay=30s|30s[a3]; [4:a]adelay=60s|60s[a4]; [a3][a4]amix[a]' \\ -map [v] \\ -map [a] \\ -pix_fmt yuv420p \\ -c:v libx264 \\ -c:a aac \\ -t 300 \\ -f flv \"rtmp://...\" 本地拉流 ffmpeg -i \"https://\" -codec copy \\ -f mp4 rr.mp4 添加水印 图片水印 ffmpeg -i 4.mp4 \\ -i sy.png \\ -filter_complex \"overlay=10:10\" birds2.mp4 添加文字水印 ffmpeg -i 4.mp4 \\ -vf \"drawtext=fontfile=/Library/Fonts/AdobeHeitiStd-Regular.otf:text='watermark测试':x=30:y=h-30:enable='if(gte(t,3),0,1)':fontsize=24:fontcolor=white@0.7\" output.mp4 添加水印后推流 ffmpeg -i f.jpg \\ -i 1.mp4 \\ -i long.mp4 \\ -i audio.mp3 \\ -i t.wav \\ -i sy.png \\ -filter_complex '[0]scale=720:1280,setpts=PTS+6/TB[in0]; [1:v]scale=720:1280[in1]; [2:v]scale=720:1280[in2]; [in0][in1][in2] concat=n=3:v=1:a=0 [v]; [3:a]adelay=30s|30s[a3]; [4:a]adelay=60s|60s[a4]; [a3][a4]amix[a]; [v][5:v]overlay[out]' \\ -map [out] \\ -map [a] \\ -pix_fmt yuv420p \\ -c:v libx264 \\ -c:a aac \\ -t 300 \\ -f flv \"rtmp://....\" 循环推流 ## 循环推流 ffmpeg -re \\ -stream_loop 4 \\ -i 1.mp4 \\ -c:v libx264 \\ -f flv \"rtmp://\" overlay 添加水印 amix 混合音频 默认值为2 adelay 延迟播放时间 scale 分辨率大小 720:1280 手机屏的 setpts 图片持续时长 pix_fmt: yuv420p 像素格式 视频默认像素格式所有图片与视频拼接也需要转换 -c:v libx264视频编码 -c:a aac音频编码 -f flv 流媒体封装 合并中，最重要的命令-filter_complex支持的参数 Filters: T.. = Timeline support // 时间线支持 .S. = Slice threading // 切片线程 ..C = Command support A = Audio input/output V = Video input/output N = Dynamic number and/or type of input/output // 动态输入/输出数和/或类型 | = Source or sink filter // 源或汇滤波器 ... abench A->A Benchmark part of a filtergraph. // filtergraph的基准部分。 ... acompressor A->A Audio compressor. // 音频压缩器 ... acontrast A->A Simple audio dynamic range compression/expansion filter. // 简单的音频动态范围压缩/扩展过滤器。 ... acopy A->A Copy the input audio unchanged to the output. // 将输入音频原封不动地复制到输出。 ... acue A->A Delay filtering to match a cue. // 延迟过滤以匹配提示。 ... acrossfade AA->A Cross fade two input audio streams. // 交叉淡入两个输入音频流。 ... acrossover A->N Split audio into per-bands streams. // 将音频分割为每个频带的流。 ... acrusher A->A Reduce audio bit resolution. // 降低音频位分辨率。 .S. adeclick A->A Remove impulsive noise from input audio. // 去除输入音频中的脉冲噪声。 .S. adeclip A->A Remove clipping from input audio. // 从输入音频中删除剪辑。 T.. adelay A->A Delay one or more audio channels. // 延迟一个或多个音频通道。 ... aderivative A->A Compute derivative of input audio. // 计算输入音频的导数。 ... aecho A->A Add echoing to the audio. // 在音频中添加回声。 ... aemphasis A->A Audio emphasis. // 音频强调 ... aeval A->A Filter audio signal according to a specified expression. // 根据指定的表达式过滤音频信号。 T.. afade A->A Fade in/out input audio. // 淡入/淡出输入音频。 TSC afftdn A->A Denoise audio samples using FFT. // 使用FFT对音频样本进行去噪。 ... afftfilt A->A Apply arbitrary expressions to samples in frequency domain. // 对频域中的样本应用任意表达式。 .S. afir AA->N Apply Finite Impulse Response filter with supplied coefficients in 2nd stream. // 在第二个流中应用具有提供系数的有限脉冲响应滤波器 ... aformat A->A Convert the input audio to one of the specified formats. // 将输入音频转换为指定格式之一。 ... agate A->A Audio gate. // 音频门 .S. aiir A->N Apply Infinite Impulse Response filter with supplied coefficients. // 应用具有提供系数的无限冲激响应滤波器。 ... aintegral A->A Compute integral of input audio. // 计算输入音频的积分。 ... ainterleave N->A Temporally interleave audio inputs. // 暂时交错音频输入。 ... alimiter A->A Audio lookahead limiter. // 音频前视限制器。 TSC allpass A->A Apply a two-pole all-pass filter. // 使用两极全通滤波器。 ... aloop A->A Loop audio samples. // 循环音频示例 ... amerge N->A Merge two or more audio streams into a single multi-channel stream. // 将两个或多个音频流合并为单个多通道流。 T.. ametadata A->A Manipulate audio frame metadata. //操纵音频帧元数据。 ... amix N->A Audio mixing. // 音频混合 ... amultiply AA->A Multiply two audio streams. // 将两个音频流相乘。 ..C anequalizer A->N Apply high-order audio parametric multi band equalizer. // 采用高阶音频参数多波段均衡器。 TSC anlmdn A->A Reduce broadband noise from stream using Non-Local Means. // 使用非本地手段减少流的宽带噪声。 ... anull A->A Pass the source unchanged to the output. // 将源不变地传递给输出。 T.. apad A->A Pad audio with silence. // 用静音键播放音频。 ... aperms A->A Set permissions for the output audio frame. // 设置输出音频帧的权限 ... aphaser A->A Add a phasing effect to the audio. // 为音频添加相位效果。 ... apulsator A->A Audio pulsator. // 音频脉冲发生器。 ... arealtime A->A Slow down filtering to match realtime. // 降低过滤速度以匹配实时性。 ... aresample A->A Resample audio data. // 重新采样音频数据。 ... areverse A->A Reverse an audio clip. // 反转音频剪辑。 ... aselect A->N Select audio frames to pass in output. // 选择要传入输出的音频帧。 ... asendcmd A->A Send commands to filters. // 向筛选器发送命令。 ... asetnsamples A->A Set the number of samples for each output audio frames. // 设置每个输出音频帧的采样数。 ... asetpts A->A Set PTS for the output audio frame. // 为输出音频帧设置PTS。 ... asetrate A->A Change the sample rate without altering the data. // 更改采样率而不更改数据。 ... asettb A->A Set timebase for the audio output link. // 设置音频输出链接的时基。 ... ashowinfo A->A Show textual information for each audio frame. // 显示每个音频帧的文本信息。 T.. asidedata A->A Manipulate audio frame side data. // 操作音频帧侧数据 T.. asoftclip A->A Audio Soft Clipper. // 音频软剪辑。 ... asplit A->N Pass on the audio input to N audio outputs. // 把音频输入传给N个音频输出。 ... astats A->A Show time domain statistics about audio frames. // 显示有关音频帧的时域统计信息 ..C astreamselect N->N Select audio streams // 选择音频流 ..C atempo A->A Adjust audio tempo. // 调整音频节奏。 ... atrim A->A Pick one continuous section from the input, drop the rest. // 从输入中选取一个连续的部分，删除其余部分。 TSC bandpass A->A Apply a two-pole Butterworth band-pass filter. // 应用两极巴特沃斯带通滤波器。 TSC bandreject A->A Apply a two-pole Butterworth band-reject filter. // 应用两极巴特沃斯带阻滤波器。 TSC bass A->A Boost or cut lower frequencies. // 提高或降低低频率。 TSC biquad A->A Apply a biquad IIR filter with the given coefficients. // 应用具有给定系数的双四IIR滤波器 ... channelmap A->A Remap audio channels. // 重新映射音频通道 ... channelsplit A->N Split audio into per-channel streams. // 将音频分割为每个频道流。 ... chorus A->A Add a chorus effect to the audio. // 为音频添加合唱效果。 ... compand A->A Compress or expand audio dynamic range. // 压缩或扩展音频动态范围 ... compensationdelay A->A Audio Compensation Delay Line. // 音频补偿延迟线。 T.. crossfeed A->A Apply headphone crossfeed filter. // 应用耳机交叉馈入滤波器。 ... crystalizer A->A Simple expand audio dynamic range filter. // 简单的扩展音频动态范围过滤器。 T.. dcshift A->A Apply a DC shift to the audio. // 对音频应用DC移位。 T.. deesser A->A Apply de-essing to the audio. //对音频应用删除操作。 ... drmeter A->A Measure audio dynamic range. // 测量音频动态范围。 T.. dynaudnorm A->A Dynamic Audio Normalizer. // 动态音频标准化器。 ... earwax A->A Widen the stereo image. // 放大立体图像。 ... ebur128 A->N EBU R128 scanner. // EBU R128扫描仪 TSC equalizer A->A Apply two-pole peaking equalization (EQ) filter. // 采用双极峰值均衡（EQ）滤波器。 T.. extrastereo A->A Increase difference between stereo audio channels. // 增加立体声音频通道之间的差异。 ..C firequalizer A->A Finite Impulse Response Equalizer. // 有限脉冲响应均衡器。 ... flanger A->A Apply a flanging effect to the audio. // 对音频应用翻边效果。 ... haas A->A Apply Haas Stereo Enhancer. // 应用Haas立体声增强剂。 ... hdcd A->A Apply High Definition Compatible Digital (HDCD) decoding. // 应用高清晰度兼容数字（HDCD）解码。 .S. headphone N->A Apply headphone binaural spatialization with HRTFs in additional streams. // 在附加流中应用耳机双耳空间化和HRTFs。 TSC highpass A->A Apply a high-pass filter with 3dB point frequency. // 使用点频率为3dB的高通滤波器。 TSC highshelf A->A Apply a high shelf filter. // 使用高架过滤器。 ... join N->A Join multiple audio streams into multi-channel output. // 将多个音频流连接到多通道输出。 ... loudnorm A->A EBU R128 loudness normalization // EBU R128响度标准化 TSC lowpass A->A Apply a low-pass filter with 3dB point frequency. // 使用点频率为3dB的低通滤波器。 TSC lowshelf A->A Apply a low shelf filter. // 使用低架过滤器。 ... mcompand A->A Multiband Compress or expand audio dynamic range. // 多波段压缩或扩展音频动态范围。 ... pan A->A Remix channels with coefficients (panning). // 带系数的混音频道（平移） ... replaygain A->A ReplayGain scanner. // 重新扫描。 ... resample A->A Audio resampling and conversion. // 音频重采样和转换。 ..C rubberband A->A Apply time-stretching and pitch-shifting. //应用时间拉伸和俯仰移位。 ... sidechaincompress AA->A Sidechain compressor. // 侧链压缩机。 ... sidechaingate AA->A Audio sidechain gate. // 音频侧链门 ... silencedetect A->A Detect silence. // 注意安静。 ... silenceremove A->A Remove silence. // 消除沉默。 ... stereotools A->A Apply various stereo tools. // 使用各种立体声工具。 T.. stereowiden A->A Apply stereo widening effect. // 应用立体加宽效果。 ... superequalizer A->A Apply 18 band equalization filter. // 应用18波段均衡滤波器。 .S. surround A->A Apply audio surround upmix filter. //应用音频环绕上混滤波器。 TSC treble A->A Boost or cut upper frequencies. // 提高或降低高频。 ... tremolo A->A Apply tremolo effect. // 应用颤音效果。 ... vibrato A->A Apply vibrato effect. // 应用振动效果。 T.C volume A->A Change input volume. // 更改输入音量。 ... volumedetect A->A Detect audio volume. // 我检测到音频音量。 ... aevalsrc |->A Generate an audio signal generated by an expression. // 生成由表达式生成的音频信号。 ... anoisesrc |->A Generate a noise audio signal. // 产生噪声音频信号。 ... anullsrc |->A Null audio source, return empty audio frames. // 空音频源，返回空音频帧。 ... hilbert |->A Generate a Hilbert transform FIR coefficients. // 生成Hilbert变换FIR系数。 ... sinc |->A Generate a sinc kaiser-windowed low-pass, high-pass, band-pass, or band-reject FIR coefficients. // 生成sinc-kaiser加窗低通、高通、带通或带阻FIR系数。 ... sine |->A Generate sine wave audio signal. // 产生正弦波音频信号。 ... anullsink A->| Do absolutely nothing with the input audio. // 对输入音频绝对不要做任何事情。 ... alphaextract V->N Extract an alpha channel as a grayscale image component. // 将alpha通道提取为灰度图像组件。 ... alphamerge VV->V Copy the luma value of the second input into the alpha channel of the first input. // 将第二个输入的luma值复制到第一个输入的alpha通道中。 .S. amplify V->V Amplify changes between successive video frames. // 放大连续视频帧之间的变化。 ... ass V->V Render ASS subtitles onto input video using the libass library. // 使用libass库将ASS字幕呈现到输入视频上。 TS. atadenoise V->V Apply an Adaptive Temporal Averaging Denoiser. // 应用自适应时间平均去噪。 TS. avgblur V->V Apply Average Blur filter. // 应用平均模糊过滤器。 T.. bbox V->V Compute bounding box for each frame. // 计算每个帧的边界框。 ... bench V->V Benchmark part of a filtergraph. // filtergraph的基准部分。 T.. bitplanenoise V->V Measure bit plane noise. // 测量位平面噪声。 ... blackdetect V->V Detect video intervals that are (almost) black. // 检测（几乎）黑色的视频间隔。 ... blackframe V->V Detect frames that are (almost) black. 检测（几乎）黑色的帧。 TS. blend VV->V Blend two video frames into each other. // 将两个视频帧混合在一起。 TS. bm3d N->V Block-Matching 3D denoiser. // 块匹配三维去噪。 T.. boxblur V->V Blur the input. // 虚输入 TS. bwdif V->V Deinterlace the input image. // 取消输入图像的隔行扫描 TS. chromahold V->V Turns a certain color range into gray. // 将某个颜色范围转换为灰色。 TS. chromakey V->V Turns a certain color into transparency. Operates on YUV colors. // 将某个颜色转换为透明。对YUV颜色进行操作。 TS. chromashift V->V Shift chroma. // 变色 ... ciescope V->V Video CIE scope. // 视频CIE示波器。 T.. codecview V->V Visualize information about some codecs. // 可视化一些编解码器的信息。 TS. colorbalance V->V Adjust the color balance. // 调整颜色平衡。 TS. colorchannelmixer V->V Adjust colors by mixing color channels. // 通过混合颜色通道调节颜色。 TS. colorkey V->V Turns a certain color into transparency. Operates on RGB colors. // 将某个颜色转换为透明。对RGB颜色进行操作。 TS. colorhold V->V Turns a certain color range into gray. Operates on RGB colors. // 将某个颜色范围转换为灰色。对RGB颜色进行操作。 TS. colorlevels V->V Adjust the color levels. // 调整颜色级别。 TS. colormatrix V->V Convert color matrix. // 转换颜色矩阵。 TS. colorspace V->V Convert between color spaces. // 在颜色空间之间转换。 TS. convolution V->V Apply convolution filter. // 应用卷积滤波器 TS. convolve VV->V Convolve first video stream with second video stream. // 将第一视频流与第二视频流卷积。 ... copy V->V Copy the input video unchanged to the output. // 将输入视频原封不动地复制到输出。 ... coreimage V->V Video filtering using CoreImage API. // 使用CoreImage API进行视频过滤。 ... cover_rect V->V Find and cover a user specified object. // 查找并覆盖用户指定的对象。 ..C crop V->V Crop the input video. // 裁剪输入视频 T.. cropdetect V->V Auto-detect crop size. // 自动检测作物大小。 ... cue V->V Delay filtering to match a cue. // 延迟过滤以匹配提示。 TS. curves V->V Adjust components curves. // 调整组件曲线 .S. datascope V->V Video data analysis. // 视频数据分析。 TS. dctdnoiz V->V Denoise frames using 2D DCT. // 使用2D-DCT对帧进行去噪。 TS. deband V->V Debands video. // 辩论录像。 T.. deblock V->V Deblock video. // 解除锁定视频。 ... decimate N->V Decimate frames (post field matching filter). // 抽取帧（后期字段匹配过滤器）。 TS. deconvolve VV->V Deconvolve first video stream with second video stream. // 用第二视频流解卷第一视频流。 TS. dedot V->V Reduce cross-luminance and cross-color. // 降低交叉亮度和交叉颜色。 TS. deflate V->V Apply deflate effect. // 应用放气效果。 ... deflicker V->V Remove temporal frame luminance variations. // 移除时间帧亮度变化。 ... dejudder V->V Remove judder produced by pullup. // 去除拉起产生的抖动。 T.. delogo V->V Remove logo from input video. // 从输入视频中删除徽标。 T.. derain V->V Apply derain filter to the input. 在输入端加上减额滤波器。 ... deshake V->V Stabilize shaky video. // 稳定不稳定的视频。 TS. despill V->V Despill video. // 视频系统。 ... detelecine V->V Apply an inverse telecine pattern. // 应用相反的电视电影模式。 TS. dilation V->V Apply dilation effect. // 应用扩张效果。 T.. displace VVV->V Displace pixels. ... doubleweave V->V Weave input video fields into double number of frames. T.. drawbox V->V Draw a colored box on the input video. ... drawgraph V->V Draw a graph using input video metadata. T.. drawgrid V->V Draw a colored grid on the input video. T.C drawtext V->V Draw text on top of video frames using libfreetype library. T.. edgedetect V->V Detect and draw edge. ... elbg V->V Apply posterize effect, using the ELBG algorithm. T.. entropy V->V Measure video frames entropy. T.C eq V->V Adjust brightness, contrast, gamma, and saturation. TS. erosion V->V Apply erosion effect. ... extractplanes V->N Extract planes as grayscale frames. .S. fade V->V Fade in/out input video. T.. fftdnoiz V->V Denoise frames using 3D FFT. T.. fftfilt V->V Apply arbitrary expressions to pixels in frequency domain. ... field V->V Extract a field from the input video. ... fieldhint V->V Field matching using hints. ... fieldmatch N->V Field matching for inverse telecine. T.. fieldorder V->V Set the field order. T.. fillborders V->V Fill borders of the input video. ... find_rect V->V Find a user specified object. T.. floodfill V->V Fill area with same color with another color. ... format V->V Convert the input video to one of the specified pixel formats. ... fps V->V Force constant framerate. ... framepack VV->V Generate a frame packed stereoscopic video. .S. framerate V->V Upsamples or downsamples progressive source between specified frame rates. T.. framestep V->V Select one frame every N frames. ... freezedetect V->V Detects frozen video input. ... frei0r V->V Apply a frei0r effect. T.. fspp V->V Apply Fast Simple Post-processing filter. TS. gblur V->V Apply Gaussian Blur filter. TS. geq V->V Apply generic equation to each pixel. T.. gradfun V->V Debands video quickly using gradients. ... graphmonitor V->V Show various filtergraph stats. TS. greyedge V->V Estimates scene illumination by grey edge assumption. TS. haldclut VV->V Adjust colors using a Hald CLUT. TS. hflip V->V Horizontally flip the input video. T.. histeq V->V Apply global color histogram equalization. ... histogram V->V Compute and draw a histogram. T.. hqdn3d V->V Apply a High Quality 3D Denoiser. .S. hqx V->V Scale the input by 2, 3 or 4 using the hq*x magnification algorithm. ... hstack N->V Stack video inputs horizontally. T.C hue V->V Adjust the hue and saturation of the input video. ... hwdownload V->V Download a hardware frame to a normal frame ... hwmap V->V Map hardware frames ... hwupload V->V Upload a normal frame to a hardware frame T.. hysteresis VV->V Grow first stream into second stream by connecting components. ... idet V->V Interlace detect Filter. T.. il V->V Deinterleave or interleave fields. TS. inflate V->V Apply inflate effect. ... interlace V->V Convert progressive video into interlaced. ... interleave N->V Temporally interleave video inputs. ... kerndeint V->V Apply kernel deinterlacing to the input. .S. lagfun V->V Slowly update darker pixels. .S. lenscorrection V->V Rectify the image by correcting for lens distortion. // 通过校正镜头失真来校正图像。 TS. limiter V->V Limit pixels components to the specified range. ... loop V->V Loop video frames. // 循环视频帧。 TS. lumakey V->V Turns a certain luma into transparency. TS. lut V->V Compute and apply a lookup table to the RGB/YUV input video. TS. lut1d V->V Adjust colors using a 1D LUT. TS. lut2 VV->V Compute and apply a lookup table from two video inputs. // 从两个视频输入计算并应用查找表。 TS. lut3d V->V Adjust colors using a 3D LUT. TS. lutrgb V->V Compute and apply a lookup table to the RGB input video. TS. lutyuv V->V Compute and apply a lookup table to the YUV input video. TS. maskedclamp VVV->V Clamp first stream with second stream and third stream. // 用第二流和第三流夹住第一流。 TS. maskedmerge VVV->V Merge first stream with second stream using third stream as mask. // 使用第三流作为掩码将第一流与第二流合并 TS. maskfun V->V Create Mask. ... mcdeint V->V Apply motion compensating deinterlacing. ... mergeplanes N->V Merge planes. ... mestimate V->V Generate motion vectors. T.. metadata V->V Manipulate video frame metadata. T.. midequalizer VV->V Apply Midway Equalization. ... minterpolate V->V Frame rate conversion using Motion Interpolation. .S. mix N->V Mix video inputs. ... mpdecimate V->V Remove near-duplicate frames. TS. negate V->V Negate input video. // 否定输入视频。 TS. nlmeans V->V Non-local means denoiser. T.. nnedi V->V Apply neural network edge directed interpolation intra-only deinterlacer. ... noformat V->V Force libavfilter not to use any of the specified pixel formats for the input to the next filter. TS. noise V->V Add noise. T.. normalize V->V Normalize RGB video. // 标准化RGB视频 ... null V->V Pass the source unchanged to the output. ... ocr V->V Optical Character Recognition. T.. oscilloscope V->V 2D Video Oscilloscope. TSC overlay VV->V Overlay a video source on top of the input. // 在输入端覆盖视频源 T.. owdenoise V->V Denoise using wavelets. ... pad V->V Pad the input video. // 把输入的视频垫上 ... palettegen V->V Find the optimal palette for a given stream. ... paletteuse VV->V Use a palette to downsample an input video stream. // 使用调色板缩小输入视频流的采样。 ... perms V->V Set permissions for the output video frame. // 设置输出视频帧的权限。 TS. perspective V->V Correct the perspective of video. // 纠正视频的透视图。 T.. phase V->V Phase shift fields. ... pixdesctest V->V Test pixel format definitions. T.. pixscope V->V Pixel data analysis. T.C pp V->V Filter video using libpostproc. // 使用libpostproc过滤视频。 T.. pp7 V->V Apply Postprocessing 7 filter. TS. premultiply N->V PreMultiply first stream with first plane of second stream. TS. prewitt V->V Apply prewitt operator. T.. pseudocolor V->V Make pseudocolored video frames. // 制作假彩色视频帧。 ... psnr VV->V Calculate the PSNR between two video streams. // 计算两个视频流之间的PSNR。 ... pullup V->V Pullup from field sequence to frames. T.. qp V->V Change video quantization parameters. // 更改视频量化参数。 ... random V->V Return random frames. T.. readeia608 V->V Read EIA-608 Closed Caption codes from input video and write them to frame metadata. // 从输入视频中读取EIA-608闭路字幕代码，并将其写入帧元数据。 ... readvitc V->V Read vertical interval timecode and write it to frame metadata. ... realtime V->V Slow down filtering to match realtime. TS. remap VVV->V Remap pixels. // 大括号像素。 TS. removegrain V->V Remove grain. T.. removelogo V->V Remove a TV logo based on a mask image. // 移除基于遮罩图像的电视徽标 ... repeatfields V->V Hard repeat fields based on MPEG repeat field flag. ... reverse V->V Reverse a clip. TS. rgbashift V->V Shift RGBA. TS. roberts V->V Apply roberts cross operator. TSC rotate V->V Rotate the input image. // 旋转输入图像。 T.. sab V->V Apply shape adaptive blur. ..C scale V->V Scale the input video size and/or convert the image format. // 缩放输入视频大小和/或转换图像格式。 ..C scale2ref VV->VV Scale the input video size and/or convert the image format to the given reference. // 缩放输入视频大小和/或将图像格式转换为给定的参考。 ... select V->N Select video frames to pass in output. // 选择要传入输出的视频帧。 TS. selectivecolor V->V Apply CMYK adjustments to specific color ranges. ... sendcmd V->V Send commands to filters. ... separatefields V->V Split input video frames into fields. // 将输入的视频帧分割为多个字段。 ... setdar V->V Set the frame display aspect ratio. ... setfield V->V Force field for the output video frame. // 输出视频帧的力场。 ... setparams V->V Force field, or color property for the output video frame. // 输出视频帧的力场或颜色属性。 ... setpts V->V Set PTS for the output video frame. // 为输出视频帧设置PTS。 ... setrange V->V Force color range for the output video frame. // 强制输出视频帧的颜色范围。 ... setsar V->V Set the pixel sample aspect ratio. ... settb V->V Set timebase for the video output link. // 设置视频输出链接的时基。 ... showinfo V->V Show textual information for each video frame. // 显示每个视频帧的文本信息。 T.. showpalette V->V Display frame palette. T.. shuffleframes V->V Shuffle video frames. // 随机播放视频帧。 T.. shuffleplanes V->V Shuffle video planes. T.. sidedata V->V Manipulate video frame side data. .S. signalstats V->V Generate statistics from video analysis. // 从视频分析生成统计信息。 ... signature N->V Calculate the MPEG-7 video signature // 计算MPEG-7视频签名 T.. smartblur V->V Blur the input video without impacting the outlines. // 在不影响轮廓的情况下模糊输入视频。 TS. sobel V->V Apply sobel operator. ... split V->N Pass on the input to N video outputs. // 将输入传给N个视频输出。 T.C spp V->V Apply a simple post processing filter. T.. sr V->V Apply DNN-based image super resolution to the input. // 对输入应用基于DNN的图像超分辨率。 ... ssim VV->V Calculate the SSIM between two video streams. // 计算两个视频流之间的SSIM。 .S. stereo3d V->V Convert video stereoscopic 3D view. // 转换视频立体三维视图。 ..C streamselect N->N Select video streams // 选择视频流 ... subtitles V->V Render text subtitles onto input video using the libass library. ... super2xsai V->V Scale the input by 2x using the Super2xSaI pixel art algorithm. T.. swaprect V->V Swap 2 rectangular objects in video. // 换视频中的两个矩形对象。 T.. swapuv V->V Swap U and V components. TS. tblend V->V Blend successive frames. ... telecine V->V Apply a telecine pattern. TS. threshold VVVV->V Threshold first video stream using other video streams. // 使用其他视频流的阈值第一视频流。 ... thumbnail V->V Select the most representative frame in a given sequence of consecutive frames. ... tile V->V Tile several successive frames together. ... tinterlace V->V Perform temporal field interlacing. TS. tlut2 V->V Compute and apply a lookup table from two successive frames. TS. tmix V->V Mix successive video frames. // 混合连续的视频帧。 .S. tonemap V->V Conversion to/from different dynamic ranges. ... tpad V->V Temporarily pad video frames. // 临时填充视频帧。 .S. transpose V->V Transpose input video. // 转置输入视频。 ... trim V->V Pick one continuous section from the input, drop the rest. TS. unpremultiply N->V UnPreMultiply first stream with first plane of second stream. TS. unsharp V->V Sharpen or blur the input video. // 锐化或模糊输入视频 T.. uspp V->V Apply Ultra Simple / Slow Post-processing filter. T.. vaguedenoiser V->V Apply a Wavelet based Denoiser. ... vectorscope V->V Video vectorscope. T.. vflip V->V Flip the input video vertically. // 垂直翻转输入视频。 ... vfrdet V->V Variable frame rate detect filter. TS. vibrance V->V Boost or alter saturation. ... vidstabdetect V->V Extract relative transformations, pass 1 of 2 for stabilization (see vidstabtransform for pass 2). ... vidstabtransform V->V Transform the frames, pass 2 of 2 for stabilization (see vidstabdetect for pass 1). T.. vignette V->V Make or reverse a vignette effect. ... vmafmotion V->V Calculate the VMAF Motion score. ... vstack N->V Stack video inputs vertically. // 垂直堆叠视频输入。 TS. w3fdif V->V Apply Martin Weston three field deinterlace. .S. waveform V->V Video waveform monitor. ... weave V->V Weave input video fields into frames. // 将输入的视频字段编织成帧。 .S. xbr V->V Scale the input using xBR algorithm. .S. xmedian N->V Pick median pixels from several video inputs. // 从多个视频输入中选取中值像素。 ... xstack N->V Stack video inputs into custom layout. // 将视频输入堆叠到自定义布局中。 TS. yadif V->V Deinterlace the input image. // 取消输入图像的隔行扫描。 ... zoompan V->V Apply Zoom & Pan effect. ... allrgb |->V Generate all RGB colors. ... allyuv |->V Generate all yuv colors. ... cellauto |->V Create pattern generated by an elementary cellular automaton. ..C color |->V Provide an uniformly colored input. ... coreimagesrc |->V Video source using image generators of CoreImage API. // 使用CoreImage API的图像生成器的视频源。 ... frei0r_src |->V Generate a frei0r source. ... haldclutsrc |->V Provide an identity Hald CLUT. ... life |->V Create life. ... mandelbrot |->V Render a Mandelbrot fractal. ... mptestsrc |->V Generate various test pattern. ... nullsrc |->V Null video source, return unprocessed video frames. // 空视频源，返回未处理的视频帧。 ... pal75bars |->V Generate PAL 75% color bars. ... pal100bars |->V Generate PAL 100% color bars. ... rgbtestsrc |->V Generate RGB test pattern. ... smptebars |->V Generate SMPTE color bars. ... smptehdbars |->V Generate SMPTE HD color bars. ... testsrc |->V Generate test pattern. ... testsrc2 |->V Generate another test pattern. ... yuvtestsrc |->V Generate YUV test pattern. ... nullsink V->| Do absolutely nothing with the input video. // 对输入的视频绝对不要做任何事情。 ... abitscope A->V Convert input audio to audio bit scope video output. // 将输入音频转换为音频位范围视频输出。 ... adrawgraph A->V Draw a graph using input audio metadata. ... agraphmonitor A->V Show various filtergraph stats. ... ahistogram A->V Convert input audio to histogram video output. // 将输入音频转换为直方图视频输出。 ... aphasemeter A->N Convert input audio to phase meter video output. // 将输入音频转换为相位计视频输出。 ... avectorscope A->V Convert input audio to vectorscope video output. // 将输入音频转换为矢量示波器视频输出。 ..C concat N->N Concatenate audio and video streams. // 连接音频和视频流。 ... showcqt A->V Convert input audio to a CQT (Constant/Clamped Q Transform) spectrum video output. // 将输入音频转换为CQT（恒定/钳制Q变换）频谱视频输出。 ... showfreqs A->V Convert input audio to a frequencies video output. // 将输入音频转换为频率视频输出。 .S. showspatial A->V Convert input audio to a spatial video output. // 将输入音频转换为空间视频输出。 .S. showspectrum A->V Convert input audio to a spectrum video output. // 将输入音频转换为频谱视频输出。 .S. showspectrumpic A->V Convert input audio to a spectrum video output single picture. // 将输入音频转换为频谱视频输出单个图片。 ... showvolume A->V Convert input audio volume to video output. // 输入音频音量转换为视频输出。 ... showwaves A->V Convert input audio to a video output. // 将输入音频转换为视频输出。 ... showwavespic A->V Convert input audio to a video output single picture. // 将输入音频转换为视频输出单个图片。 ... spectrumsynth VV->A Convert input spectrum videos to audio output. // 将输入频谱视频转换为音频输出。 ..C amovie |->N Read audio from a movie source. ..C movie |->N Read from a movie source. ... afifo A->A Buffer input frames and send them when they are requested. ... fifo V->V Buffer input images and send them when they are requested. ... abuffer |->A Buffer audio frames, and make them accessible to the filterchain. ... buffer |->V Buffer video frames, and make them accessible to the filterchain. // 缓冲视频帧，并使其可被过滤器链访问。 ... abuffersink A->| Buffer audio frames, and make them available to the end of the filter graph. // 缓冲音频帧，并使它们在过滤图的末尾可用。 ... buffersink V->| Buffer video frames, and make them available to the end of the filter graph. // 缓冲视频帧，并使它们在过滤图的末尾可用。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/七牛云存储中没有外链域名，无法下载的问题.html":{"url":"Tools/七牛云存储中没有外链域名，无法下载的问题.html","title":"七牛云存储中没有外链域名，无法下载的问题","keywords":"","body":"七牛云存储中没有外链域名，无法下载的问题 有三种方式来获取文件 方案一（外网流量下载） 您可以使用最新的图形化工具kodo-browser上传/下载试下，支持批量管理。 https://developer.qiniu.com/kodo/tools/5972/kodo-browser 注意：该工具目前使用源站域名进行上传/下载，流量费用为「外网流出流量，0GB - 100 TB 0.29 元/GB」 方案二（CDN流量批量下载） 您需要先新建一个同区域存储空间，会分配一个新的测试域名到新空间。 1）qshell account 此处填写ak 此处填写sk 此处填写自定义的用户名 （ak sk可以在控制台右上角的个人面板，密钥管理里找到） 1) qshell listbucket 原bucket名 -o list.txt （list出全部文件，https://github.com/qiniu/qshell/blob/master/docs/listbucket.md） 3）cat list.txt | awk '{print $1}' >list_final.txt （ 用awk获取list结果的第一列） 4）qshell batchcopy 原bucket名 新bucket名 -i list_final.txt （复制到新bucket的文件和原bucket文件名一致，https://github.com/qiniu/qshell/blob/master/docs/batchcopy.md） 5）qshell qdownload newfilelist.txt （newfilelist.txt为下载的配置文档，https://github.com/qiniu/qshell/blob/master/docs/qdownload.md） qshell安装包及文档请参考https://developer.qiniu.com/kodo/tools/1302/qshell windows环境下安装教程参考 https://developer.qiniu.com/kodo/kb/5545/windows-environment-installation-qshell-tutorial 方案三（CDN流量单文件下载） 使用工具qrsctl https://developer.qiniu.com/kodo/tools/1300/qrsctl qrsctl get By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/基于WebAssembly的H265播放.html":{"url":"Tools/基于WebAssembly的H265播放.html","title":"基于WebAssembly的H265播放","keywords":"","body":"基于WebAssembly的H265播放 为什么视频编码很重要 例子： 540 * 960的视屏，每秒15帧，1分钟的数据量 一帧： 540 960 8 * 3 一分钟bit 540 960 8 3 15 * 60 540 960 8 3 15 * 60 /8 /1024/1024 = 1334Mb 1. 视频编码主要做什么：压缩视频尺寸 编码类型： H264 视频原尺寸: 1334Mb 压缩后尺寸：11Mb 压缩率：120倍 2. 带宽费用： 视频编码技术的发展 为什么选择H.265 优点 视频画质更高 压缩率更高 码率要求更低 缺点： 浏览器支持情况差 方案选型 最终方案： FLV + WebAssembly + FFMpeg + H.265 FFMpeg: 跨平台的音视频录制、转码解决方案 WebAssembly：可在浏览器运行的高性能模块； 直播通用架构 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/使用TravisCI构建Gitbook.html":{"url":"Tools/使用TravisCI构建Gitbook.html","title":"使用TravisCI构建Gitbook","keywords":"","body":"使用TravisCI构建Gitbook 为什么要使用Travis CI构建Gitbook？ 为你的项目接入Travis CI后，可以实现你的项目完全在线自动部署，无论你在任何地方，只要能够接入到互联网，访问到GitHub，就可以更新你的电子书，同时使其自动发不到GitHub上。 特别是你的电子书需要和一些非技术类同学协作时，借助Gitbook，他可以只通过浏览器对电子书进行修改，无需在自己的电脑上偶主Gitbook环境 配置你步骤 创建.travis.yml文件 Travis的构建基于.travis.yml文件进行的，因此，为了让Travis CI能够正常构建，我们先来创建.travis.yml。 访问你的GitHub项目主页，点击其中的Create new File 在弹出的界面中填入文件名 .travis.yml language: node_js node_js: - \"8\" cache: directories: - $HOME/.npm before_install: - export TZ='Asia/Shanghai' install: - npm install gitbook-cli -g - gitbook install script: - gitbook build . ./build branches: only: - master deploy: provider: pages skip_cleanup: true github_token: $GITHUB_TOKEN local_dir: build on: branch: master 添加完成后，选择**commit new file 即可。 配置Travis CI 如果你想要借助Travis CI来构建，除了创建配置文件以外，还需要使用你的GitHub账号登陆Travis CI，进行一些简单的配置 访问Travis CI官网，使用Github登陆 登陆后，点击右上角用户头像，在谭书的下拉窗口中选择Settings。 在下方的Repository列表中找到你的项目 点击项目后的Settings，进入到项目的界面 在项目界面中找到More Options，选择其中的Settings 进入到项目的设置界面。 在下方的Environment Variables中添加一个新的名为$GITHUB_TOKEN的环境变量，将你自己的Personal Access Token填入其中，用于后续的GitHub Pages自动上传 添加完成后，再次点击More Options，选择其中的Trigger Build。 在弹出的界面中直接点击按钮开始构建 随后，你就可以等待Travis CI 的自动构建了 稍等片刻，当你发现Travis CI的构建变成绿色后，就说明已经构建完成了 此时，回到Github，点击界面中的Branc，就可以看到所有的分支了 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/持续集成和travisCI.html":{"url":"Tools/持续集成和travisCI.html","title":"持续集成和travisCI","keywords":"","body":"持续集成和travisCI 持续集成的作用 优点： 快速发现错误 防止分支大幅偏离主干 核心措施是，代码集成到主干之前，必须通过自动话测试。只要有一个测试用例失败，就不能集成。 GitHub最流行的CI Travis CI Circle CI Jenkins AppVeyor CodeShip Drone Semaphore CI Buildkite Wercker TeamCity 接入Travis CI https://travis-ci.org/使用GitHub账号登陆 在https://travis-ci.org/account/respositories为项目开启 项目根目录下新增.travis.yml travis.yml内容 install安装项目依赖 script运行测试用例 language: node_js sudo: false cache: apt: true directories: - node_modules node_js: stable install: - npm install -D - cd ./test/smoke/template - npm install -D - cd ../../../ scripts: npm test By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Nginx基础.html":{"url":"Tools/Nginx基础.html","title":"Nginx基础","keywords":"","body":"Nginx基础 nginx是一款自由的、开源的、高性能的HTTP服务器和反响代理服务器；同时也是一个IMAP、POP3、SMT代理服务器。 Nginx可以作为一个HTTP服务器进行网站的发布处理，另外Nginx可以作为反向代理进行负载均衡的实现。 正向代理 用途： 访问原来无法访问的资源，如Google 可以作为缓存，加速访问资源。 对客户端访问授权，上网进行认证 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息 反向代理 明白了什么是正向代理，我们继续看关于反向代理的处理方式，举例如我国的某宝网站，每天同时连接到网站的访问人数已经爆表，单个服务器远远不能满足人民日益增长的购买欲望了。 此时就出现了一个大家耳熟能详的名词：分布式部署；也就是通过部署多台服务器来解决访问人数限制的问题。 某宝网站中大部分功能也是直接使用 Nginx 进行反向代理实现的，并且通过封装 Nginx 和其他的组件之后起了个高大上的名字：Tengine。 有兴趣的童鞋可以访问 Tengine 的官网查看具体的信息： http://tengine.taobao.org/ 客户端是无感知代理的存在的，反向代理对外都是透明的，访问者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。 方向代理的作用： 保证内网的安全，通常将方向代理作为公网访问地址，Web服务器是内网 负载均衡，通过反向代理服务器来优化网站的负载 通常情况下，我们在实际项目操作时，正向代理和反向代理很有可能会存在同一个应用场景中，正向代理代理客户端的请求去访问目标服务器，目标服务器是一个反向单利服务器，反向代理了多台真实的业务处理服务器。 负载均衡 这里提到的客户端发送的、Nginx反向代理服务器接收到的请求数量，就是我们说的负载量 请求数量按照一定的规则进行分发，到不同的服务器处理的规则，就是一种均衡规则/ 所以将服务器接收到的请求按照规则分发的过程，称为负载均衡。 负载均衡在实际项目操作过程中，有硬件负载均衡和软件负载均衡两种，硬件负载均衡也称为硬负载，如F5负载均衡，相对造价昂贵成本较高 但是数据稳定性安全性等等有非常好的保障，如中国移动中国联通这样的公司才会选择硬负载进行操作。 Nginx 支持的负载均衡调度算法方式如下： 1) weight轮询（默认)：接收到的请求按照顺序逐一分配到不同的后端服务器，即使在使用过程中，某一台后端服务器宕机，Nginx 会自动将该服务器剔除出队列，请求受理情况不会受到任何影响。 这种方式下，可以给不同的后端服务器设置一个权重值（weight），用于调整不同的服务器上请求的分配率。 权重数据越大，被分配到请求的几率越大；该权重值主要是针对实际工作环境中不同后端服务器一件配置进行调整的。 2）ip_hash：每个请求按照发起客户端的IP的hash结果进行匹配，这样的算法下一个固定IP地址的客户端总会访问到同一个后端服务器，这也在一定成都上解决了集群部署环境下Session共享的问题。 3） fair： 只能调整调度算法，动态的根据后端服务器的请求处理到响应的时间进行均衡分配。 响应时间短处理效率高的服务器分配到请求的概率高，响应时间长处理效率低的服务器分配到的请求少，它是结合了前两者的优点的一种调度算法。 但是需要注意的是Nginx默认不支持fair算法，如果要使用这种调度算法，请安装upstream_fair模块。 4） url_hash：按照访问的URL的hash结果分配请求，每个请求的URL会指向后端固定的某个服务器，可以在Nginx作为静态服务器的情况下提高缓存效率/ 同样要注意Nginx默认不支持这种调度算法，要使用的话需要安装Nginx的哈说软件包 Web 服务器对比 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Mac下安装nginx.html":{"url":"Tools/Mac下安装nginx.html","title":"Mac下安装nginx","keywords":"","body":"Mac下安装nginx 必备工具 homebrewhttps://brew.sh/index_zh-cn.html（安装方法自行搜索） 安装完毕后代开命令行 步骤一：先更新homebrew brew update 如果上面操作长时间没有任何动静，请更换镜像，参考清华镜像 https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/ 步骤二： 查看nginx信息 brew search nginx 步骤三： 安装nginx brew install nginx 安装完毕 主页的文件在/usr/local/var/www 文件夹下 对应的配置文件地址在/usr/local/etc/nginx/nginx.conf 步骤四：运行nginx nginx nginx默认使用8080端口 如果发现端口被占用了，可以杀掉使用使用改端口的进程，也可以修改/usr/local/etc/nginx/nginx.conf 下的 http { server { listen 8181; server_name localhost; #charset koi8-r; ..... } } 重新启动nginx nginx -s reload 成功看到欢迎页面～！（对应的html是/usr/local/var/www/index.html） By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/Mac上安装Elasticsearch，基本操作.html":{"url":"Tools/Mac上安装Elasticsearch，基本操作.html","title":"Mac上安装Elasticsearch，基本操作","keywords":"","body":"Mac上安装Elasticsearch，基本操作 简介 Elasticsearch是一个高度可扩展的开源的分布式RESTFul全文搜索和分析引擎。它允许用户快速的（近实时的）存储、搜索和分析海量数据。他通常用作底层引擎技术，，为具有复杂搜索功能和要求的应用程序提供支持 一下是ES可用于的一些场景： 电商网站提供搜索功能：可使用ES来存储产品的目录和库存，并为他们提供搜索和自动填充建议 搜集日志和交易数据，并进行分析：可食用Logstash来搜集、聚合和解析数据，然后让Logstash将次数据提供给ES。然后可在ES中搜索和聚合开发者感兴趣的信息。 需要快速调查、分析、可视化查询大量数据的特定问题：可以使用ES存储数据，然后使用Kibana构建自定义仪表板，来可视化展示数据。还可以使用ES的聚合功能针对这些数据复杂的商业分析。 安装 在Mac上可以使用brew快速安装Elasticsearch 安装Elasticsearch brew install elasticsearch 安装完成后可使用elasticsearch --version查看ES版本信息 然后执行启动命令 elasticsearch 启动成功后，ES的默认端口是9200，可在浏览器中看到ES的基本信息 安装Kibana Kibana是ES的一个配套工具，让用户在网页中可以直接与ES进行交互。 安装命令： brew install kibana 安装完成后直接执行kibana命令启动 Kibana的默认端口是5601 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Tools/学习笔记一.html":{"url":"Tools/学习笔记一.html","title":"学习笔记一","keywords":"","body":"学习笔记一 什么是 Kubernetes？ Kubernetes是一个可移植的，可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。它拥有一个庞大且开素增长的生态系统。 Kubernetes的服务，支持和工具广泛可用。 容器部署时代 容器类似于VM，但是它们具有轻松的隔离属性，可以在应用程序之间共享操作系统（OS）。因此，容器被认为是轻质的。与VM相似，容齐具有自己的文件系统，CPU，内存，进程空间等。由于它们与基础架构分离，因此可以跨云和OS分发进行移植。容齐之所以受欢迎，是因为它提供了额外的好处，例如： 敏捷的应用程序创建和部署：与使用VM映像相比，容器映像创建的简便性和效率更高 持续的开发，集成和部署：通过快速简单的回滚（由于图像不可更改），提供可靠且频繁的容器映像构建和部署。 开发和运营的关注点分离：在构建/发布时而不是在部署时创建应用程序容器映像，从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他信号。 跨开发，测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 云和操作系统分发的可移植性：可在Ubuntu，RHEL，CoreOS，本地，Google Kubernetes Engine和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行OS到使用逻辑资源在OS上运行应用程序。 松散耦合，分布式，弹性，解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理–而不是在一台大型单机上运行的单片堆栈。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 为什么需要Kubernetes以及它能做什么 容器是捆绑和运行应用程序的好方法。在生产环境中，您需要管理运行应用程序的容器，并确保没有停机时间。例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是Kubernetes的救援方法！Kubernetes为您提供了一个可弹性运行分布式系统的框架。它负责应用程序的扩展和故障转移，提供部署模式等。例如，Kubernetes可以轻松管理系统的Canary部署。 Kubernetes为您提供： 服务发现和负载均衡 Kubernetes可以使用DNS名称或使用其自己的IP地址公开容器。如果到容器流量很高，Kubernetes可以负载平衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes允许您自动挂载您选择的存储系统，例如本地存储，公共云提供商等。 存储部署和回滚 您可以使用Kubernetes为部署的容器描述所需的状态，它可以以受控的速率将实际状态改为所需的状态。例如，您可以自动化Kubernetes来为您部署新容器，删除现有容器并将其所有资源用于新容器 自动垃圾箱打包 您为Kubernetes提供了一个节点集群，可用于运行容器化任务。您告诉Kubernetes每个容器需要多少CPU和内存（RAM）。 Kubernetes可以将容器安装到您的节点上，以充分利用您的资源。 自我修复的 Kubernetes会重启失败的容器，替换容器，杀死对用户定义的运行状况检查没有响应的容器，并且在准备好服务之前不会将其通告给客户端 秘密和配置管理 Kubernetes允许您存储和管理敏感信息，例如密码，OAuth令牌和ssh密钥。您可以部署和更新机密和应用程序配置，而无需重建容器映像，也无需在堆栈配置中公开机密 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/":{"url":"React/","title":"React","keywords":"","body":"React React总结 React高级开发 umi-request返回拦截处理 在隔离中开发组件 React滚动加载实现 基于ReactCSSTransitionGroup实现react-router过渡动画 文本转换为markdown实现 react项目中import文件路径优化 React组件开发指南 react通过脚手架创建项目 componentWillReceiveProps的使用 ant-design-pro-layout底部Footer更改 React生命周期 React-routerv4路由配置方法 服务端渲染SSR By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/summery.html":{"url":"React/summery.html","title":"React 总结","keywords":"","body":"React 组件化 react组件有dom视图和state组成，state是数据中心，它的状态决定着视图的状态。react只负责UI的渲染，与其他框架监听数据动态改变DOM不同，React采用setState来控制视图的更新。setState会自动调用render函数，触发视图的重新渲染，如果仅仅是state数据的变化二没有调用setState，并不会触发更新。说到组件，就必须了解react组件的生命周期 组件之间通信 react是单向数据流，自上而下的传递数据。解决复杂组件之间通信的方法很多。一般父子组件通信是最简单的，父组件将一个回调函数传递给子组件，子组件通过this.props直接调用该函数与父组件通信。 如果组件之间嵌套很深，可以使用上下文getChildContext来传递信息，这样在不需要将函数一层层往下传，任何一层的子级都可以通过this.context直接访问，react-redux内部实现就是利用此方法。 兄弟组件之间无法直接通信，它们需要利用同一层的上级作为中转站。 Redux redux不是必须的，如果不是复杂的组件通信，逻辑简单，用context就行。redux并不是react特有的，其他框架也可以使用redux。简单说说redux。redux由三部分组成：store、reducer、action store是一个对象，它主要有三个方法： dispatch 用于action分发，当action传入dispatch会立即执行，有时候我们不想它立刻触发，可以在createStore中使用middleware中间件对dispatch进行改造，例如redux-thunk，不过这是react-redux做的事情了 subscribe 顾名思义，监听器，监听state的变化，这个函数在store调用dispatch时会注册一个listener监听state变化。 getState 获取store中的state，当我们用action触发reducer改变了state时，需要拿到新的state里面的数据。getState在两个地方会用到，一是通过dispatch提交action后store需要拿到state里面的数据，二是利用subscribe监听到state发生变化后调用它来获取新的state数据。 说了这么多，store的核心代码其实很短： function createStore(reducer) { let state = null const listeners = [] const subscribe = listener => listeners.push(listener) const getState = () => state const dispatch = action => { // 覆盖原对象 state = reducer(state, action) listeners.forEach(listener => listener()) } // 初始化state dispatch({}) return { getState, dispatch, subscribe } } 另一部分，reducer一个纯函数（pure function），它接收一个state和action作为参数，根据action的type返回一个新的state，简单实现如下： const reducer = (state, action) => { if (!state) { return { title: \"这是reducer初始化的标题\", content: \"这是reducer初始化的内容\" } } switch(action.type) { case 'CHANGE_TITLE': return { ...state, title: action.newTitle } default: return state; } } export default reducer action比较简单，它返回一个对象，其中type属性是必须的，同时也可以传入一些其他的数据。 使用例子如下： constructor(){ super() // state更新重新渲染 store.subscribe(() => { this.setState(store.getState()) }) } React-Redux react-redux则是对redux做了分装，可以在react中直接使用，并且提供了Provider和connect。 Provider是一个组件，它接受store作为props，然后通过context往下传，这样react中任何组件都可以通过context获取store conntect是一个函数，也是一个高阶组件（HOC），通过传入state和dispatch返回一个新的组件，它的写法是如下 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/React高级开发.html":{"url":"React/React高级开发.html","title":"React 高级开发","keywords":"","body":"React和第三方库 1. React中npm包：命令、包管理流程 fetch 包管理工具 包管理流程2. 主流开发包：moment 纯lib插件：不涉及任何UI 时间格式 import moment from 'moment' // 放到全局，外部也可以使用 window.moment = moment 标准格式: moment('2018-09-22', 'YYYY-MM-DD') By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/umi-request返回拦截处理.html":{"url":"React/umi-request返回拦截处理.html","title":"umi-request 返回拦截处理","keywords":"","body":"场景： 在使用ant-design-pro框架开发前端服务的的时候，官方推荐使用umi-request作请求库，在对request封装文件中，想对后台接口的相应进行一些统一的处理，设置响应拦截器的时候，发现拿不到后台返回的数据 解决方法 拦截器中用response.clone处理完成后把response返回 官网：umi-request By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/在隔离中开发组件.html":{"url":"React/在隔离中开发组件.html","title":"在隔离中开发组件","keywords":"","body":"通常，在应用程序中，您有许多UI组件，并且每个组件都有许多不同的状态。例如，一个简单的按钮组件可以具有以下状态： 在常规状态下，带有文本标签。 在禁用模式下。 处于加载状态。 通常，如果没有运行示例应用程序或一些示例，很难看到这些状态。 默认情况下，创建React App不包含任何工具，但您可以轻松地将Storybook for React（源代码）或React Styleguidist（源代码）添加到项目中。这些是第三方工具，可让您开发组件并独立于应用程序查看其所有状态。 您还可以将Storybook或样式指南部署为静态应用程序。这样，团队中的每个人都可以查看和查看UI组件的不同状态，而无需启动后端服务器或在应用中创建帐户。 Storybook Storybook是React UI组件的开发环境。它允许您浏览组件库，查看每个组件的不同状态，以及交互式开发和测试组件。 在应用程序目录中运行以下命令： npx -p @storybook/cli sb init 之后，请按照屏幕上的说明进行操作。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/React滚动加载实现.html":{"url":"React/React滚动加载实现.html","title":"React 滚动加载实现","keywords":"","body":"场景 在我个人博客中文章列表，采用滚动加载的方式显示文章 componentDidMount() { if (this.props.location.pathname === '/hot') { this.setState( { likes: true, }, () => { this.handleSearch(); }, ); } else { this.handleSearch(); } window.onscroll = () => { if (getScrollTop() + getWindowHeight() > getDocumentHeight() - 100) { // 如果不是已经没有数据了，都可以继续滚动加载 if (this.state.isLoadEnd === false && this.state.isLoading === false) { this.handleSearch(); } } }; document.addEventListener('scroll', lazyload); } 为当前页面的页面滚动事件添加处理函数。 window.onscroll = funcRef 获取页面顶部被卷起来的高度 export function getScrollTop() { return Math.max( //chrome document.body.scrollTop, //firefox/IE document.documentElement.scrollTop, ); } 获取浏览器视口的高度 export function getWindowHeight() { return document.compatMode === 'CSS1Compat' ? document.documentElement.clientHeight : document.body.clientHeight; } 获取页面文档的总高度 export function getDocumentHeight() { //现代浏览器（IE9+和其他浏览器）和IE8的document.body.scrollHeight和document.documentElement.scrollHeight都可以 return Math.max( document.body.scrollHeight, document.documentElement.scrollHeight, ); } lazyload // 用新的 throttle 包装 scroll 的回调 const lazyload = throttle(() => { // 获取所有的图片标签 const imgs = document.querySelectorAll('#list .wrap-img img'); // num 用于统计当前显示到了哪一张图片，避免每次都从第一张图片开始检查是否露出 let num = 0; for (let i = num; i = 100) { // 给元素写入真实的 src，展示图片 let hasLaySrc = imgs[i].getAttribute('data-has-lazy-src'); if (hasLaySrc === 'false') { imgs[i].src = imgs[i].getAttribute('data-src'); imgs[i].setAttribute('data-has-lazy-src', true); } // 前 i 张图片已经加载完毕，下次从第 i+1 张开始检查是否露出 num = i + 1; } } }, 1000); throttle fn是我们需要包装的事件回调, delay是时间间隔的阈值 export function throttle(fn, delay) { // last为上一次触发回调的时间, timer是定时器 let last = 0, timer = null; // 将throttle处理结果当作函数返回 return function() { // 保留调用时的this上下文 let context = this; // 保留调用时传入的参数 let args = arguments; // 记录本次触发回调的时间 let now = +new Date(); // 判断上次触发的时间和本次触发的时间差是否小于时间间隔的阈值 if (now - last By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/基于ReactCSSTransitionGroup实现react-router过渡动画.html":{"url":"React/基于ReactCSSTransitionGroup实现react-router过渡动画.html","title":"基于ReactCSSTransitionGroup 实现 react-router 过渡动画","keywords":"","body":"https://yuerblog.cc/2016/11/15/transition-animation-with-reactcsstransitiongroup-and-react-router/ By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/文本转换为markdown实现.html":{"url":"React/文本转换为markdown实现.html","title":"文本转换为 markdown 实现","keywords":"","body":"将后端返回的文本转换为markdown，显示为HTML页面 转换为markdown //markdown.js // https://www.cherylgood.cn/detail/5bdaf4722382b4646c27143b.html const highlight = require(\"highlight.js\"); const marked = require(\"marked\"); const tocObj = { add: function (text, level) { var anchor = `#toc${level}${++this.index}`; this.toc.push({ anchor: anchor, level: level, text: text }); return anchor; }, toHTML: function () { let levelStack = []; let result = \"\"; const addStartUL = () => { result += ''; }; const addEndUL = () => { result += \"\\n\"; }; const addLI = (anchor, text) => { result += '' + text + \"\\n\"; }; this.toc.forEach(function (item) { let levelIndex = levelStack.indexOf(item.level); // 没有找到相应level的ul标签，则将li放入新增的ul中 if (levelIndex === -1) { levelStack.unshift(item.level); addStartUL(); addLI(item.anchor, item.text); } // 找到了相应level的ul标签，并且在栈顶的位置则直接将li放在此ul下 else if (levelIndex === 0) { addLI(item.anchor, item.text); } // 找到了相应level的ul标签，但是不在栈顶位置，需要将之前的所有level出栈并且打上闭合标签，最后新增li else { while (levelIndex--) { levelStack.shift(); addEndUL(); } addLI(item.anchor, item.text); } }); // 如果栈中还有level，全部出栈打上闭合标签 while (levelStack.length) { levelStack.shift(); addEndUL(); } this.toc = []; this.index = 0; return result; }, toc: [], index: 0 }; class MarkUtils { constructor() { this.rendererMD = new marked.Renderer(); this.rendererMD.heading = function (text, level, raw) { var anchor = tocObj.add(text, level); return `${text}\\n`; }; this.rendererMD.table = function (header, body) { return '' + header + body + '' } highlight.configure({ useBR: true }); marked.setOptions({ renderer: this.rendererMD, headerIds: false, gfm: true, tables: true, breaks: false, pedantic: false, sanitize: false, smartLists: true, smartypants: false, highlight: function (code) { return highlight.highlightAuto(code).value; } }); } async marked(data) { if (data) { let content = await marked(data); let toc = tocObj.toHTML(); return { content: content, toc: toc }; } else { return null; } } } const markdown = new MarkUtils(); export default markdown; 调用转换 // 在其他文件中引用 const contentmd = markdown.marked(content); contentmd.then(md => { // 进一步处理 })； 页面渲染处理 // 示例 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/react项目中import文件路径优化.html":{"url":"React/react项目中import文件路径优化.html","title":"react 项目中 import 文件路径优化","keywords":"","body":"使用create-react-app初始化react应用 mkdir test && cd test create-react-app 文件import ..from '../../../'， from的路径引用优化，@表示src当前路径，修改配置文件./config/webpack.config.js，查找文件的alias： alias: { // Support React Native Web // https://www.smashingmagazine.com/2016/08/a-glimpse-into-the-future-with-react-native-for-web/ 'react-native': 'react-native-web', }, 在alias对象中加入一行配置 '@': path.join(__dirname, '..', 'src'), 完整的alias： alias: { // Support React Native Web // https://www.smashingmagazine.com/2016/08/a-glimpse-into-the-future-with-react-native-for-web/ 'react-native': 'react-native-web', '@': path.join(__dirname, '..', 'src'), }, 使用create-umi脚手架搭建react项目 mkdir test && cd test create-umi 项目初始化以后，根目录下的文件jsconfig.json中已经配置好了路径 \"paths\": { \"@/*\": [\"./src/*\"] } 所以开发中可直接使用'@/' By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/React组件开发指南.html":{"url":"React/React组件开发指南.html","title":"React 组件开发指南","keywords":"","body":"轻松开发一个react组件 很多前端开发者在开发react组件的时候往往不知从何下手，如果你也有此困惑，那么读完本文肯定让你拨云见日立即手撸一个组件出来，这里将使用Damujiangr开发的yeoman脚手架轻松搭建一个react组件开发工程 建议在github或gitlab上创建工程，然后clone到本地，再执行以下操作，这样可以保证在生成package.json时自动填充repository、bugs、homepage等git仓库信息字段 首先是工具安装 [sudo] npm install -g yo # yo工具安装 [sudo] npm install -g generator-react-component-magic # template安装 yo react-component-magic # 工程初始化 node.js版本建议选择8.x，windows系统可能有webpack plugin的兼容问题 ├── doc // 使用文档 ├── examples // 示例代码 ├── package.json ├── rollup.config.js ├── src // 源代码 ├── style // 源代码样式 └── webpack.config.js src目录结构 ├── __tests__ // 测试目录 ├── components // 组件目录 │ ├── Block.js // 单一组件 │ └── __tests__ // 测试目录 └── index.js // 主入口 复制代码 此结构支持单一组件的开发和导出，如图例中的Block.js，也支持多个组件同时开发和导出，在\bcomponent下创建文件夹或者新的文件即可，只要在index.js中组织export的方式即可 单一文件导出示例 import Block from './components/Block'; export default Block; 多个组件导出示例 export { default as Block } from './components/Block'; export { default as Other } from './components/Other'; export { default as More } from './components/More'; 代码规范约束使用的eslint，遵循airbnb代码规范 rollup.config.js说明 作用是导出组件的外链版本，供在HTML中通过srcipt:src方式引入 在配置中可以通过修改output.name指定组件对外暴露的变量名 webpack.config.js 作用是通过npm start运行本地开发时的一些配置 在配置中可以通过resolve.alias指定npm包名 执行npm run build会构建出组件代码，用于上传NPM，目录如下，其他style源代码也是生产代码 ├── dist // 产物外链版本 ├── esm // 产物ES6版本 ├── lib // 产物ES5版本 └── style // 源代码样式 在package.json中已经配置好入口： \"main\": \"lib/index.js\", // 一般情况下的主入口 \"module\": \"esm/index.js\" // 提案，用于引入ES Module的入口 本地开发测试和测试脚本 测试对组件开发来说是非常必要的，本例中可以在example中可以直接通过import方式引入正在开发的组件，进行本地测试 在example/app.js引入组件 import 'react-component-template/style/component.css';// eslint-disable-line import Block from 'react-component-template/src'; // eslint-disable-line 通过npm start命令自动打开浏览器并打开热更新功能，可以方便的进行本地开发测试 测试框架使用的是Jest,可以在每级目录的test目录下编写对接的测试文件，执行npm run test命令即可得到测试结果，代码覆盖率报告使用的codecov, 如果你的工程上传到github，并配置好travis CI，可以通过codecov上传测试报告 但是我在执行npm run start时报错了，网上查到了解决方法 解决方案 上传NPM 首先要有一个npm账号 npm login npm version <> npm publish 到此为止，你就可以愉快地开发属于你自己的react组件了，并且可以发布到NPM上分享给其他人 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/react通过脚手架创建项目.html":{"url":"React/react通过脚手架创建项目.html","title":"react 通过脚手架创建项目","keywords":"","body":"官方 介绍create-umi umi通过create-umi提供脚手架能力，包含： project，通用项目脚手架，支持选择是否启用 TypeScript，以及 umi-plugin-react 包含的功能 ant-design-pro，仅包含 ant-design-pro 布局的脚手架，具体页面可通过 umi block 添加 block，区块脚手架 plugin，插件脚手架 library，依赖（组件）库脚手架，基于 umi-plugin-library npm install create-umi -g create-umi By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/componentWillReceiveProps的使用.html":{"url":"React/componentWillReceiveProps的使用.html","title":"componentWillReceiveProps 的使用","keywords":"","body":"什么时componentWillReceiveProps？ componentWillReceiveProps是\bReact生命周期中的一个环节，有关React的生命周期，同学们可以在这里详细了解。 componentWillReceiveProps在初始化render的时候不会执行，它会在Component接受到新的状态(Props)时被触发，一般用于父组件状态更新时子组件的重新渲染。这个东西十分好用，但是一旦用错也会造成十分严重的后果。 在componentWillReceiveProps这个回调函数中，我们可以获取到就是props，通过this.props来获取，然后新的props则是通过函数的参数传入，在这里我们可以比较两个props从而对本组件的state作出安全的变更然后重新渲染我们的组件或者是触发\b子组件内的某些方法。 //这种方式十分适合父子组件\b的互动，通常是父组件需要通过某些状态控制子组件渲染亦或销毁... componentWillReceiveProps(nextProps) {//componentWillReceiveProps方法中第一个参数代表即将传入的新的Props if (this.props.sharecard_show !== nextProps.sharecard_show){ //在这里我们仍可以通过this.props来获取旧的外部状态 //通过新旧状态的对比，来决定是否进行其他方法 if (nextProps.sharecard_show){ this.handleGetCard(); } } } 上面是componentWillReceiveProps常用的方式，但是如果使用不当可能会导致以下后果，一般体现为组件陷入渲染死循环，他会一直接受新的外部状态导致自身一直在重渲染。 componentWillReceiveProps(nextProps) { if (this.props.sharecard_show !== nextProps.sharecard_show){ if (nextProps.sharecard_show){ this.handleGetCard(); } } } handleGetCard() { this.props.test() } //父组件内 test() { this.setState({ sharecard_show: !this.state.sharecard_show }) } 一旦项目中出现这样的代码，有很大几率就会陷入死循环，这两个组件会一直在传递状态并且重新渲染，然后页面估计就卡挂了。这是其中一个需要注意的地方，另外我们需要谨记，在componentWillReceiveProps中想作任何变更最好都将两个状态进行比较，假如状态\b有异才执行下一步。不然容易造成组件的多次渲染，并且这些渲染都是没有意义的。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/ant-design-pro-layout底部Footer更改.html":{"url":"React/ant-design-pro-layout底部Footer更改.html","title":"ant-design-pro-layout 底部 Footer 更改","keywords":"","body":"ant-design-pro-layout底部Footer更改 场景 项目中使用ant-design-pro脚手架初始化项目，底部Footer更改 方案 https://github.com/ant-design/ant-design-pro-layout/issues/31 github上issues上找到相同问题，但目前还没有看到支持Footer，我目前这样解决的 const footerRender = (_, defaultDom) => { if (!isAntDesignPro()) { return React.cloneElement(defaultDom, { links: [], copyright: \"2019 created by coco\" }) } return ( <> {defaultDom} ); }; 将文件./utils/utils.js的函数return 改为false，if里的ANT_DESIGN_PRO_ONLY_DO_NOT_USE_IN_YOUR_PRODUCTION改为自定义的BLOG_ADMIN export const isAntDesignPro = () => { if (BLOG_ADMIN === 'site') { return false; } return window.location.hostname === 'preview.pro.ant.design'; }; By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/React生命周期.html":{"url":"React/React生命周期.html","title":"React 生命周期","keywords":"","body":"React生命周期 总体看下React的生命周期图 React16废弃的三个生命周期函数 componentWillMount componentWillReceiveProps componentWillUpdate 取而代之的是两个新的生命周期函数 static getDrivedStateFromProps getSnapshotBeforeUpdate 我们将React的生命周期分为三个阶段，然后详细讲解每个阶段具体调用了什么函数，这三个阶段是： 挂载阶段 更新阶段 卸载阶段 挂载阶段 挂载阶段，也可以理解为组件的初始化阶段，就是我们的组件插入到DOM中，只会发生一次 这个阶段的生命周期函数调用如下： constructor getDerivedStateFromProps componentWillMount/UNSAVE_componentWillMount render componentDidMount constructor 组件构造函数，第一个被执行 如果没有显示定义它，我们会拥有一个默认的构造函数 如果显示定义了构造函数，我们必须在构造函数第一行执行super（props）,否则我们无法在构造函数里拿到this对象，这些否属于ES6的知识 在构造函数里面我们一般会做两件事： 初始化state对象 给自定义方法绑定this（或使用箭头函数，这里不需要绑定） 禁止在构造函数中调用setState，可以直接给state设置初始值 getDerivedStateFromProps static getDerivedStateFromProps(nextProps, preState) 一个静态方法，所以不能在这个函数里面使用this，这个函数有两个参数props和state，分别接受到的新参数和当前的state对象，这个函数会返回一个对象用来更新当前的state对象，如果不需要更新可以返回null 该函数会在挂载时，接收到新的props，调用了setState和forceUpdate时被调用 在Reactv16.3时只有在挂载时和接收到新的props被调用， 据说这是官方的失误，后来修复了 这个方法就是为了取而代之之前的componentWillMount、componentWillReceiveProps、componentWillUpdate 当我们接受到新的属性想去修改我们的state，可以使用getDerivedStateFromProps class ExampleComponent extends React.Component { state = { isScrollingDown: false, lastRow: null } static getDerivedStateFromProps(nextProps, prevState) { if (nextProps.currentRow !== prevState.lastRow) { return { isScrollingDown: nextProps.currentRow > prevState.lastRow, lastRow: nextProps.currentRow } } return null } } componentWillMount/UNSAFE_componentWillMount 在16版本这两个方法并存，但是在17版本componentWillMount，目的是为了向下兼容，对于新的应用，用getDerivedStateFromProps代替它们 由于componentWillMount/UNSAFE_componentWillMount是在render之前调用，所以就算在这个方法中调用setState也不会触发重新渲染 render React中最核心的方法，一个组件中必须要有这个方法 返回的类型有一下集中： 原生的DOM，如div React组件 Fragment（片段） Portals（插槽） 字符串和数字，被渲染成text节点 Boolean和null，不会渲染任何东西 关于Fragment和Portals是React16新增的，如果大家不清楚可以去阅读官方文档，在这里就不展开了 render函数是纯函数，里面只做一件事情，就是返回需要渲染的东西，不应该包含其他的业务逻辑，如数据请求，对于这些业务逻辑请移到componentDidMount和componentDid Update中 componentDidMount 组件装在之后调用，此时我们可以获取到DOM节点并操作，比如对canvas，svg的操作，服务器请求，订阅都可以写在这个里面，但是记得componentWillUnmount取消订阅 componentDidMount() { const { progressCanvas, progressSVG } = this const canvas = progressCanvas.current const ctx = canvas.getContext('2d') canvas.width = canvas.getBoundingClientRect().width canvas.height = canvas.getBoundingClientRect().height const svg = progressSVG.current const rect = document.createElementNS('http://www.w3.org/2000/svg', 'rect') rect.setAttribute('x', 0) rect.setAttribute('y', 0) rect.setAttribute('width', 0) rect.setAttribute('height', svg.getBoundingClientRect().height) rect.setAttribute('style', 'fill:red') const animate = document.createElementNS('http://www.w3.org/2000/svg', 'animate') animate.setAttribute('attributeName', 'width') animate.setAttribute('from', 0) animate.setAttribute('to', svg.getBoundingClientRect().width) animate.setAttribute('begin', '0ms') animate.setAttribute('dur', '1684ms') animate.setAttribute('repeatCount', 'indefinite') animate.setAttribute('calcMode', 'linear') rect.appendChild(animate) svg.appendChild(rect) svg.pauseAnimations() this.canvas = canvas this.svg = svg this.ctx = ctx } 在componentDidMount中调用setState会出发一次额外的渲染，多调用了一次render函数，但是用户对此没有感知，因为它是在浏览器刷新屏幕前执行，但是我们应该在constructor中初始化我们的state对下行，而不应该在componentDidMount调用state方法 更新阶段 更新阶段，当组建的props改变了，或组件内部调用了setState或着forceUpdate发生，会发生多次这个阶段的生命周期函数调用如下： componentWillReceiveProps/UNSAFE_componentWillReceiveProps getDerivedStateFromProps shouldComponentUpdate componentWillUpdate/UNSAFE_componentWillUpdate render getSnapshotBeforeUpdate componentDidUpdate componentWillReceiveProps/UNSAFE_componentWillReceiveProps componentWillReceiveProps(nextProps, prevState) UNSAFE_componentWillReceiveProps(nextProps, prevState) 在16版本这两个方法并存，但是在17版本中componentWillReceiveProps被删除，UNSAFE_componentWillReceiveProps，目的是为了做向下兼容，对于新的应用，用getDerivedStateFromProps代替它们 注意，当我们父组件重新渲染的时候，也会导致我们的子组件调用componentWillReceiveProps/UNSAFE_componentWillReceiveProps，即使我们的属性和之前的一样，所以需要我们在这个方法里面去进行判断，如果前后属性不一致才去调用setState 在装载阶段这两个函数不会被触发，在组件内部调用了setState和forceUpdate也不会触发这两个函数 getDerivedStateFromProps 这个方法在装载阶段已经讲过了，这里不再赘述，记住在更新阶段，无论我们接收到新的属性，调用了setState还是调用了forceUpdate，这个方法都会被调用 shouldComponentUpdate shouldComponentUpdate(nextProps, nextState) 有两个参数nextProps和nextState，表示新的属性和变化之后的state，返回一个布尔值，true表示会触发重新渲染，false表示不会触发重新渲染，默认返回true 注意当我们调用forceUpdate并不会触发此方法 因为默认是返回true，也就是只要接收到新的属性和调用了setState都会触发重新的渲染，这会带来一定的性能问题，所以我们需要将this.props与nextProps以及this.state与nextState进行比较来决定是否返回false，来减少重新渲染 但是官方提倡我们使用PureComponent来减少重新渲染的次数而不是手工编写shouldComponentUpdate代码，具体该怎么选择，全凭开发者自己选择 componentWillUpdate/UNSAFE_componentWillUpdate componentWillUpdate(nextProps, nextState) UNSAFE_componentWillUpdate(nextProps, nextState) 在16版本这两个方法并存，但是在17版本中componentWillUpdate被删除，UNSAFE_componentWillUpdate，目的是为了做向下兼容 在这个方法里，你不能调用setState，因为能走到这个方法，说明shouldComponentUpdate返回true，此时下一个state状态已经被确定，马上就要执行render重新渲染了，否则会导致整个生命周期混乱，在这里也不能请求一些网络数据，因为在异步渲染中，可能会导致网络请求多次，引起一些性能问题， 如果你在这个方法里保存了滚动位置，也是不准确的，还是因为异步渲染的问题，如果你非要获取滚动位置的话，请在getSnapshotBeforeUpdate调用 render 更新阶段也会触发，装载阶段已经讲过了，不再赘述 getSnapshotBeforeUpdate getSnapshotBeforeUpdate(prevProps, prevState) 这个方法在render之后，componentDidUpdate之前调用，有两个参数prevProps和prevState，表示之前的属性和之前的state，这个函数有一个返回值，会作为第三个参数传给componentDidUpdate，如果你不想要返回值，请返回null，不写的话控制台会有警告 前面说过这个方法时用来代替componentWillUpdate/UNSAVE_componentWillUpdate，下面举个例子说明下： class ScrollingList extends React.Component { constructor(props) { super(props); this.listRef = React.createRef(); } getSnapshotBeforeUpdate(prevProps, prevState) { // Are we adding new items to the list? // Capture the scroll position so we can adjust scroll later. if (prevProps.list.length {/* ...contents... */} ); } } componentDidUpdate componentDidUpdate(prevProps, prevState, snapshot) 该方法在getSnapshotBeforeUpdate方法之后被调用，有三个参数prevProps，prevState，snapshot，表示之前的props，之前的state，和snapshot。第三个参数是getSnapshotBeforeUpdate返回的 在这个函数里我们可以操作DOM，和发起服务器请求，还可以setState，但是注意一定要用if语句控制，否则会导致无限循环 这个函数里我们可以操作DOM，和发起服务器请求，还可以setState，但是注意一定要用if语句控制，否则会导致无限循环 卸载阶段 卸载阶段，当我们的组件被卸载或者销毁了 这个阶段的身影周期函数只有一个： componentWillUnmount componentWillUnmount 当我们的组件被卸载或者销毁了就会调用过，我们可以在这个函数里清楚一些定时器，取消网络请求，清理无效的DOM元素等垃圾清理工作 注意不要在这个函数里调用setState，因为组件不会重新渲染了 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/React-routerv4路由配置方法.html":{"url":"React/React-routerv4路由配置方法.html","title":"React-routerv4 路由配置方法","keywords":"","body":"React-routerv4路由配置方法 转载 react-router v4 的到来已经过了一些时日了，注意到官方文档中有篇介绍其新式设计哲学的文章。其中区分了传统静态路由（Static Routing）和新版中引入的动态路由（Dynamic Routing）。 动态路由么，Interesting（呵呵）。 听起来并不是很新鲜，但其实想当然了。这里面存在一个理解误区。需要注意这里定义的 Dynamic Routing，他不是指初始化路由的时候可以加点业务逻辑去动态生成路由配置，也不是指通过路由参数（query）可以动态控制页面的输出。它抛开了传统静态路由在程序渲染前定义好的做法，采用程序在渲染过程中动态生成的方式。 静态路由/Static Routing 先来看我们熟悉并惯用的传统静态路由。 React.render(( ), document.body) 这里路由进行集中配置，UI 与路由强绑定。 传统静态路由是在页面渲染前声明配置好的。像 Angular，Express, Ember，都是这种方式，如果使用过其中某个框架，一定不会陌生。react-router v4 之前的版本，也是沿袭这一方式的。 静态路由的问题 v4 版本将整个库进行重构，并不是拍脑袋决定的。按照官方文档上的记载，Michael 和 Ryan （库的核心作者）感觉现有的实现严重地受 React API 的制约，并且实现方式也不够优雅（后面会讲）。反正就是写起来用起来都不爽。 v4 版本将整个库进行重构，并不是拍脑袋决定的。按照官方文档上的记载，Michael 和 Ryan （库的核心作者）感觉现有的实现严重地受 React API 的制约，并且实现方式也不够优雅（后面会讲）。反正就是写起来用起来都不爽。 缘，妙不可言。 两人心照不宣地想到了一块，都从这次工作坊的分享内容中获得了灵感，即 v4 中引入的动态路由。 所以这次重构是线下碰撞出来的火花。 那么，之前的静态路由，到底有什么问题。本来用得好好的，到这里，不免心生疑惑。诚然，要说问题（Gotchas），那肯定是有的。没有完美的库解决所有人的问题，不然你以为这 4000+ 的 issue 数怎么来的。 话题回到静态路由上来。基于静态配置的方式，让版本来到 v3 的 react-router，难以为继更加复杂的功能，而我们从该库身上期望的还有很多。前面 issue 中，只是用户的问题，作为库的开发维护者，所考虑的是功能实现及程序扩展性的问题。如果不是核心贡献者，想必很难体味其中的韵味。所以在 v4.0.0-beta 这个开发分支的 README 中，react-router 团队对面临的挑战和存在的问题进行了简要阐述，顺便答一些疑解一些惑（FAQ 秀一波）。 其中Why a major version bump? 这一段便说出了现在的一些痛点。 路由写法则需要满足约定的格式，你甚至都不能将 脱离 在组件中任意组合。坦白地说，这一点也不 React。React 理念是可以声明式灵活性进行组件组装的（Declarative Composability）。 // NOPE! const CoolRoute = (props) => 因为其实 接管了你的组件，内部它实现了createElement，render和createRouterMiddleware等方法。同时，也需要提供组件生命周期回调，onEnter,onLeave及onChange，而这些生命周期React本身就有componentWillMount, componentWillReceiveProps 及 componentWillUnmount。 v3 Router.js const propTypes = { history: object, children: routes, routes, // alias for children render: func, createElement: func, onError: func, onUpdate: func, // PRIVATE: For client-side rehydration of server match. matchContext: object } 配置路由的过程也是程序结构重现的过程，一级一级的嵌套，和组件在洁面的实图大致对应。但其实我们在写页面过程中，堆积组件时，就已经体现出程序的结构层级了。 为了适应代码拆分（code split），引入legetComponent和getChildRoutes。为了支持热替换（hot module replacement），又得整一堆很hack的东西出来 ... 所以之前应该是方向上出了点偏差。要是接下来的 v4 版本没能解决这些问题，是要负责任的（我指的是对社区）。 动态路由 怎么讲？就是不存在路由的概念了，当然也就无需路由配置了。 那我们还怎么切页面？ 通过控制组件的渲染，来切页面。 react-router v4 中，请把路由看成普通的 React 组件，传递 props 来正常使用（之前会忽略掉这些属性），借助它来控制组件的展现。展示的逻辑及权利回归到了组件本身，回归到了熟悉的 React。这样，没有了静态配置的路由规则，取而代之的是程序在运行渲染过程中动态控制的展现。 这便是 v4 中称之为的动态路由。 当路由 回归组件后，react-router 提供的那些 API，像上面讨论的那些，就不复存在了，因为它是普通的 React 组件，它的 API 就是 React 提供的那些 API。 曾记否，Linus 大神的警世箴言「Talk is cheap. Show me the code.」所以接下来看一段示例代码，以展示两个路由页的程序在 v3 与 v4 中分别如何实现。 下面的代码来自文章All About React Router 4 v3: import { Router, Route, IndexRoute } from 'react-router' const PrimaryLayout = props => ( Our React Router 3 App {props.children} ) const HomePage =() => Home Page const UsersPage = () => Users Page const App = () => ( ) v4: import { BrowserRouter, Route } from 'react-router-dom' const PrimaryLayout = () => ( Our React Router 4 App ) const HomePage =() => Home Page const UsersPage = () => Users Page const App = () => ( ) render(, document.getElementById('root')) v4 中react-router仓库拆分成了许多个包进行发布 react-router路由基础库 react-router-dom 浏览器中使用的封装 react-router-native React native封装 示例中BrowserRouter组件便来自react-router-dom这个包 如果之前静态路由的观念根植于心中，那么在 v4 的版本中，似乎看不到路由的影子了，它穿插在了各组件中。这一点在需要嵌套路由（Nested Routes）的场景时尤为明显。 来自官方文档的示例代码 const App = () => ( {/* here's a div */} {/* here's a Route */} ) // when the url matches `/tacos` this component renders const Tacos = ({ match }) => ( // here's a nested div {/* here's a nested Route, match.url helps us make a relative path */} ) 上面， 组件展示与否，取决于当前路由是否与 /tacos 匹配。而 则可以理解为一个容器型组件，它做的事情很简单，将传入的 path 与当前的 location 比较，匹配则渲染 component 属性传入的组件，否则 return null（实际实现以源码为准）。 由 react-router v3 转到 v4，在思想上的转变还是需要些时间来磨合。正如 react-router 刚发布时，对于将路由写在 jsx 中，我们是拒绝的。 响应式路由 说出来你可能不信，改为动态路由的实现方式后，其展现完全由组件控制，所以竟然可以和媒体查询（Media queries）结合，实现不同屏幕大小路由到不同的界面样式。官方的一个示例便展示了这一骚气操作。 const App = () => ( ) const Invoices = () => ( {/* always show the nav */} {screenIsSmall => screenIsSmall // small screen has no redirect ? // large screen does! : } ) react-media 动态路由的问题 尽说了优点，看起来很美好，那新版的动态路由实现方式，社区的人买不买帐，广大人民群众同意不同意呢？ 在我看来， 不够直观了。你无法知道程序中所有的路由（是时候造一个 sitemap 的轮子了）。 测试成了老大难的问题。组件中掺杂了路由逻辑，原本对针对组件的单元测试（功能层面）完全不需要知道路由的存在，而现在就要考虑了。正如文章You might not need React Router 下面的评论： 总结 本质上，可以看做是页面自己控制了页面，这个逻辑给到了每个组件本身，而不是由顶级路由配置来决定页面的展示，更加的灵活。路由回归到了组件的本质，拥抱更加纯正的 React，所以如果你在使用 v4 的时候还遇到什么问题的话，那大部分都应该是使用 React 能遇到的问题。 还有一点好处是基于原生 React 组件后，没有自己造的生涩轮子，搭建好基础平台后社区可以基于 React 贡献更多插件。 更加详尽的关于 react-router v4 的东西推荐阅读All About React Router 4（讲真我很奇怪这文章为什么发在了 css-tricks，黑人问号？） ，或者查看这个在线 Demo。 不像硬气的 iOS 每次都墙裂建议所有用户升级（结果证明越升越慢），v4 beta 文档里关于「Do I have to upgrade? 」的回答显得特别务实： 没有必要升级，v2/3 会持续维护（当然不会告诉你所谓的维护仅仅是指合并社区贡献的 bug fix）。不过新项目倒是可以把玩一下的，谁不喜新厌旧呢。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/服务端渲染SSR.html":{"url":"React/服务端渲染SSR.html","title":"服务端渲染 SSR","keywords":"","body":"服务端渲染SSR（React） 什么是客户端渲染？ react在客户端执行，消耗客户端性能。客户端渲染，页面初始加载的HTML页面中无网页展示内容，需要加载执行JavaScript文件中的React代码，通过JavaScript渲染生成页面，同时，JavaScript代码会完成页面交互事件的绑定。详细流程可参考下图 客户端渲染流程 浏览器发送请求–>服务器返回HTML–>浏览器发送bundle.js请求–>服务器返回bundle.js–>浏览器执行bundle.js中的react代码完成渲染 什么时服务端渲染？ 服务端渲染（Server-Side Render），是指将单页应用（SPA）在服务端渲染为HTML片段，发送到到浏览器，然后为期绑定状态与事件，成为完全可交互页面的过程。 服务端渲染（SSR），客户端渲染（CSR） 对比SPA站点和SSR站点在SEO区别： SSR优势在于： 更友好的SEO（Search Engine Optimization）：爬虫可以直接抓取渲染之后的页面，CSR首次返回的HTMPL文档中，是空节点（root），不包含内容。而SSR返回渲染之后的HTML片段，内容完整，所以能更好地被爬虫分析与索引 更快的首屏加载速度：无需等待JavaScript完成下载且执行才显示内容，更快捷地看到完整渲染的页面。有更好的用户体验 需要服务端支持，Umijs主要关注是应用UI曾渲染，完成SSR需要服务端（例如，node） 服务端渲染流程 浏览器发送请求->服务器运行React代码生成页面->服务器返回页面 什么是同构？ 一套react代码，在服务端执行一次，在客户端也执行一次。在服务端执行同构renderToString知识返回界面展示，并不能绑定事件，需要在客户端再次执行js代码绑定事件 服务器运行React代码渲染出HTML–>发送HTML给浏览器–>浏览器接收到内容展示–>浏览器加载js文件–>Js中的React代码在浏览器端重新执行–>JS中的React代码接管页面操作 路由同构 让路由在服务端、客户端各跑一遍 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"React/href属性使用js代码段在React下warning问题.html":{"url":"React/href属性使用js代码段在React下warning问题.html","title":"href属性使用js代码段在React下warning 问题","keywords":"","body":"href 属性使用 js 代码段在 React 下 warning 问题 我们在使用 a 标签的使用，若不想用 a 表爱你的 href 跳转，而要使用自己绑定的 click 或其他时间时，往往会通过插入 js 代码段的方式设置 href 为javascript:;或javascript:void(0);等达到 href 无跳转的结果。但是这么设置在 React 中会提示warning：A future version of React will block javascript: URLs as a security precaution. Use event handlers instead if you can. If you need to generate unsafe HTML try using dangerouslySetInnerHTML instead. React was passed \"javascript:;\"，这样的话，我们该如何处理。 处理方法 在 href 中插入代码段的方式是不可行的，转而可以使用以下方式： { e.preventDefault() } }> javascript:scroll(0, 0)跳转到顶部 处理方法： window.scrollTo(\"0\", \"0\");}> By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Charts/":{"url":"Charts/","title":"图表","keywords":"","body":"图表 三大图标库:ECharts、BizCharts和G2，该如何选择 three.js在React中的运用 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Charts/三大图标库:ECharts、BizCharts和G2，该如何选择.html":{"url":"Charts/三大图标库:ECharts、BizCharts和G2，该如何选择.html","title":"三大图标库:ECharts、BizCharts 和 G2，该如何选择","keywords":"","body":"阿里开源的BizCharts图标哭库基于React技术栈，各个图表项皆采用了组件的形式，贴近React的使用特点。同时BizCharts基于G2进行封装，BizCharts也继承了G2相关特性。公司目前同意使用的是ECharts图表库， 下文对3种图标库进行分析对比 BizCharts 官网地址：BizCharts 一、安装 通过npm/yarn 引入 npm install bizcharts --save yarn add bizcharts --save 二、引用 成功安装完成之后，即可使用import或require进行引用。 例子： import { Chart, Geom, Axis, Tooltip, Legend } from 'bizcharts'; import chartConfig from './assets/js/chartConfig'; 三、DataSet BizCharts中可以通过dataset（数据处理模块）来对图标数据进行处理，该方法继承自G2，在下文中将对此进行详细分析。 G2 BizCharts基于G2进行开发，在研究BizCharts的过程中页一起对G2进行了实践 一、安装 和BizCharts一样，可以通过npm/yarn 引入 npm install @antv/g2 --save yarn add @antv/g2 --save 与BizCharts不同，G2初始化数据并非以组件的形式引入，而是需要获取在某个DOM下初始化图表。获取该DOM的唯一属性ID之后，通过chart()进行初始化。 引用 示例： import React from 'react'; import G2 from '@antv/g2'; class g2 extends React.Component {constructor(props) { super(props); this.state = { data :[ { genre: 'Sports', sold: 275 }, { genre: 'Strategy', sold: 115 }, { genre: 'Action', sold: 120 }, { genre: 'Shooter', sold: 350 }, { genre: 'Other', sold: 150 } ] }; } componentDidMount() { const chart = new G2.Chart({ container: 'c1', // 指定图表容器 ID width: 600, // 指定图表宽度 height: 300 // 指定图表高度 }); chart.source(this.state.data); chart.interval().position('genre*sold').color('genre'); chart.render(); } render() { return ( ); } } export default g2; 三、DataSet DataSet主要有两方面的功能，解析数据（Connector）&加工数据（Transform）。官方文档描述得比较详细，可以参考官网的分类： 源数据的解析，将csv, dsv,geojson 转成标准的JSON，查看Connector 加工数据，包括 filter,map,fold(补数据) 等操作，查看Transform 统计函数，汇总统计、百分比、封箱 等统计函数，查看 Transform 特殊数据处理，包括 地理数据、矩形树图、桑基图、文字云 的数据处理，查看 Transform ECharts ECharts是一个成熟的图表库，使用方便、图表种类多、容易上手。文档资源页比较丰富。ECharts文档 ECharts & BizCharts & G2对比 对比BizCharts和G2两种图表库，BizCharts主要进行了一层封装，使的图表可以以组件的形式进行调用，按需加载，使用起来更加方便 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Charts/three.js在React中的运用.html":{"url":"Charts/three.js在React中的运用.html","title":"three.js 在 React 中的运用","keywords":"","body":"three.js在React中的运用 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/":{"url":"Browser/","title":"浏览器","keywords":"","body":"浏览器 一个TCP连接可以发送多少个HTTP请求 使用http-proxy-middleware代理跨域 跨域方式实现原理 常见HTTP请求头，响应头，实体 CORS跨域详解 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/一个TCP连接可以发送多少个HTTP请求.html":{"url":"Browser/一个TCP连接可以发送多少个HTTP请求.html","title":"一个 TCP 连接可以发送多少个 HTTP 请求","keywords":"","body":"Q1 现代浏览器在与服务器建立了一个TCP连接后是和否会在一个HTTP请求完成后断开？什么情况下会断开？ 在HTTP/1.0中，一个服务器在发送完一个HTTP像影后，会断开TCP链接。但是这样每次请求都会重新建立和断开TCP连接，代价过大（TCP的三次握手四次挥手）。所以虽然标准中没有设定，某些服务器对Connection：keep-alive的Header进行了支持。意思是说，完成这个HTTP请求之后，不要断开HTTP请求使用TCP连接，以及如果维持连接，那么SSL的开销也可以避免，两张图篇是我短时间内两次反问https://github.com/ 初始化连接和SSL开销小时了，说明使用的是同一个TCP连接 持久连接：既然维持TCP连接好处这么多，HTTP/1.1就把Connection头写进标准，并且默认开启持久连接，除非请求中写明Connection：close，那么浏览器和服务器之间是会维持一段时间的TCP连接，不会一个请求结束就会断开 R1: 默认情况下建立TCP连接不会断开，只有在请求报头中声名Connection：close才会在请求完成后关闭连接。 Q2 一个TCP连接可以对应几个HTTP请求？ R2：如果维持连接，一个TCP连接是可以发送多个HTTP请求的。 Q3 一个TCP连接中HTTP请求可以一起发送吗（比如一起发三个请求，在三个响应一起接收）？ HTTP/1.1存在一个问题，单个TCP连接在同一时刻只能处理一个请求，意思是说，两个请求的生命周期不能重叠，任意两个HTTP请求从开始到结束的时间在同一个TCP连接里不能重叠。 虽然HTTP/1.1规范中规定了Pipelining来试图解决这个问题，但是这个功能在浏览器中默认是关闭的。 先来看一下Pipelining是什么，RFC2616中规定了： 一个支持持久连接的客户端可以在一个连接中发送多个请求（不需要等待任意请求的响应）。收到请求的服务器必须按照请求收到的顺序发送响应【翻译】 至于标准为什么这么设定，我们可以大概推测一个原因，由于HTTP/1.1是个文本协议，同时返回的内容并不能区分对应于哪个发送的请求，所以顺序必须一致。服务器返回两个结果浏览器是没有办法根据响应结果来判断响应对应于哪一个请求的。 Pipelining这种设想看起来比较美好，但是在实践中会出现许多问题： 一些代理服务器不能正确的处理HTTP Pipelinling。 正确的流水线实现是复杂的。 Head-of-line Blocking 连接头阻塞：在建立期一个TCP连接之后，假设客户端在这个连接连续向服务器发送几个请求。按照标准，服务器应该按照收到请求的顺序返回结果，假设服务器在处理首个请求花费了大量时间，那么后面所欲的请求都需要等着这个请求结束后才能响应。 所以现代浏览器默认是不开启HTTP Pipelining的。 但是，HTTP2提供了 Multiplexing 多路传输特性，可以在一个TCP连接中同时完成多个HTTP请求。至于Multiplexing具体怎么实现的就是另一个问题了。我们可以看一下使用HTTP2的效果。 绿色是发起请求到请求返回的等待时间，蓝色是响应的下载时间，可以看到都是在同一个Connection，并行完成的。 R3: 在HTTP/1.1存在Pipelinling技术可以完成这个多个请求同时发送，但是由于浏览器默认是关闭的，所以可以认为是不可行的。在HTTP2中由于Multiplexing特点的寻在，多个HTTP请求是可以在同一个TCP连接中并行进行 那么在HTTP/1.1时代，浏览器是如何提高页面加载效率的呢？主要有下面两点： 维持和服务器已经建立的TCP 连接，在同一连接上顺序处理多个请求。 和服务器建立多个TCP连接 Q4 为什么有的时候刷新页面不需要重新建立SSL连接？ R4 TCP连接有的时候会被浏览器和服务端维持一段时间。TCP不需要重新建立，SSL自然也会用之前的。 Q5 浏览器对同一Host建立TCP连接的数量有没有限制？ 假设我们还处在HTTP/1.1时代，哪个时候没有多路传输，当浏览器拿到一个有几十张图片的网页的时候该怎么办？肯定不能只开一个TCP连接顺序下载，那样用户肯定等的难受，但是如果每个图片都开一个TCP连接发HTTP请求，那电脑或者服务器都可能受不了，要是有1000张图片的话不能开1000个TCP连接吧，你的电脑同意NAT也不一定会同意 所以答案是：有，Chrome最多允许对同意Host建立六个TCP连接。不同浏览器有一些区别 总结 那么回到最开始的问题，收到的 HTML 如果包含几十个图片标签，这些图片是以什么方式、什么顺序、建立了多少连接、使用什么协议被下载下来的呢？ 如果图片都是HTTPS连接并且在同一个域名下，那么浏览器在SSL握手之后会和服务器商量能不能用HTTP2，如果能的话就是用Multiplexing功能在这个连接上进行多路传输。不过也未必会有挂在这个域名的资源都会使用一个TCP连接区获取，但是可以确定的是Multiplexing很可能回用到 如果发现用不了HTTP2呢？ 或者用不了HTTPS（实际场景中HTTP2都是在HTTPS上实现的，所以也就是只能使用HTTP/1.1）那浏览器就会在一个HOST上建立多个TCP连接，连接数量的最大限制取决于浏览器的设置，这些连接会在空闲的时候被连览器用来发送新的请求，如果所有的连接都正在发送请求呢？ 那其他请求就只能等等了。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/使用http-proxy-middleware代理跨域.html":{"url":"Browser/使用http-proxy-middleware代理跨域.html","title":"使用 http-proxy-middleware 代理跨域","keywords":"","body":"使用http-proxy-middleware代理跨域 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/跨域方式实现原理.html":{"url":"Browser/跨域方式实现原理.html","title":"跨域方式实现原理","keywords":"","body":"跨域方式实现原理 前后端数据交互经常会碰到请求跨域，什么是跨域，以及有哪几种跨域方式，这是本文要探讨的内容。 一、什么是跨域？ 1. 什么是同源策略及其限制内容？ 同源策略是一种约定，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到XSS、CSRF等攻击。所谓同源是指“协议+域名+端口”三者相同，即便两个不同的域名指向同一个IP地址。也非同源。 同源策略限制内容有： Cookie、LocalStorage、IndexedDB等存储性内容 DOM节点 AJAX请求发送后，结果浏览器拦截了 但是有三个标签是允许跨域加载资源的： 2. 常见跨域场景 当协议、子域名、主域名、端口号中任意一个不同时，都算作不同域。不同域之间相互请求资源，就算做“跨域”。常见跨域场景 特别说明两点： 如果是协议和端口造成的跨域问题“前台”是无能为力的 在跨域问题上，仅仅是通过“URL的首部”来识别而不会根据域名对应的IP地址是否相同来判断。“URL的首部”可以理解为“协议”，域名和端口必须匹配 这里你或许有个疑问：请求跨域了，那么请求到底发出去没有？ 跨域并不是请求发不出去，请求能发出去，服务端能收到请求并正常返回结果，只是结果被浏览器拦截了。你可能会疑问明明通过表单的方式可以发起跨域请求，为什么 Ajax 就不会?因为归根结底，跨域是为了阻止用户读取到另一个域名下的内容，Ajax 可以获取响应，浏览器认为这不安全，所以拦截了响应。但是表单并不会获取新的内容，所以可以发起跨域请求。同时也说明了跨域并不能完全阻止 CSRF，因为请求毕竟是发出去了。 二、跨域解决方案 1. jsonp 1) JSONP原理 利用 标签没有跨域限制的漏洞，网页可以得到从其他来源动态产生的JSONP数据。JSONP请求一定需要对方的服务器做支持才可以 2）JSONP和AJAX对比 JSONP和AJAX相同，都是客户端向服务端发送请求，从服务端获取数据的方式。但AJAX属于同源策略，JSONP属于非同源策略 3）JSONP优缺点 JSONP有点是简单兼容性好，可用于解决主流浏览器的跨域数据访问的问题。缺点是仅支持get方法具有局限性，不安全可能会遭受XSS攻击 demo // 假如需要从服务器（http://www.a.com/user?id=123）获取数据如下： {\"id\": 123, \"name\": \"张三\", \"age\": 12} // 那么，使用JSONP方式请求（http://www.a.com/user?id=123?callback=foo）的数据将会是： foo({\"id\": 123, \"name\" : 张三, \"age\": 17}); 这时候我们只要定义一个foo（）函数，并动态地创建一个script标签，使其的src属性为http://www.a.com/user?id=123?callback=foo： function executeJsonp(url){ 　　var eleScript= document.createElement(\"script\"); 　　eleScript.type = \"text/javascript\"; 　　eleScript.src = url; 　　document.getElementsByTagName(\"head\")[0].appendChild(eleScript); } function foo(data){ for(var p in data){ console.log(data[p]); } } var url = \"http://www.a.com/user?id=123?callback=foo\"; executeJsonp(url) 便可以使用foo函数来调用返回的数据了。 在开发中可能会遇到多个JSONP请求的回调函数名是相同的，这时候就需要自己封装一个JSONP函数。 // index.html function jsonp({url, params, callback}) { return new Promise((resolve, reject) => { let script = document.creatElement('script') window[callback] = function (data) { resolve(data) document.body.removeChild(script) } params = { ...params, callback } // wd = b & callback=show let arrs = [] for (let key in params) { arrs.push(`${key} = ${params[key]}`) } script.src = `${url}?${arr.join('&')}` document.body.appendChild(script) }) } jsonp({ url: 'http://localhost:3000/say', params: { wd: 'Iloveyou'}, callback: 'show' }).then(data => { console.log(data) }) 上面这段代码相当于向http://localhost:3000/say?wd=Iloveyou&callback=show这个地址请求数据，然后后台返回show('我不爱你')，最后会运行show()这个函数，打印出'我不爱你' // server.js let express = require('express') let app = express() app.get('/say', function(req, res) { let { wd, callback } = req.query console.log(wd) // Iloveyou console.log(callback) // show res.end(`${callback}('我不爱你')`) }) app.listen(3000) 5) jQuery的jsonp形式 JSONP都是GET和异步请求的，不存在其他的请求凡事和同步请求，且jQuery默认就会给JSONP的请求清除缓存 $.ajax({ url: \"http://crossdomain.com/jsonServerResponse\", dataType: \"jsonp\", type: \"get\", jsonpCallback: \"show\", // 自定义传递给服务器的函数名，而不是使用jQuery自动生成的，可省略 jsonp: \"callback\", // 把传递函数名的那个形参callback，可省略 success: function (data) { console.log(data); } }) 2. cors 跨域资源共享（CORS）是一种机制，它使用额外的HTTP头来告诉浏览器让运行在一个origin（domain）的Web应用被准许访问来自不同源服务器上的指定资源。当一个资源从与该资源本身所在的服务器不同的域、协议或端口请求一个资源时，资源就会发起一个跨域HTTP请求 CORS需要浏览器和后端同时支持。IE 8 和 9 需要通过 XDomainRequest 来实现。 浏览器会自动进行 CORS 通信，实现 CORS 通信的关键是后端。只要后端实现了 CORS，就实现了跨域。 服务端设置 Access-Control-Allow-Origin 就可以开启 CORS。 该属性表示哪些域名可以访问资源，如果设置通配符则表示所有网站都可以访问资源。 虽然设置 CORS 和前端没什么关系，但是通过这种方式解决跨域问题的话，会在发送请求时出现两种情况，分别为简单请求和复杂请求。 1）简单请求 只要同时满足以下两大条件，就属于简单请求 条件1：使用下列方法之一： GET HEAD POST 条件2：Content-Type 的值仅限于下列三者之一： text/plain multipart/form-data application/x-www-form-urlencoded 请求中的任意 XMLHttpRequestUpload 对象均没有注册任何事件监听器； XMLHttpRequestUpload 对象可以使用 XMLHttpRequest.upload 属性访问。 2) 复杂请求 不符合以上条件的请求就肯定是复杂请求了。 复杂请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\"预检\"请求,该请求是 option 方法的，通过该请求来知道服务端是否允许跨域请求。 我们用PUT向后台请求时，属于复杂请求，后台需做如下配置： // 允许哪个方法访问我 res.setHeader('Access-Control-Allow-Methods', 'PUT') // 预检的存活时间 res.setHeader('Access-Control-Max-Age', 6) // OPTIONS请求不做任何处理 if (req.method === 'OPTIONS') { res.end() } // 定义后台返回的内容 app.put('/getData', function(req, res) { console.log(req.headers) res.end('我不爱你') }) 接下来我们看下一个完整复杂请求的例子，并且介绍下CORS请求相关的字段 // index.html let xhr = new XMLHttpRequest() document.cookie = 'name=xiamen' // cookie不能跨域 xhr.writeCredentials =true // 前端设置时候带cookie xhr.open('PUT', 'http://localhost:4000/getData', true) xhr.setRequestHeader('name', 'xiamen') xhr.onreadystatechange = function() { if (xhr.readyState === 4) { if ((xhr.status >= 200 && xhr.status //server1.js let express = require('express'); let app = express(); app.use(express.static(__dirname)); app.listen(3000); //server2.js let express = require('express') let app = express() let whitList = ['http://localhost:3000'] //设置白名单 app.use(function(req, res, next) { let origin = req.headers.origin if (whitList.includes(origin)) { // 设置哪个源可以访问我 res.setHeader('Access-Control-Allow-Origin', origin) // 允许携带哪个头访问我 res.setHeader('Access-Control-Allow-Headers', 'name') // 允许哪个方法访问我 res.setHeader('Access-Control-Allow-Methods', 'PUT') // 允许携带cookie res.setHeader('Access-Control-Allow-Credentials', true) // 预检的存活时间 res.setHeader('Access-Control-Max-Age', 6) // 允许返回的头 res.setHeader('Access-Control-Expose-Headers', 'name') if (req.method === 'OPTIONS') { res.end() // OPTIONS请求不做任何处理 } } next() }) app.put('/getData', function(req, res) { console.log(req.headers) res.setHeader('name', 'jw') //返回一个响应头，后台需设置 res.end('我不爱你') }) app.get('/getData', function(req, res) { console.log(req.headers) res.end('我不爱你') }) app.use(express.static(__dirname)) app.listen(4000) 3. postMessage postMessage是HTML5 XMLHttpRequest Level 2中的API，且是为数不多可以跨域操作的window属性之一，它可用于解决一下方面的问题： 页面和七打开的新窗口的数据传递 多窗口之间消息传递 页面与嵌套的iframe消息传递 上面场景的跨域数据传递 postMessage()方法允许来自不同源的脚本采用异步方式进行有限的通信，可以实现跨文本档、多窗口、跨域消息传递。 otherWindow.postMessage(message, targetOrigin, [transfer]); message: 将要发送到其他window的数据 targetOrigin：通过窗口的origin属性来指定哪些窗口能接收到消息时间，其值可以是 ransfer(可选)：是一串和message 同时传递的 Transferable 对象. 这些对象的所有权将被转移给消息的接收方，而发送一方将不再保有所有权。 接下来我们看个例子： http://localhost:3000/a.html页面向http://localhost:4000/b.html传递“我爱你”,然后后者传回\"我不爱你\"。 // a.html //等它加载完触发一个事件 //内嵌在http://localhost:3000/a.html function load() { let frame = document.getElementById('frame') frame.contentWindow.postMessage('我爱你', 'http://localhost:4000') //发送数据 window.onmessage = function(e) { //接受返回数据 console.log(e.data) //我不爱你 } } // b.html window.onmessage = function(e) { console.log(e.data) //我爱你 e.source.postMessage('我不爱你', e.origin) } 4.websocket Websocket是HTML5的一个持久化的协议，它实现了浏览器与服务器的全双工通信，同时也是跨域的一种解决方案。WebSocket和HTTP都是应用层协议，都基于 TCP 协议。但是 WebSocket 是一种双向通信协议，在建立连接之后，WebSocket 的 server 与 client 都能主动向对方发送或接收数据。同时，WebSocket 在建立连接时需要借助 HTTP 协议，连接建立好了之后 client 与 server 之间的双向通信就与 HTTP 无关了。 原生WebSocket API使用起来不太方便，我们使用Socket.io，它很好地封装了webSocket接口，提供了更简单、灵活的接口，也对不支持webSocket的浏览器提供了向下兼容。 我们先来看个例子：本地文件socket.html向localhost:3000发生数据和接受数据 // socket.html let socket = new WebSocket('ws://localhost:3000'); socket.onopen = function () { socket.send('Iloveyou'); // 向服务器发送数据 } socket.onmessage = function (e) { console.log(e.data); // 接收服务器返回的数据 } // server.js let express = require('express'); let app = express(); let Websocket = require('ws'); let wss = new WebSocket.Server({ port: 3000 }); wss.on('connection', function (ws) { ws.on('message', function (data) { console.log(data); ws.send('I'); }) }) 5. Node中间件代理（两次跨域） 实现原理：同源策略是浏览器需要遵循的标准，而如果是服务器向服务器请求就无需遵循同源策略。代理服务器，需要做一下几个步骤： 接受客户端请求 将请求转发给服务器 拿到服务器相应数据 将响应转发给客户端 我们先来看个例子：本地文件index.html文件，通过代理服务器http://localhost:3000向目标服务器http://localhost:4000请求数据 // index.html(http://127.0.0.1:5500) $.ajax({ url: 'http://localhost:3000', type: 'post', data: { name: 'xiamen', password: '123456' }, contentType: 'application/json;charset=utf-8', success: function (result) { console.log(result) // {\"title\":\"fontend\",\"password\":\"123456\"} }, error: function (msg) { console.log(msg) } }) // server1.js 代理服务器（http://localhost:3000） const http = require('http'); // 第一步：接受客户端请求 const server = http.createServer((request, response) => { // 代理服务器，直接和浏览器直接交互，需要设置CORS的首部字段 response.writeHead(200, { 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Method': '*', 'Access-Control-Allow-Headers': 'Content-Type', }) // 第二步：将请求转发给服务器 const proxy Request = http .request({ host: '127.0.0.1', port: 4000, url: '/', method: request.menthod, headers: request.headers }, response => { // 第三步：收到服务器的响应 let body = '' response.on('data', chunk => { body += chunk }) response.on('end', () => { console.log('The data is ' + body) // 第四步：将响应的结果转发给浏览器 response.end(body) }) }).end() }) server.listen(3000, () => { console.log('The proxyServer is running at http://localhost:3000') }) // server2.js（http://localhost:4000） const http = requires('http'); const data = { title: 'fontend', password } const server = http.createServer((request, response) => { if (request.url === '/') { response.end(JSON.stringify(data)) } }) server.listen(4000, () => { console.log('The server is running at http://localhost:4000') }) 上述代码经过两次跨域，值得注意的是浏览器向代理服务器发送请求，也遵循同源策略，最后在index.html文件打印出{\"title\":\"fontend\",\"password\":\"123456\"} 6. nginx反向代理 实现原理类似于Node中间件代理，需要你搭建一个中转nginx服务器，用于转发请求。 使用nginx反向代理实现跨域，是最简单的跨域方式。只需要修改nginx的配置即可解决跨域问题，支持所有浏览器，支持session ，不需要修改任何代码，并且不会影响服务器性能 实现思路：通过nginx配置一个代理服务器（域名与domain1相同，端口不同）做跳板机，反向代理访问domain2接口，并且可以顺便修改cookie中domain信息，方便当前域cookie写入，实现跨域登陆。 下载nginx，然后将nginx目录下的nginx.conf修改如下： // prox服务器 server { listen 81; server_name www.domain1.com; location / { proxy_pass http://www.domain2.com:8080; #反向代理 proxy_cookie_domain www.domain2.com www.domain1.com; #修改cookie里域名 index index.html index.htm; # 当用webpack-dev-server等中间件代理接口访问nginx时，此时无浏览器参与，故没有同源限制，下面的跨域配置可不启用 add_header Access-Control-Allow-Origin http://www.domain1.com; #当前端只跨域不带cookie时，可为* add_header Access-Control-Allow-Credentials true; } } 最后通过命令行nginx -s reload重启nginx // index.html let xhr = new XMLHttpRequest(); // 前端开关：浏览器是否读写cookie xhr.withCredentails = true; xhr.open('get', 'http://www.domain1.com:81/?user=admin', true); xhr.send(); // server.js var http = require('http'); var server = http.createServer(); var qs = require('querystring'); server.on('request', function(req, res) { var params = qs.parse(req.url.substring(2)); // 向前台写cookie res.writeHead(200, { 'Set-Cookie': 'l=a123456;Path=/;Domain=www.domain2.com;HttpOnly' // HttpOnly:脚本无法读取 }); res.write(JSON.stringify(params)); res.end(); }); server.listen('8080'); console.log('Server is running at port 8080...'); 7. window.name + iframe window.name属性的独特之处：name值在不同的页面（甚至不同域名）加载后依旧存在，并且可以支持非常长的name值（2MB）。 其中a.html和b.html是同域名的，都是http://localhost:3000；而c.html是http://localhost:4000 // a.html(http://localhost:3000/b.html) let first = true // onload事件会触发2次，第1次加载跨域页，并留存数据于window.name function load() { if (first) { // 第1次onload(跨域页)成功后，切换到同域代理页面 let iframe = document.getElementById('iframe'); iframe.src = 'http://localhost:3000/b.html'; first = false; } else { // 第2次onload(同域b.html页)成功后，读取同域window.name中数据 console.log(iframe.contentWindow.name); } } b.html为中间代理页，与a.html同域，内容为空 // c.html(http://localhost:4000/c.html) window.name = 'I don't love you' 总结：通过iframe的src属性由外域转向本地域名，跨域数据即由iframe的window.name从外域传递到本地域。这个就巧妙地绕过了浏览器的跨域访问限制，但同时它又是安全操作 localtion.hash + iframe 实现原理：a.html域与c.html跨域相互通信，通过中间页b.html来实现。三个页面，不同域之间利用iframe的location.hash传值，相同域之间直接js访问来通信 具体实现步骤：一开始a.html给c.html传一个hash值，然后c.html收到hash值后，再把hash值传递给b.html，最后b.html将结果放到a.html的hash值中。 同样的，a.html和b.html是同域的，都是http://localhost:3000;而c.html是http://localhost:4000 // a.html window.onhashchange = function () { //检测hash的变化 console.log(location.hash); } // b.html window.parent.parent.location.hash = location.hash //b.html将结果放到a.html的hash值中，b.html可通过parent.parent访问a.html页面 // c.html console.log(location.hash); let iframe = document.createElement('iframe'); iframe.src = 'http://localhost:3000/b.html#idontloveyou'; document.body.appendChild(iframe); 9. document.domain + iframe 该方式只能用于二级域名相同的情况下，比如a.test.com和b.test.com适用于该方式。只需要给页面添加document.domain = 'test.com'表示二级域名都相同就可以实现跨域。 实现原理：两个页面都通过js强制设置document.domain为基础主域，就实现了同域。 我们看个例子：页面a.zf1.cn:3000/a.html获取页面b.zf1.cn:3000/b.html中a的值 // a.html helloa document.domain='zf1.cn' function load() { console.log(frame.contentWindow.a) } hellob document.domain = 'zf1.cn' let a = 100; 总结 CORS支持所有类型的HTTP请求，是跨域HTTP请求的根本解决方案 JSONP只支持GET请求，JSONP的又是在于支持老式浏览器，以及可以像不支持CORS的网站请求数据 不管是node中间件代理，=还是nginx反向代理，主要是通过同源策略对服务器不加限制 日常工作中，用的比较多的跨域方法是cors和nginx反向代理 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/常见HTTP请求头，响应头，实体.html":{"url":"Browser/常见HTTP请求头，响应头，实体.html","title":"常见 HTTP 请求头，响应头，实体","keywords":"","body":"常见HTTP请求头，响应头，实体 一、常用的http请求头 1.Accept Accept: text/html 浏览器可以接受服务器回调的类型为text/html。 Accept: /代表浏览器可以处理所有类型，（一般浏览器发送给服务器都是发这个） 2. Accept-Encoding Accept-Encoding：gzip，deflate浏览器申明自己接受的编码方法，通常指定压缩方法，是否支持压缩，支持什么压缩方法（gzip，deflate） 3. Accept-Language Accept-Language: zh-CN,zh;q=0.9 浏览器申明自己接收的语言 4. Connection Connection: keep-alive当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 5. Host（发送请求时，该报头域是必需的） Host:www.baidu.com请求报头域主要用于指定被请求资源的Internet主机和端口号，它通常从HTTP URL中提取出来的。 6.Referer Referer:https://www.baidu.com/?tn=62095104_8_oem_dg 当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的，服务器籍此可以获得一些信息用于处理。 7. User-Agent User-Agent:Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36 告诉HTTP服务器， 客户端使用的操作系统和浏览器的名称和版本。 8.Cache-Control Cache-Control:private 默认为private 响应只能够作为私有的缓存，不能再用户间共享 Cache-Control:public 响应会被缓存，并且在多用户间共享。正常情况, 如果要求HTTP认证,响应会自动设置为 private. Cache-Control:must-revalidate 响应在特定条件下会被重用，以满足接下来的请求，但是它必须到服务器端去验证它是不是仍然是最新的。 Cache-Control:no-cache 响应不会被缓存,而是实时向服务器端请求资源。 Cache-Control:max-age=10 设置缓存最大的有效时间，但是这个参数定义的是时间大小（比如：60）而不是确定的时间点。单位是[秒 seconds]。 Cache-Control:no-store 在任何条件下，响应都不会被缓存，并且不会被写入到客户端的磁盘里，这也是基于安全考虑的某些敏感的响应才会使用这个。 9.Cookie Cookie是用来存储一些用户信息以便让服务器辨别用户身份的（大多数需要登录的网站上面会比较常见），比如cookie会存储一些用户的用户名和密码，当用户登录后就会在客户端产生一个cookie来存储相关信息，这样浏览器通过读取cookie的信息去服务器上验证并通过后会判定你是合法用户，从而允许查看相应网页。当然cookie里面的数据不仅仅是上述范围，还有很多信息可以存储是cookie里面，比如sessionid等。 10. Range(用于断点续传) Range:bytes=0-5 指定第一个字节的位置和最后一个字节的位置。用于告诉服务器自己想取对象的哪部分。 二、常用的http响应头 1. Cache-Control（对应请求中的Cache-Control） Cache-Control:private 默认为private 响应只能够作为私有的缓存，不能再用户间共享 Cache-Control:public 浏览器和缓存服务器都可以缓存页面信息。 Cache-Control:must-revalidate 对于客户机的每次请求，代理服务器必须向服务器验证缓存是否过时。 Cache-Control:no-cache浏览器和缓存服务器都不应该缓存页面信息。 Cache-Control:max-age=10 是通知浏览器10秒之内不要烦我，自己从缓冲区中刷新。 Cache-Control:no-store 请求和响应的信息都不应该被存储在对方的磁盘系统中 2. Content-Type Content-Type：text/html;charset=UTF-8 告诉客户端，资源文件的类型，还有字符编码，客户端通过utf-8对资源进行解码，然后对资源进行html解析。通常我们会看到有些网站是乱码的，往往就是服务器端没有返回正确的编码。 3. Content-Encoding Content-Encoding:gzip 告诉客户端，服务端发送的资源是采用gzip编码的，客户端看到这个信息后，应该采用gzip对资源进行解码。 4. Date Date: Tue, 03 Apr 2018 03:52:28 GMT 这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。http协议中发送的时间都是GMT的，这主要是解决在互联网上，不同时区在相互请求资源的时候，时间混乱问题。 Server Server：Tengine/1.4.6 这个是服务器和相对应的版本，只是告诉客户端服务器信息。 7. Expires Expires:Sun, 1 Jan 2000 01:00:00 GMT 这个响应头也是跟缓存有关的，告诉客户端在这个时间前，可以直接访问缓存副本，很显然这个值会存在问题，因为客户端和服务器的时间不一定会都是相同的，如果时间不同就会导致问题。所以这个响应头是没有Cache-Control：max-age=*这个响应头准确的，因为max-age=date中的date是个相对时间，不仅更好理解，也更准确。 8. Last-Modified Last-Modified: Dec, 26 Dec 2015 17:30:00 GMT 所请求的对象的最后修改日期(按照 RFC 7231 中定义的“超文本传输协议日期”格式来表示) 9. Connection Connection：keep-alive 这个字段作为回应客户端的Connection：keep-alive，告诉客户端服务器的tcp连接也是一个长连接，客户端可以继续使用这个tcp连接发送http请求。 10. Etag ETag: \"737060cd8c284d8af7ad3082f209582d\" 就是一个对象（比如URL）的标志值，就一个对象而言，比如一个html文件，如果被修改了，其Etag也会被修改，所以，ETag的作用跟Last-Modified的作用差不多，主要供WEB服务器判断一个对象是否改变了。比如前一次请求某个html文件时，获得了其 ETag，当这次又请求这个文件时，浏览器就会把先前获得ETag值发送给WEB服务器，然后WEB服务器会把这个ETag跟该文件的当前ETag进行对比，然后就知道这个文件有没有改变了。 11. Refresh Refresh: 5; url=http://baidu.com 用于重定向，或者当一个新的资源被创建时。默认会在5秒后刷新重定向。 12. Access-Control-Allow-origin Access-Control-Allow-Origin: 号代表所有网站可以跨域资源共享，如果当前字段为*那么Access-Control-Allow-Credentials就不能为true Access-Control-Allow-Origin: www.baidu.com 指定哪些网站可以跨域资源共享 13. Access-Control-Allow-Methods Access-Control-Allow-Methods：GET,POST,PUT,DELETE 允许哪些方法来访问 14. Access-Control-Allow-Credentials Access-Control-Allow-Credentials: true 是否允许发送cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。如果access-control-allow-origin为*，当前字段就不能为true 15. Content-Range Content-Range： bytes 0-5/7877指定整个实体中的一部分的插入位置，他也指示了整个实体的长度。在服务器向客户返回一个部分响应，它必须描述响应覆盖的范围和整个实体长度。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/CORS跨域详解.html":{"url":"Browser/CORS跨域详解.html","title":"CORS 跨域详解","keywords":"","body":"CORS跨域详解 一、为什么会有跨域问题 浏览器同源策略 (跨域是浏览器的限制，抓包工具等可以拿到数据) 浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉 二、如何解决跨域 jsonp nginx反向代理 CORS（跨域资源共享），支持所有类型的http请求，本文主要介绍这种方式 三、简单请求和非简单请求 Ajax 按照条件可以分为两种请求方式，简单请求和非简单请求。 同时满足以下两个条件，就属于简单请求 使用下列方法之一 head get post 请求的header是 Accept Accept-Language Content-Language Content-Type 只限于三个值 application/x-www-form-urlencodes multipart/form-data text/plain 对于简单请求，浏览器直接发出CORS请求。在头部字段中，增加一个Origin字段。（chrome有时会隐藏这个字段） 四、CORS请求相关的字段，都以Access-Control-开头 Access-Control-Allow-Origin ：必选 （一个或者多个允许跨越的）请求头Origin字段的值 *：接受任何域名 Access-Control-Allow-Credentials： 可选 true：表示允许发送cookie，此时 Access-Control-Allow-Origin 不能设置为*，必须指定明确的，与请求网页一致的域名； 不设置该字段，不需要浏览器发送cookie Access-Control-Expose-Headers：可选 列出了哪些首部可以作为响应的一部分暴露给外部 默认，只有六种暴露给外部 Cache-Control Content-Language Content-Type Expires Last-Modified Pragma 如果想让客户端可以访问到其他的首部信息，可以将他们在Access-Control-Expose-Headers里面列出来 五、withCredentials 属性 CORS默认不发送Cookie和HTTP认证信息，如果要把Cookie 发送到服务器，一方面需要服务器同意，设置响应头 Access-Control-Allow-Credentials：true；另一方面，客户端发送请求时，也要进行一些设置，例如 xhr.withCredentials = true; 六、非简单请求 非简单请求就是那种对服务器有特殊要求的请求，比如请求方法为PUT 或者 DELETE，或者 Content-Type 为 application/json； 预检请求和回应 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，成为“预检”。浏览器会询问服务器，当前网页所在的域名是否在是否在许可名单之内，以及可以使用哪些 HTTP 动词和头部信息段，只有得到肯定答复，才会发出正式的接口请求。否则，报错。 1、预检请求 用OPTIONS方法，询问。预检请求包括三个字段 Origin：表示请求来自哪个域； Access-Control-Request-Method：必须，浏览器会使用的请求方法； Access-Control-Request-Headers: 浏览器发送 CORS 请求会额外发送的头部信息段； 2、预检回应 服务器收到预检请求后，检查了Origin，Access-Control-Request-Method，Access-Control-Request-Headers字段后，确认允许跨域，就可以做出回应 如果浏览器否认了“预检”请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头部信息字段，浏览器会认为不同意，触发一个错误 服务器回应的其他CORS字段 Access-Control-Allow-Methods: 必需，逗号分隔的字符串，表示服务器支持的所有跨域请求方法； Access-Control-Allow-Headers：浏览器支持的所有头部字段；Access-Control-Allow-Credentials：Cookie Access-Control-Allow-Max-Age: 指定本次请求的有效期； 七、服务端如何设置CORS 如果设置请求头Content-Type：application/x-www-form-urlencoded，则为简单请求 直接设置响应头 Access-Control-Allow-Origin 为 *，或者具体的域名； 如果响应头Access-Control-Allow-Credentials 为 true，则此时Access-Control-Allow-Origin 不能设置为*，必须指定明确的域名； 如果设置请求头 Content-Type：application/json，则为非简单请求 处理 OPTIONS 请求，服务端可以单独写一个路由 可以把这部分抽离处理，作为一个中间件，例如Koa； By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/SEO小技巧.html":{"url":"Browser/SEO小技巧.html","title":"SEO小技巧","keywords":"","body":"SEO小技巧 https://ahrefs.com/blog/zh/seo-tips/ By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/URL的最大长度是多少？.html":{"url":"Browser/URL的最大长度是多少？.html","title":"URL的最大长度是多少？","keywords":"","body":"URL的最大长度是多少？ 简短回答-事实上限制为2000个字符 虽然说明书中的HTTP协议不指定任何最大长度，实际限制是有网络浏览器和服务器软件加强的。 翻译 Microsoft Internet Explorer（浏览器） Micrsoft申明：Internet Explorer的最大长度为2083个字符，URL的路径部分不超过2048个字符。在我的测试中，尝试使用比此更长的URL会在Internet Explorer中生成明确的错误消息。 Firfox(浏览器) 在65536个字符后，位置栏不再显示Window Firefox 1.5.x中的URL。但是，较长的网址可以使用。我在100，000个字符后停止测试。 Safari（浏览器） 至少80，000个字符可以使用。我在80，000个字符后停止测试。 Opera（浏览器） 至少190，000个字符可以使用，我在190，000个字符停止测试。Opera 9 for windows继续在位置栏中显示完全可以编辑，可复制和可粘贴的URL，即使是190，000个字符。 Apache（服务器） 我早起尝试在Web浏览器中测量最大长度URL长度的过程会导致服务器URL长度限制大约为4，000个字符，之后Apache会产生“413” Entity Too Large\"错误。我使用了当前最新的Red Hat Enterprise Linux 4中的Apache构建。官方Apache文档仅提到了请求中单个字段的8192字节限制 Microsoft Internet Information Server 默认限制为16,384个字符（是的，Microsoft的Web服务器接受比Microsoft的Web浏览器更长的URL）。这是可配置的。 Perl HTTP ::守护进程（服务器） 最多可以使用8,000个字节。那些使用Perl的HTTP :: Daemon模块构建Web应用程序服务器的人将在所有HTTP请求头的组合大小上遇到16,384字节的限制。这并没有包括POST方法的表单数据，文件上传等，但它确实包含的URL。实际上，当URL明显长于8,000个字符时，这会导致413错误。可以轻松删除此限制。在Daemon.pm中查找所有出现的16x1024，并用更大的值替换它们。当然，这确实会增加您遭受拒绝服务攻击的风险。 建议 极长的URL通常是一个错误。超过2，000个字符的网址在最受欢迎的网络浏览器中无效。如果您希望网站为大多数互联网用户工作，请不要使用它们。 当您希望提交包含许多字段的表单时，否则回生成一个非常长的URL，标准解决方案是使用POST方法而不是GET方法： ... 然后，表单字段作为HTTP事务正文的一部分传输，而不是作为UR的一部分传输，并且不受URL长度限制的约束。短期信息不应存储在URL中。 根据经验，如果由于返回收藏夹或书签而不需要从新生成同意页面的信息，则它不属于URL。 书签问题 在极少数情况下，在URL中保留大量“状态”信息可能很有用。例如，地图导航网站的用户可能希望将当前显示的地图添加到他们的“书签”或“收藏夹”列表中并稍后返回。如果您必须这样做并且您的网址长度接近2,000个字符，请尽可能紧凑地表达您的信息，尽可能多地挤出“空气”。如果您的字段名称占用太多空间，请使用固定字段顺序。挤出任何不需要加入书签的字段。并避免使用大的十进制数 - 只使用尽可能多的精度，并考虑使用字母和数字的base-64表示（我没有说这很容易）。 在极端情况下，请考虑使用gzip算法压缩漂亮但过长的URL。然后仅使用URL中合法的字符对base64中的二进制数据进行重新编码。这可以产生3-4倍的空间增益，当您在下次访问时再次解压缩URL时会花费一些CPU时间。再一次，我从未说过这很容易！ 另一种方法是将状态信息存储在文件或数据库中。然后，您只能存储在URL中再次查找该信息所需的标识符。这里的缺点是你将拥有许多状态文件或数据库记录。其中一些可能与其他人运营的网站相关联。此问题的一个解决方案是删除在一定时间后未重新访问的URL的状态文件或数据库记录。 如果服务器的URL太长，会发生什么？ 如果支持超长URL的浏览器（如Firefox）将长URL提交给不支持很长URL的Web服务器（例如Apache的标准版本），会发生什么？ 答案：没什么戏剧性的。Apache响应“413 Entity Too Large”错误，请求失败。 这种响应比缩短URL更好，因为剪切URL的结果是不可预测的。这对Web应用程序意味着什么？它有所不同。因此，请求失败会更好。 在过去的糟糕时期，一些Web服务器和Web浏览器无法截断或忽略长URL，从而导致危险的“缓存区溢出”情况。这些可用于插入不属于它的可执行代码。。。导致可能被利用来做坏事的安全漏洞。 目前，主要的浏览器和服务器可以抵御这种明显的攻击-尽管经常回发现更微妙的安全漏洞（并且通常回立即修复） 虽然现代服务器本身可以很好地地域长URL，但仍然寻在编写错误的CGI程序。那些用C语言和其他第几语言便携CGI程序的人必须负责密切的煮鱼潜在的缓存区溢出。该CGIC库可以帮助这一点。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/深入理解Session和Cookie.html":{"url":"Browser/深入理解Session和Cookie.html","title":"深入理解Session和Cookie","keywords":"","body":"深入理解Session和Cookie 最近在写Web服务，顺便梳理了一下Cookie和Session的知识 Cookie简介 由于HTTP是一种无状态的协议，服务器单从网络连接上无从知道客户身份。 怎么办呢？就给客户端们颁发一个通行证吧，每人一个，无论谁访问都必须携带自己通行证。 这样服务器就能从通行证上确认客户身份了。这就是Cookie的工作原理。 Cookie实际上是一小段的文本信息。 客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。 客户端浏览器会把Cookie保存起来。 当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。 服务器检查该Cookie，以此来辨认用户状态。 服务器还可以根据需要修改Cookie的内容。 Cookie机制 Cookie技术是客户端的解决方案，Cookie就是由服务器发给客户端的特殊信息，而这些信息以文本文件的方式存放在客户端， 然后客户端每次向服务器发送请求的时候都会带上这些特殊的信息。 具体过程如下： 用户使用浏览器访问一个支持Cookie的网站的时候，用户会提供包括用户名在内的个人信息并且提交至服务器； 服务器在向客户端回传相应的超文本的同时也会发回这些个人信息，当然这些信息并不是存放在HTTP响应体 （Response Body)中的，而是存放于HTTP响应头(Response Header) 客户端浏览器接收到来自服务器的响应之后，浏览器会将这些信息存放在一个统一的位置。 对于Windows操作系统而言，我们可以从： [系统盘]:\\Documents and Settings[用户名]\\Cookies目录中找到存储的Cookie； 客户端再次向服务器发送请求的时候，都会把相应的Cookie再次发回至服务器。 而这次，Cookie信息则存放在HTTP请求头(equest Header)了。 HTTP的Cookie机制 Web应用程序是使用HTTP协议传输数据的。HTTP协议是无状态的协议。 一旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建立新的连接。 这就意味着服务器无法从连接上跟踪会话。 举个例子，用户A购买了一件商品放入购物车内， 当再次购买商品时服务器已经无法判断该购买行为是属于用户A的会话还是用户B的会话了。 要跟踪该会话，必须引入一种机制。 Cookie就是这样的一种机制。它可以弥补HTTP协议无状态的不足。 在Session出现之前，基本上所有的网站都采用Cookie来跟踪会话。 Set-Cookie和Cookie 两个Http头部和Cookie有关 : Set-Cookie和Cookie 当服务器返回给客户端一个Http响应信息时，其中如果包含Set-Cookie这个头部，说明： 指示客户端建立一个cookie 在后续的Http请求中自动发送这个cookie到服务器端，直到这个cookie过期。 如果cookie的生存时间是整个会话期间的话，那么浏览器会将 cookie 保存在内存中， 浏览器关闭时就会自动清除这个cookie。 如果将 cookie 保存在客户端的硬盘中，浏览器关闭的话，该 cookie 也不会被清除， 下次打开浏览器访问对应网站时，这个cookie就会自动再次发送到服务器端。 一个cookie的设置以及发送过程分为以下四步： 客户端发送一个http请求到服务器端 服务器端发送一个http响应到客户端，其中包含Set-Cookie头部 客户端发送一个http请求到服务器端，其中包含Cookie头部 服务器端发送一个http响应到客户端 在客户端的第二次请求中包含Cookie头部，提供给了服务器端可以用来唯一标识客户端身份的信息。 这时，服务器端也就可以判断客户端是否启用了cookie。 尽管，用户可能在和应用程序交互的过程中突然禁用cookie的使用， 但是，这个情况基本是不太可能发生的，所以可以不加以考虑，这在实践中也被证明是对的。 Cookie的不可跨域名行 很多网站都会使用Cookie。例如，Google会向客户端颁发Cookie，Baidu也会向客户端颁发Cookie。 那浏览器访问Google会不会也携带上Baidu颁发的Cookie呢？或者Google能不能修改Baidu颁发的Cookie呢？ 答案是否定的。Cookie具有不可跨域名性。 根据Cookie规范，浏览器访问Google只会携带Google的Cookie，而不会携带Baidu的Cookie。 Google也只能操作Google的Cookie，而不能操作Baidu的Cookie。 Cookie在客户端是由浏览器来管理的。 浏览器能够保证Google只会操作Google的Cookie而不会操作Baidu的Cookie，从而保证用户的隐私安全。 浏览器判断一个网站是否能操作另一个网站Cookie的依据是域名。 Google与Baidu的域名不一样，因此Google不能操作Baidu的Cookie。 注意： 虽然网站images.google.com与网站google.com同属于Google， 但是域名不一样，二者同样不能互相操作彼此的Cookie。 用户登录网站google.com之后会发现访问images.google.com时登录信息仍然有效，而普通的Cookie是做不到的。 这是因为Google做了特殊处理。 Session简介 Session是一种记录客户状态的机制，不同于Cookie的是Cookie保存在客户端浏览器中，而Session保存在服务器上。 客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。 客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。 如果说Cookie机制是通过检查客户身上的\"通行证\"来确定客户身份的话， 那么Session机制就是通过检查服务器上的\"客户明细表\"来确认客户身份。 Session相当于程序在服务器上建立的一份客户档案， 客户来访的时候只需要查询客户档案表就可以了。 Session机制 一方面，我们可以把客户端浏览器与服务器之间一系列交互的动作称为一个 Session。 从这个语义出发，我们会提到Session持续的时间，会提到在Session过程中进行了什么操作等等。 另一方面，Session指的是服务器端为客户端所开辟的存储空间，该空间保存的信息就是用于保持状态。 从这个语义出发，我们则会提到往Session中存放什么内容，如何根据键值从Session中获取匹配的内容等。 要使用Session，当然是先要创建Session。那么Session在何时创建呢？ Session在服务器端程序运行的过程中创建的，不同语言实现的应用程序有不同创建Session的方法， 在Java中是通过调用HttpServletRequest的getSession方法(使用true作为参数)创建的。 创建Session的同时，服务器会为该Session生成唯一的session id， 这个session id在随后的请求中会被用来重新获得已经创建的Session Session被创建之后，就可以调用Session相关的方法往Session中增加内容了， 而这些内容只会保存在服务器中，发到客户端的只有session id 当客户端再次发送请求的时候，会将这个session id带上， 服务器接受到请求之后就会依据session id找到相应的Session，从而再次使用Session。 Session的生命周期 Session保存在服务器端。为了获得更高的存取速度，服务器一般把Session放在内存中。 每个用户都会有一个独立的Session。 如果Session内容过于复杂，当大量客户访问服务器时可能会导致内存溢出。 因此，Session里的信息应该尽量精简。 Session在用户第一次访问服务器的时候自动创建。 需要注意只有访问JSP、Servlet等程序时才会创建Session， 只访问HTML、IMAGE等静态资源并不会创建Session。 如果尚未生成Session，也可以使用request.getSession(true)强制生成Session。 Session生成后，只要用户继续访问，服务器就会更新Session的最后访问时间，并维护该Session。 用户每访问服务器一次，无论是否读写Session，服务器都认为该用户的Session\"活跃(active)\"了一次。 Session的有效期 由于会有越来越多的用户访问服务器，因此Session也会越来越多。 为防止内存溢出，服务器会把长时间内没有活跃的Session从内存删除。 这个时间就是Session的超时时间。如果超过了超时时间没访问过服务器，Session就自动失效了。 Session的超时时间为maxInactiveInterval属性， 可以通过对应的getMaxInactiveInterval()获取，通过setMaxInactiveInterval(longinterval)修改。 Session的超时时间也可以在web.xml中修改。 另外，通过调用Session的invalidate()方法可以使Session失效。 实现会话跟踪的技术 1. Cookie 向客户端发送Cookie Cookie c =new Cookie(\"name\",\"value\"); //创建Cookie c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天 response.addCookie(c); //把Cookie放入到HTTP响应中 从客户端读取Cookie： 优点： 数据可以持久保存，不需要服务器资源，简单，基于文本的key-value 缺点：大小受到限制，用户可以警用Cookie功能，由于保存在本地，有一定的安全风险。 2. URL重写 在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾一标识一个会话 优点：在Cookie被禁用的时候依然可以使用 缺点：必须对网站的URL进行编码，所有页面必须动态生成，不能用预先记录下来的URL进行访问。 3. 隐藏的表单域 优点：Cookie被禁止时可以使用 缺点：所有页面必须是表单提交之后的结果 4. Session 当一个用户第一次访问某个网站时会自动创建HttpSession，每个用户可以访问它自己的HttpSession。 可以通过HttpServletRequest对象的getSession方法获得HttpSession。通过HttpSession的setAttribute方法可以将一个值放在HttpSession中， 通过调用 HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。 与上面三种方式不同的是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面。 即使目前的Servlet容器可以在内存将满时将 HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。 添加到 HttpSession 中的值可以是任意Java对象，这个对象最好实现了 Serializable接口， 这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。 Cookie和Session的的区别 HTTP协议是无状态的协议，服务端需要记录用户的状态，就需要用某种机制来识别具体的用户，这个机制就是Session。 Session典型的应用场景就是购物车，当点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的， 所以服务端要为特定的用户创建了特定的Session，用于标识这个用户，并且跟踪用户，这样才知道购物车里面的商品情况。 这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。 集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群， 用来保存用户会话，这个时候 Session 信息都是放在内存的，此外，一些缓存服务比如Memcached之类的来放 Session。 服务端使用Cookie来识别特定的客户。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的， 第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个session id， 以后每次请求把这个 session id发送到服务器，这样就可以使用对应的Seesion了。 如果客户端的浏览器禁用了 Cookie 怎么办？ 一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪， 即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下， 设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？ 这个信息可以写到Cookie里面，访问网站的时候， 网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了， 能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 总结： Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中。 Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/HTTP切面流程.html":{"url":"Browser/HTTP切面流程.html","title":"HTTP切面流程","keywords":"","body":"HTTP切面流程 HTTP 生命过程 http请求 路由操作 权限处理 数据安全 业务操作 数据操作 书单查询 http响应 响应操作 Koa.js的HTTP过程 请求 中间件 响应 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/Node与浏览器之间的区别.html":{"url":"Browser/Node与浏览器之间的区别.html","title":"Node与浏览器之间的区别","keywords":"","body":"Node与浏览器之间的区别 在Node.js中比啊些JavaScript应用程序与在浏览器中为Web编程有何不同 浏览器和Node均使用JavaScript作为编程语言 构建在浏览器中运行的应用程序与构建在Node.js应用程序完全不同 尽管事实始终是JavaScript，但仍存在一些关键差异，试体验完全不同 作为广泛使用Javascript的前端开发人员，Node应用程序带来了巨大的优势-使用单一语言轻松编程所有内容（前端和后端） 生态系统发生了什么变化。 区别一 在浏览器中，大多数时候您正在做的是与DOM或其它Web平台 API（例如cookies）进行交互。当然，那些在Node中不存在。你没有document，window并通过浏览器提供的所有其它对象 而且在浏览器中，我们没有Node.js通过其模块提供的所有不错的API，例如文件系统访问功能 区别二 另一个很大的不同是，在Node.js中，您可以控制环境。除非您构建一个任何人都可以在任何地方部署的开源应用程序。否则您将知道将在哪个版本的Node上运行该应用程序。与浏览器环境相比，在浏览器环境中，您无法选择访客会使用的浏览器，这非常方便 这意味着您可以编写你的Node版本支持的所有现代ES6-7-8-9JavaScript 由于JavaScript的移动速度如此之快，但是浏览器可能会变慢，而用户的升级会变慢，有时是在网络上，因此您不得不使用较旧的JavaScript/ECMAScript版本。 您可以使用Babel将代码转换为与ES5兼容，然后再将其交付给浏览器，但是在Node中，则不需要这样做。 区别三 另一个区别是Node使用CommonJS模块系统，而在浏览器中，我们开始看到正在实现ES模块标准 实际上，这意味着你暂时require（）在Node和import浏览器中使用。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Browser/JavaScript如何在浏览器和Node中工作.html":{"url":"Browser/JavaScript如何在浏览器和Node中工作.html","title":"JavaScript如何在浏览器和Node中工作","keywords":"","body":"JavaScript如何在浏览器和Node中工作 JavaScript 运行时，回调队列和事件循环以及Web API的可视化。这里没有主要技术。 JavaScript非常易于理解，并且是前端开发的重要组成部分。但是与其它编程语言不同，它是单线程的。这意味着代码执行将一次完成一次。由于代码的执行是顺序执行的，因此任何需要较长时间执行的代码都会阻塞需要执行的所有内容。因此，有时你在使用Google Chrome浏览器时会看到以下屏幕 在浏览器中打开网站时，它使用单个JavaScript执行线程。该线程负责处理所有内容，例如滚动网页，在网页上打印内容，侦听DOM事件（例如用户单机按钮时）以及执行其它操作。但是，当JavaScript执行被阻止时，浏览器将停止执行所有这些操作，这意味着浏览器将仅冻结摒弃不响应任何内容。 你可以在下面的永恒while循环中看到实际效果。 while (true) {} 上述语句之后的任何代码都不会执行，因为while循环将无限循环，直到系统资源不足为止。这也可能发生在无限递归函数调用中。 多亏了现代浏览器，并非所有打开的浏览器选项卡都依赖与单个JavaScript线程。而是每个标签或每个域使用单独的JavaScript线程。如果使用的是Google浏览器，则可以打开具有不同网站的多个标签，并在永恒while循环之上运行。这只会冻结执行该代码的当前选项卡，而其它选项卡将正常运行。当Chrome实施每个站点一个进程的策略并且一个进程使用相同JavaScript执行线程时，从同一域/相同网站打开页面的任何标签也会冻结。 为了可视化JavaScript如何执行程序，我们需要了解JavaScript运行时。 像任何其它编程语言一样，JavaScript运行时具有一个堆栈和一个栈存储。我将不对堆进行更多的解释。你可以 在这里阅读。我们感兴趣的是stack。堆栈LIFO（后进先出）数据存储，用于存储程序的当前功能执行上下文。但我们的程序加载到内存中时，它将从第一个函数调用开始执行foo(). 直到函数返回某些内容（函数执行时），它才会从堆栈中弹出。该条目（函数）返回某个值后，堆栈将逐个弹出条目，并且它将继续执行未决的函数 在每个条目处，堆栈的状态也称为堆栈框架。如果在给定的堆栈框架上进行任何函数调用均产生错误，则JavaScript将打印堆栈跟踪，这只是该堆栈框架上代码执行的快照。 函数baz（）{ 抛出新错误（'出事了。'）; } 函数bar（）{ baz（）; } 函数foo（）{ bar（）; } foo（）; 在上面程序中，我们从baz函数中抛出了错误，JavaScript将在堆栈跟踪下面打印出来，以找出错误的原因和位置。 由于JavaScript是单线程的，因此它只有一个堆栈和一个堆。因此，如果任何其它程序想要执行某些操作，则必须等待直到之前的程序完全执行 因此，让我们考虑一种情况，如果浏览器发送HTTP请求以通过网络加载一些数据或加载要在网页上显示的图像该怎么办。浏览器会冻结直到钙请求得到解决？如果确实如此，那么对用户体验是非常不利的。 浏览器附带一个JavaScript引擎，该引擎提供JavaScript运行时环境。例如，谷歌浏览器使用他们开发的V8 JavaScript。但是请猜测，浏览器不仅仅使用JavaScript引擎。这就是幕后的浏览器。 看起来真的很复杂，但是很容易理解。JavaScript运行时实际上由两个组件组成。事件循环和回调队列。回调队列也称为消息队列或任务队列。 除了JavaScript引擎外，浏览器还包含不同的应用程序，这些应用程序可以执行各种操作，例如发送HTTP请求，侦听DOM事件，使用以下命令延迟执行setTimeout或setInterval，缓存，数据库存储等等。浏览器的这些功能可帮助我们创建丰富的Web应用程序。 但是请考虑一下，如果浏览器必须使用相同的JavaScript线程来执行这些功能，用户体验将非常糟糕。因为即使用户只是滚动网页，后台也会发生很多事情。因此，浏览器使用低级语言C++来执行这些操作并提供干净的JavaScript API来使用。这些API被称为Web API 这些Web API是异步的。这意味着，您可以指示这些API在后台执行某些操作，并在完成后返回数据，于此同时，我们可以继续执行JavaScript代码。在指示这些API在后台执行操作时，我们必须提供一个回调函数。回调函数的责任是在Web API完成工作后执行一些JavaScript。让我们了解所有部分如何协同工作。 因此，当你调用函数时，它会被压入堆栈。如果该函数包含Web API调用，则JavaScript将使用回调函数将其控制权委托给Web API，然后移至下一行，直到函数返回内容。一旦函数击中return语句，该函数将从堆栈中弹出并移至下一个堆栈条目。同时，Web API在后台执行器工作，并记住与该工作相关联的回调函数。作业完成后，Web API 将该作业的结果绑定到回调函数，并使用该回调将消息发布到消息队列（AKA回调队列）。事件循环的唯一工作就是查看回调队列，一旦回调队列有待处理的东西，就将该回调推入堆栈。一旦堆栈唯恐，事件循环就一次将一个回调函数推入堆栈。稍后，堆栈将执行回调函数 让我们看看如何使用setTimeout Web API 逐步进行所有工作。setTimeout Web API主要用于在几秒钟后执行某些操作。一旦程序中的所有代码执行完毕（堆栈为空），就会执行此执行。setTimeout函数的语法如下。 setTimeout（callbackFunction, timeInMilliseconds）; callbackFunction是一个回调函数，将在之后执行timeInMilliseconds。让我们修改我们早期程序并使用此API。 function printHello() { console.log('from baz Hello'); } function baz() { setTimeout(printHello, 3000) ; } function bar() { baz() } function foo() { bar() } foo() 对该程序做的唯一修改是，我们将console.log（）执行延迟了3s。在这种情况下，堆栈会像一样继续建立foo() => bar() => baz()。 一旦baz开始执行并点击setTimeout API调用\b，JavaScript就会将回调函数传递给Web API并移至下一行。由于没有下一行，所有将弹出栈baz，然后bar再foo调用函数。同时Web API等待3秒中过去。一旦过了3秒，它将这个回调推入到回调队列，并且由于堆栈诶空，事件循环会将这个回调放回执行回调的堆栈上。 Philip Robers创建了一个了不起的在线工具。用于可视化JavaScript的工作原理。在链接上可以找到我们上面的示例。 说到Node.js，它必须做更多的事情，因为Node承诺了更多。对于浏览器，我们只能在后台进行操作。但是Node中，即使是简单的JavaScript程序，我们也几乎可以在后台执行大多数操作。但是，如何运作？ Node.js使用Google的V8引擎来提供JavaScript运行时，但不进依赖于它的事件循环。它使用libuv库（用C编写）与V8事件循环一起使用，以扩展可以在后台执行的操作。Node遵循与Web API相同的回调方法，并且以与浏览器类似的方式工作。 如果将浏览器图与上述节点图进行比较，可以看到相似之处。整个右侧部分看起来像Web API，但它还包含事件队列（回调队列/消息队列）和事件循环。但是V8，事件队列和事件循环在单线程上运行，而工作线程负责提供异步I/O操作。这就是为什么据说Node.js具有非阻塞事件驱动的异步I/O体系结构。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Webpack/":{"url":"Webpack/","title":"Webpack","keywords":"","body":"Webpack Webpack5更新日志 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Webpack/webpack5更新日志.html":{"url":"Webpack/webpack5更新日志.html","title":"Webpack5 更新日志","keywords":"","body":"Webpack5更新日志 转载 webpack 团队于北京时间 10 月 12 日凌晨发布了 v5.0.0-beta.0 版本，本文译自 webpack/changelog-v5。此部分主要面向非插件开发的 webpack 使用者。 简要说明 此版本重点关注以下内容： 我们尝试通过持久化存储优化构建性能。 我们尝试采用更好的算法与 defalut 来改善长效缓存。 我们尝试通过更好的 Tree Shaking 和代码生成来改善 bundle 的大小。 我们尝试清除内部结构中奇怪的代码，同时在不影响 v4 功能基础上实现了新特性。 我们目前尝试通过引入破坏性更改来为新特性做准备，以便于我们能尽可能长期地使用 v5。 主要更改 移除废弃的代码 v4 中所有废弃的代码均已删除。 迁移：以确保你的 webapck 4 不打印弃用警告。 以下是已删除但在 v4 中没有弃用警告的内容： 现在必须为 IgnorePlugin 和 BannerPlugin 传递一个 options 对象。 自动一处Node polyfills 早期，webpack的目的是允许在浏览器中运行大多数node模块，但是模块整体格局发生了变化，现在许多模块的主要用途是以编写前端为目的 webpack 虽然这使得为node便携模块变得简单，但它会将超大的polyfill添加到package中。在许多情况下，这些polyfill并非必要 webpack 5会停止自动polyfill这些核心模块，并专注于前端兼容的模块 迁移： 尽可能擦韩国女士使用与前端兼容的模块。 可以为node核心模块手动添加polyfil。错误信息将提示如何进行此操作 package作者：在package.json中使用browser字段，以是的package与前端代码兼容。为browser提供可选的implementations/dependencies。 采用新算法生成chunk ID 以及 module ID 添加了用于长效缓存的新算法。在生产模式下，默认启用这些功能。 chunkIds: \"deterministic\", moduleIds: \"deterministic\" 此算法采用确定性的方式将短数字 ID（3 或 4 个字符）分配给 modules 和 chunks。这是基于 bundle 大小和长效缓存间的折中方案。 迁移： 最好使用 chunkIds 和 moduleIds 的默认值。你还可以选择使用旧的默认值，chunkIds: \"size\", modules: \"size\"，这将生成较小的 bundle，但这会使得它们频繁地进行缓存。 以新算法混淆export名称 添加了新算法来处理export的名称。默认情况下启用 如果可能，它将以确定性方式破坏export的名称 迁移：不需要进行任何操作 为chunk IDs命名 在开发模式下默认启用，以新的算法为 chunk id 命名，给 chunk（以及文件名）提供易于理解的名称。module ID 由其相对于 context 的路径决定。chunk ID 由 chunk 的内容决定。 因此，你不再需要使用 import(/ webpackChunkName: \"name\" / \"module\") 进行调试。但是，如果你要控制生产环境的文件名，那仍可使用 可以在生产中使用 chunkIds: \"named\"，但要确保在使用时不会意外地泄露有关模块名称的敏感信息。 迁移：如果你不喜欢在开发中更改文件名，则可以传递 chunkIds: \"natural\" 以使用旧的数字模式。 JSON模块 JSON 模块现在符合规范，并会在使用非默认导出时发出警告。 迁移：使用默认导出。 嵌套tree-shaking webpack 现在可以追踪对 exports 嵌套属性的访问。重新导出 namespace 对象，这可以改善 Tree Shaking 操作（未使用 export elimination 和 export mangling）。 // inner.js export const a = 1; export const b = 2; // module.js import * as inner from \"./inner\"; export { inner } // user.js import * as module from \"./module\"; console.log(module.inner.a); 在此示例中，可以在生成模式下移除 export b。 内部模块（inner-module） tree-shaking webpack 4 没有分析模块 export 与 import 之间的依赖关系。webpack 5 有一个新的选项 optimization.innerGraph，该选项在生产模式下默认启用，它对模块中的符号进行分析以找出从 export 到 import 的依赖关系。 如下述模块所示： import { something } from \"./something\"; function usingSomething() { return something; } export function test() { return usingSomething(); } 内部图算法将确定仅在使用 export 的 test 时使用 something。这样可以将更多 export 标记为未使用，并从 bundle 中删除更多的代码。 如果设置了 \"sideEffects\": false，则可以省略更多模块。在此示例中，当未使用 export 的 test 时，将忽略 ./something。 如需获取有关未使用的 export 的信息，需使用 optimization.unusedExports。如需删除无副作用的模块，需使用 optimization.sideEffects。 此方式可以分析以下符号： 函数声明（function declarations） class 声明（class declarations） 带有 export default 或带有变量声明（variable declarations）的 函数表达式（function expressions） class 语句（class expressions） /#PURE/ 表达式 局部变量（local variables） imported bindings 编译器空闲并关闭（idle and close） 现在需要再使用编译器（compilers）后将其关闭。编译器具有 enter 和 leave 空闲状态，并具有这些状态的 hook。插件可以使用这些 hook 执行不重要的工作。（即，持久化缓存将延迟存储到磁盘）。在编译器关闭时，所有剩余工作应尽快完成。回调执行时，表明关闭已完成。 插件及其各自的作者应该会期望某些用户可能会忘记关闭编译器。因此，所有工作最终也应该在空闲时完成。当工作完成时，应防止进程退出。 当传递 callback 时，webpack() 实例会自动调用 close。 迁移：使用 node.js API 时，请确保在完成后调用 Complier.close。 改进代码生成 此版本添加了新的选项 output.ecmaVersion。它允许为 webpack 生成的运行时代码指定最大 EcmaScript 版本。 webpack 4 仅能于生成 ES5 的代码。webpack 5 现支持 ES5 或 ES2015 的代码。 默认配置将生成 ES2015 的代码。如果你需要支持旧版浏览器（例如，IE11），则可以将其降为 output.ecmaVersion: 5。 默认配置将生成 ES2015 的代码。如果你需要支持旧版浏览器（例如，IE11），则可以将其降为 output.ecmaVersion: 5。 生产模式中的默认压缩（default minimizing）也使用 ecmaVersion 选项生成较小的代码。（自 alpha.31 起） chunk 分割以及 module size 与之前展示单个数值相比，模块现在以更好的方式展示其 size。除此之外，现在也拥有了不同类型的 size。 目前，SplitChunksPlugin 已知道如何处理这些不同的 size，并将它们应用于 minSize 和 maxSize。默认情况下，仅处理 javascript 的 size，但你可以传递多个参数来管理它们： minSize: { javascript: 30000, style: 50000, } 迁移：检查构建中使用了哪些类型的 size，并在 splitChunks.minSize 和可选的 splitChunks.maxSize 中进行配置。 持久化缓存 目前包含文件系统缓存。它是可选的，可以通过以下配置启用： cache: { // 1. 设置缓存类型为 filesystem type: \"filesystem\", buildDependencies: { // 2. 将你的配置添加为 buildDependency 以在更改配置时，使得缓存失效。 config: [__filename] // 3. 如果你还有其他需要构建的内容，可以在此处添加它们 // 请注意，loader 和所有模块中配置中引用的内容会自动添加 } } 重要内容： 默认情况下，webpack 会假定其所处的 node_modules 目录仅由包管理器修改。针对 node_modules 目录，将跳过哈希和时间戳处理。出于性能方面考虑，仅使用 package 的名称和版本。symlinks（例如，npm/yarn link）很友好。除非你使用 cache.managedPaths: [] 选项取消此优化，否则请不要直接在 node_modules 中编辑文件。 默认情况下，缓存将分别存储在 node_modules/.cache/webpack 中（当使用 node_modules 时）和 .pnp/.cache/webpack（当使用 Yarn PnP 时，自 alpha.21 起）。你可能永远不必手动删除它。 当使用 Yarn PnP webpack 时，如果 yarn 的缓存不可变（通常不会发生变化）。你可以通过 cache.immutablePaths: [] 退出此优化。 用于 single-file-target 的 chunk 分割 目前，仅允许启动单个文件 target（如 node，WebWorker，electron main）支持在运行时自动加载引导程序所需的相关代码片段。 这允许对带有 chunks: \"all\" 的 target 使用 splitChunks。 值得注意的是，由于 chunk 加载是异步的，因此这也会使初始估算也为异步操作。当使用 output.library 时，这可能会出现问题，因为导出的值的类型目前为 Promise。从 alpha.14 开始，这将不适用于 target: \"node\"，因为 chunk 加载在此 target 下为同步。 更新解析器 enhanced-resolve 已更新至 v5。具体改进如下： 当使用 Yarn PnP 时，解析器将直接处理无需其他插件 此 resolve 可追踪更多的依赖项，例如文件缺失 别名（aliasing）可能包含多种选择 可以设置别名（aliasing）为 false 性能提升 不包含 JS 的 chunk 不包含 JS 代码的 chunk 将不再生成 JS 文件。 实验阶段特性 并非所有特性从开始就文档。在 webpack 4 中，我们添加了实验性功能，并在 changelog 中指出它们是实验性的，但是从配置中并不能很清楚的了解这些功能是实验性的。 并非所有特性从开始就文档。在 webpack 4 中，我们添加了实验性功能，并在 changelog 中指出它们是实验性的，但是从配置中并不能很清楚的了解这些功能是实验性的。 并非所有特性从开始就文档。在 webpack 4 中，我们添加了实验性功能，并在 changelog 中指出它们是实验性的，但是从配置中并不能很清楚的了解这些功能是实验性的。 以下实验性功能将随 webpack 5 一同发布： 像 webpack 4 一样对 .mjs 提供支持（experiments.mjs） 像 webpack 4 一样对旧版 WebAssembly 提供支持（experiments.syncWebAssembly） 根据更新规范[2] 对新版 WebAssembly 提供支持（experiments.asyncWebAssembly） 这使得 WebAssembly 模块成为异步模块 Top Level Await[3] Stage 3 阶段提案（experiments.topLevelAwait） 在顶层使用 await 使模块成为异步模块 使用 import 引入异步模块（experiments.importAsync） 使用 import await 引入异步模块（experiments.importAwait） asset 模块类似类似于 file-loader（experiments.asset）（自 alpha.19 起） 导出 bundle 作为模块（experiments.outputModule）（自 alpha.31 起） 这将从 bundle 中移除 IIFE 的包装器，强制执行严格模式，通过 进行懒加载，并在 module 模式下将其进行压缩 请注意，这也意味着针对 .mjs 的支持和 WebAssembly 的支持将被默认禁用。 Stats chunk 间关系默认情况下是隐藏的。可以使用 stats.chunkRelations 进行切换。 Stats 现阶段可以区分 files 和 auxiliaryFiles。 （自 alpha.19 起） 默认情况下，Stats 会隐藏模块和 chunk id。可以使用 stats.ids 进行切换。 所有模块的列表均按照到 entrypoint 的距离排序。可以使用 stats.modulesSort 进行切换。 chunk 模块列表和 chunk 根模块列表分别根据模块名进行排序。可以分别使用 stats.chunkModulesSort 和 stats.chunkRootModulesSort 进行更改。 在串联模块中，嵌套模块列表进行拓扑排序。可以通过 stats.nestedModulesSort 进行更改。 chunks 和 assets 会显示 chunk id 的提示。 最低 Node.js 版本 Node.js 的最低支持版本从 6 变更为 8。 迁移：升级到最新的 node.js 可用版本。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Webpack/Webpack中实现静态资源内联.html":{"url":"Webpack/Webpack中实现静态资源内联.html","title":"Webpack 中实现静态资源内联","keywords":"","body":"Webpack中实现静态资源内联 什么是静态资源内联（inline source） 就是将一个资源以内联的方嵌入进另一个资源里面 JS、HTML、CSS、图片和字体 // index.css .search { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABJ0lEQVQ4T6XSsUoEMRAG4H/ClZaLmbSW1pZ6+gAnFrK+gZXoK6jvIILgE6gIcnYWgmJno6AgYp1Z2EcIGQnsHbuaQ9abMkO+TGaGMGfQnPfxC3DOrajqPoB1AArgnohOvffPucc6ADMfAjgCUMYYH9MFY8wagEsAxyKScp2YAtbaERGNRST7LWZWVd2squq2LbSBMyK6E5GrXKnW2i1jzMh7v5sFmPkzhDCs69rngKIo3GAweBKRpVnAVwhh9Q/gRUQWs4Bz7jzGeFNV1ThXATOXAA5EJDV1Gr2aSETb3vvrLJAOmTmNKY2yVNUHVSVjzBDABYA3ADsi8j4TSIlmkfYAbABYUNUPACdE9NpAHaTXKjPz8k+kF9B8s4P0BibIpBf/AtpN/AYx54AR58WxmQAAAABJRU5ErkJggg==) no-repeat; } 资源内联的意义 工程维护 我们去打包多页面应用的时候会借助html-webpack-plugin，每个页面会有一个HTML模板与之对应。每个HTML都会包含很多相似的内容，比如meta信息、或SSR时需要用的有些占位符等等。试想一下，如果将下面这段meta代码分别复制一份放到每个HTML模版里面将会对代码维护造成的影响。 这个时候推荐的做法是维护一份meta.html，将上面的这个代码内容放置进去。每个HTML模板将meta.html片段内联进去 页面加载新能 资源内联的第二点意义在于可以减少HTTP的请求数，当然如果你的网站有使用HTTP2这点的意义可能不会那么大。将各种小图片、小字体（比如：小于5k）在生产环境base64到代码里面可以极大的减少页面的请求数量，从而提神页面的加载事件 页面加载体验 浏览器器解析HTML源码是从上到下戒子，因此我们会把CSS放大头部，JS放置到底部。以SSR场景为例，如果不将打包的CSS内联HTML里面，HTML出来的时候页面的结构已经有了，但是还需要发送一次请求去请求CSS，这个时候就会出现页面闪烁，网络情况差的时候更明显 资源内联的类型 HTML JS CSS 图片、字体 如果你曾经使用过FIS或者看过FIS的文档，你会发现FIS对于资源内联的支持非常棒 HTML内联 借助raw-loader实现的内联语法： // 内联HTML ${ require('raw-loader!./meta.html')} // 内联JS，如果有ES6语法则需要babel-loader转换 ${require('raw-loader!babel-loader!../node_modules/lib-flexible/flexible.js')} CSS内联 通常情况下，为了更好的加载体验，我们桂江打包好的CSS内联到HTML头部，这样HTML加载完成CSS就可以直接渲染出来，避免页面闪动的情况， CSS内联的核心思路是：将页面打包过程产生的所有CSS提取成一个独立的文件，然后将这个CSS文件内联进HTML的head里面。这里需要借助mini-css-extract-plugin和html-inline-css-webpack-plugin来实现CSS的内联功能 // webpack.config.js const path = require('path'); module.exports = { entry: { index: './src/index.js', search: './src/search.js' }, output: { path: path.join(__dirname, 'dist'), filename: '[name]_[chunkhash:8].js' }, mode: 'production', plugins: [ new MiniCssExtractPlugin({ filename: '[name]_[contenthash:8].css' }), new HtmlWebpackPlugin(), new HTMLInlineCSSWebpackPlugin() ] }; 图片、字体内联 基础版： 图片和字体的内联可以借助url-loader，比如你可以通过修改webpack配置让小于10k的图片或者字体在构建阶段自动base64. // webpack.config.js const path = require('path'); module.exports = { entry: { index: './src/index.js', search: './src/search.js' }, output: { path: path.join(__dirname, 'dist'), filename: '[name]_[chunkhash:8].js' }, mode: 'production', plugins: [ new MiniCssExtractPlugin({ filename: '[name]_[contenthash:8].css' }), new HtmlWebpackPlugin(), new HTMLInlineCSSWebpackPlugin() ] }; 增强版： 不过url-loader做资源哪辆的缺陷就是不能个性化的设置某张图片自动编码，针对这个问题，我们可以借鉴下FIS的语法糖，实现？_inline的语法糖，引用某个图片的时候看到这个后缀则自动的将这张图片进行base64编码。这个功能实现其俩也很简单，可以参考inline-file-loader，核心代码： export default function loader(content) { const options = loaderUtils.getOptions(this) || {}; validateOptions(schema, options, { name: 'File Loader', baseDataPath: 'options', }); const hasInlineFlag = /\\?__inline$/.test(this.resource); if (hasInlineFlag) { const file = this.resourcePath; // Get MIME type const mimetype = options.mimetype || mime.getType(file); if (typeof content === 'string') { content = Buffer.from(content); } return `module.exports = ${JSON.stringify( `data:${mimetype || ''};base64,${content.toString('base64')}` )}`; } 有了图片的内联功能，我们可以将前面的搜索icon图标内联的额写法修改： // index.css .search { background: url(./search-icon.png?__inline) no-repeat; } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"CSS/":{"url":"CSS/","title":"CSS","keywords":"","body":"布局 flex布局 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"CSS/flex布局.html":{"url":"CSS/flex布局.html","title":"flex 布局","keywords":"","body":"flex布局 flex布局 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"CSS/移动端适配1px的问题.html":{"url":"CSS/移动端适配1px的问题.html","title":"移动端适配 1px 的问题","keywords":"","body":"移动端适配1px的问题 移动项目中的边框全部都变粗了 造成边框变粗的原因 其实这个原因很简单，因为CSS中的1px并不等于欧移动设备的1px，这些由于不同的手机有不同的像素密度。在window对象中有一个devicePixelRatio属性，它可以反应CSS中的像素与设备像素 devicePixelRatio的官方定义为：设备无力橡树和设备独立像素的比例，也就是devicePixelRatio=物理像素/独立像素 以iPhone6为例 iPhone6的屏幕宽度为375px，设计师做的视觉稿一般是750px,也就是2px，这个时候设计师在视觉稿上画了1px的边框，于是你就写了“border-width:1px”，所以，1px边框问题产生了 对设计师来说它的1px是相对于750px的（物理像素），对你来说你的1px是相对于375px的（css像素）“实际上你应该是border-width:0.5px”。 解决边框变粗的6种办法 1. 0.5px边框 在2014年的WWDC，“设计响应的Web体验”一讲中，，Ted O’Connor 讲到关于“retinahairlines”（retina 极细的线）：在retina屏上仅仅显示1物理像素的边框，开发者应该如何处理呢。 0.5px边框 问题是 retina 屏的浏览器可能不认识0.5px的边框，将会把它解释成0px，没有边框。包括 iOS 7 和之前版本，OS X Mavericks 及以前版本，还有 Android 设备。 解决方案： 用小数点写边框 媒体查询，加小数点的写法： .border { border: 1px solid #999 } @media screen and (-webkit-min-device-pixel-ratio: 2) { .border { border: 0.5px solid #999 } } @media screen and (-webkit-min-device-pixel-ratio: 3) { .border { border: 0.333333px solid #999 } } 优点：方便噻 缺点: 安卓与低版本IOS不适用, 这个或许是未来的标准写法 2. 使用border-image实现 弄出1px像素边框的实质是弄出0.5px这样的边框，所以我们可以利用类似于这样的图片，使得“border-image-slice”为2，那么实际上边框有一半是透明的，即可得到我们想要的“1px边框” 1像素边框 .test{ border: 1px solid transparent; border-image: url('./border-1px.png') 2 repeat; } 修改颜色麻烦, 需要替换图片 圆角需要特殊处理，并且边缘会模糊 3. 使用background-image实现 background-image 跟border-image的方法一样，你要先准备一张符合你要求的图片。然后将边框模拟在背景上。 样式设置： .background-image-1px { background: url(../img/line.png) repeat-x left bottom; -webkit-background-size: 100% 1px; background-size: 100% 1px; } 优点： 缺点： 修改颜色麻烦, 需要替换图片 圆角需要特殊处理，并且边缘会模糊 4. 多背景渐变实现 与background-image方案类似，只是将图片替换为css3渐变。设置1px的渐变背景，50%有颜色，50%透明。 样式设置： .background-gradient-1px { background: linear-gradient(180deg, black, black 50%, transparent 50%) top left / 100% 1px no-repeat, linear-gradient(90deg, black, black 50%, transparent 50%) top right / 1px 100% no-repeat, linear-gradient(0, black, black 50%, transparent 50%) bottom right / 100% 1px no-repeat, linear-gradient(-90deg, black, black 50%, transparent 50%) bottom left / 1px 100% no-repeat; } /* 或者 */ .background-gradient-1px{ background: -webkit-gradient(linear, left top, left bottom, color-stop(.5, transparent), color-stop(.5, #c8c7cc), to(#c8c7cc)) left bottom repeat-x; background-size: 100% 1px; } 优点： 缺点： 5. 使用box-shadow模拟边框 利用css 对阴影处理的方式实现0.5px的效果 .box-shadow-1px { box-shadow: inset 0px -1px 1px -1px #c8c7cc; } 优点： 缺点： 6. viewport + rem 实现 同时通过设置对应viewport的rem基准值，这种方式就可以像以前一样轻松愉快的写1px了。 在devicePixelRatio = 2 时，输出viewport： 这种兼容方案相对比较完美，适合新的项目，老的项目修改成本过大。 对于这种方案，可以看看https://github.com/amfe/article/issues/17 优点： 缺点： 7. 伪类 + transform实现 利用伪类标签，更具父级定位，大小更具媒体查询缩放实现效果（注意别忘记了“transform-origin: left top;”） 1像素边框问题 // less .border-1px{ position: relative; &::before{ content: \"\"; position: absolute; left: 0; top: 0; width: 200%; border:1px solid red; color: red; height: 200%; -webkit-transform-origin: left top; transform-origin: left top; -webkit-transform: scale(0.5); transform: scale(0.5); pointer-events: none; /* 防止点击触发 */ box-sizing: border-box; @media screen and (min-device-pixel-ratio:3),(-webkit-min-device-pixel-ratio:3){ width: 300%; height: 300%; -webkit-transform: scale(0.33); transform: scale(0.33); } } } 需要注意的input button是没有:before,:after伪元素的 优点： 其实不止是圆角，其他的边框也可以这样做出来 缺点： 代码量也很大，占据了伪元素，容易引起冲突 8. boder-shadow 利用阴影来模拟边框 .border-1px{ box-shadow: 0px 0px 1px 0px red inset; } 2倍屏 3倍屏幕 总结 从兼容性和灵活性来考虑，个人还是推荐tranform加伪类标签的写法，节约时间成本。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/":{"url":"Node/","title":"Node","keywords":"","body":"Node By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/npm依赖管理之peerDependencies.html":{"url":"Node/npm依赖管理之peerDependencies.html","title":"npm 依赖管理之 peerDependencies","keywords":"","body":"npm 依赖管理之 peerDependencies 插件开发中经常用到 peerDependencies npm2 中 dependencies 与 peerDependencies 区别 假设我们当前的项目是 MyProject，项目中有一些依赖，比方其中有一个依赖包 PackageA，该包的package.json文件指定了对 PackageB 的依赖： { \"dependencies\": { \"PackageB\": \"1.0.0\" } } 如果我们在我们的 MyProject 项目中执行npm install PackageA, 我们会发现我们项目的目录结构会是如下形式： ;MyProject | -node_modules | -PackageA | -node_modules | -PackageB 那么在我们的项目中，我们能通过下面语句引入\"PackageA\"： var packageA = require('PackageA') 但是，如果你想在项目中直接引用 PackageB: var packageA = require('PackageA') var packageB = require('PackageB') 这是不行的，即使 PackageB 被安装过；因为 Node 只会在“MyProject/node_modules”目录下查找 PackageB，它不会在进入 PackageA 模块下的node_modules下查找。 所以，为了解决这个问题，在 MyProject 项目package.json中我们必须直接声明对 PackageB 的依赖并安装。 但是，有时我们不用在当前项目中声明对 PackageB 的依赖就可以直接引用，尤其是，PackageA 是一个类似于 grunt 的插件，例如grunt-contrib-jshint。 为什么在项目中不用声明就可以直接使用呢？这就不得不说说peerDependencies的作用了。 peerDependencies 的引入 为了解决这种问题： 于是 peerDependencies 就被引入了。例如上面 PackageA 的 package.json 文件如果是下面这样： { \"peerDependencies\": { \"PackageB\": \"1.0.0\" } } 那么，它会告诉 npm：如果某个 package 把我列为依赖的话，那么那个 package 也必需应该有对 PackageB 的依赖。 也就是说，如果你npm install PackageA，你将会得到下面的如下的目录结构： MyProject |- node_modules |- PackageA |- PackageB 你可能注意到： 在 npm2 中，即使当前项目 MyProject 中没有直接依赖 PackageB，该 PackageB 包依然会安装到当前项目的node_modules文件夹中。 下面的代码现在可以正常工作了，因为两个包在\"MyProject/node_modules\"中被安装了： var packageA = require('PackageA') var packageB = require('PackageB') 总结一句话，peerDependencies的具体作用： peerDependencies的目的是提示宿主环境去安装满足插件peerDependencies所指定依赖的包，然后在插件 import 或者 require 所依赖的包的时候，永远都是引用宿主环境统一安装的 npm 包，最终解决插件与所依赖包不一致的问题。 举个例子，就拿目前基于 react 的 ui 组件库 ant-design@3.x 来说，因该 ui 组件库只是提供一套 react 组件库，它要求宿主环境需要安装指定的 react 版本。具体可以看它 package.json 中的配置： \"peerDependencies\": { \"react\": \">=16.0.0\", \"react-dom\": \">=16.0.0\" } 它要求宿主环境安装 react@>=16.0.0 和 react-dom@>=16.0.0 的版本，而在每个 antd 组件的定义文件顶部： import * as React from 'react'; import * as ReactDOM from 'react-dom'; 组件中引入的 react 和 react-dom 包其实都是宿主环境提供的依赖包。 npm2 和 npm3 中 peerDependencies 的区别 正如上一节谈论的，在 npm2 中，PackageA 包中peerDependencies所指定的依赖会随着 npm install PackageA 一起被强制安装，所以不需要在宿主环境的package.json文件中指定对 PackageA 中 peerDependencies 内容的依赖。 但是在 npm3 中，peerDependencies的表现与 npm2 不同： npm3 中不会再要求 peerDependencies 所指定的依赖包被强制安装，相反 npm3 会在安装结束后检查本次安装是否正确，如果不正确会给用户打印警告提示。 就拿上面的例子来说，如果我们npm install PackageA安装 PackageA 时，你会得到一个警告提示说： PackageB 是一个需要的依赖，但是没有被安装。 这时，你需要手动的在 MyProject 项目的 package.json 文件指定 PackageB 的依赖。 另外，在 npm3 的项目中，可能存在一个问题就是你所依赖的一个 package 包更新了它 peerDependencies 的版本，那么你可能也需要在项目的 package.json 文件中手动更新到正确的版本。否则会出现的警告信息： By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/如何解决npm-unmet-peer-dependency.html":{"url":"Node/如何解决npm-unmet-peer-dependency.html","title":"如何解决 npm unmet peer dependency","keywords":"","body":"如何解决 npm unmet peer dependency 我在 Mac 系统上，yarn 安装依赖包，出现以下告警 UNMET PEER DEPENDENCY @webpack .... npm 不再安装对等项依赖项，因此您需要手动安装对等项依赖项，只需 npm install 对所需的 deps 进行操作，然后尝试再次安装主要对等项。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/如何发布一个npm包.html":{"url":"Node/如何发布一个npm包.html","title":"如何发布一个npm包","keywords":"","body":"转载 开始之前确保本机nodejs已经安装，检查 node -v 有输出版本号，说明已安装 首先在npm网站上注册一个账号，这个账号会在之后用到 pixiv-login pixiv-login的功能就是模拟用户登陆网站pixiv，获取cookie，安装 源码 npm install --save pixiv-login 使用： const pixivLogin = require('pixiv-login'); pixivLogin({ username: '你的用户名', password: '你的密码' }).then((cookie) => { console.log(cookie); }).catch((error) => { console.log(error); }); 开发工具 Mac平台 使用的vscode 初始化项目 mkdir npm-test cd npm-test npm init 一路回车就好 安装依赖 要模拟登陆，我们就需要一个http库，这里我选择了axios，同时获取的html字符串我们需要解析，cheerio就是首选了 npm i axios cheerio --save 现在便携index.js文件 const axios = require('axios'); const cheerio = require('cheerio'); axios.get('https://www.pixiv.net') .then(function(response) { const $ = cheerio.load(response.data); const title = $('title').text(); debugger; console.log(title); }) .catch(function(error) { console.log(error); }); 正式开始 虽然我们最后是要写一个npm包，但是首先，我们先把获取cookie的功能实现了，然后再思考怎么封装一个npm包，供其他人使用。 进入登陆页面，我们先登陆一次，看看前端向后端发送了那些数据。 这里需要注意的是，我们要勾选preserve log，这样，即使页面刷新跳转了，http请求记录仍然回记录下来 可以看到，post_key是登陆的关键点，p站使用了该值来防止CSRF post_key怎么获取呢？ 经过页面分析，发现登陆页面，有个隐藏的表单域（后来发现，其实首页就已经写出来了） 可以清除看到，post_key已经写出来了，我们只需要用cheerio解析出该input的值就OK了 const post_key = $('input[name=\"post_key\"]').val(); 获取post_key const axios = require('axios'); const cheerio = require('cheerio'); const LOGIN_URL = 'https://accounts.pixiv.net/login?lang=zh&source=pc&view_type=page&ref=wwwtop_accounts_index'; const USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'; const LOGIN_API = 'https://accounts.pixiv.net/api/login?lang=zh'; const getKey = axios({ method: 'get', url: LOGIN_URL, headers: { 'User-Agent': USER_AGENT } }).then((response) => { const $ = cheerio.load(response.data); const post_key = $('input[name=\"post_key\"]').val(); const cookie = response.headers['set-cookie'].join('; '); if (post_key && cookie) { return { post_key, cookie }; } return Promise.reject(\"no post_key\"); }).catch((error) => { console.log(error); }); getKey.then(({ post_key, cookie }) => { debugger; }) 注意：打开注册页时，注册页会返回一些cookie，这些cookie在登陆时也是需要随密码，用户名一起发送过去的 获取到了post_key，cookie，我们就可以愉快的把登陆数据发送给后台接口了 const querystring = require('querystring'); getKey.then(({ post_key, cookie }) => { axios({ method: 'post', url: LOGIN_API, headers: { 'User-Agent': USER_AGENT, 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Origin': 'https://accounts.pixiv.net', 'Referer': 'https://accounts.pixiv.net/login?lang=zh&source=pc&view_type=page&ref=wwwtop_accounts_index', 'X-Requested-With': 'XMLHttpRequest', 'Cookie': cookie }, data: querystring.stringify({ pixiv_id: '你的用户名', password: '你的密码', captcha: '', g_recaptcha_response: '', post_key: post_key, source: 'pc', ref: 'wwwtop_accounts_index', return_to: 'http://www.pixiv.net/' }) }).then((response) => { if (response.headers['set-cookie']) { const cookie = response.headers['set-cookie'].join(' ;'); debugger; } else { return Promise.reject(new Error(\"no cookie\")) } }).catch((error) => { console.log(error); }); }); 注意其中这段代码： data: querystring.stringify({ pixiv_id: '你的用户名', password: '你的密码', captcha: '', g_recaptcha_response: '', post_key: post_key, source: 'pc', ref: 'wwwtop_accounts_index', return_to: 'http://www.pixiv.net/' }) 注意：axios默认数据格式是json，如果你想发送application/x-www-form-urlencoded的数据，就需要使用querystring模块 如果一切正常，那么效果如下： 其中，的PHPESSID和device_token就是服务器端返回的登陆标识，说明我们登陆成功了 程序运行的同时，你也很可能收到P站的登陆邮件 好了，目前为止，我们已经成功获取到了cookie，实现了最基本的功能 特别注意 程序不要运行太多次，因为每次运行，你就登陆一次P站，如果被P栈监测到频繁登陆，它会开启验证码登陆模式，这时，你除了需要发送用户名和密码，还需要向后台发送验证码值 data: querystring.stringify({ pixiv_id: '你的用户名', password: '你的密码', captcha: '你还需要填验证码', g_recaptcha_response: '', post_key: post_key, source: 'pc', ref: 'wwwtop_accounts_index', return_to: 'http://www.pixiv.net/' }) 也就是，captcha字段不再是空值了！ 基本功能的完整代码 const axios = require('axios'); const cheerio = require('cheerio'); const querystring = require('querystring'); const LOGIN_URL = 'https://accounts.pixiv.net/login?lang=zh&source=pc&view_type=page&ref=wwwtop_accounts_index'; const USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'; const LOGIN_API = 'https://accounts.pixiv.net/api/login?lang=zh'; const getKey = axios({ method: 'get', url: LOGIN_URL, headers: { 'User-Agent': USER_AGENT } }).then((response) => { const $ = cheerio.load(response.data); const post_key = $('input[name=\"post_key\"]').val(); const cookie = response.headers['set-cookie'].join('; '); if (post_key && cookie) { return { post_key, cookie }; } return Promise.reject(\"no post_key\"); }).catch((error) => { console.log(error); }); getKey.then(({ post_key, cookie }) => { axios({ method: 'post', url: LOGIN_API, headers: { 'User-Agent': USER_AGENT, 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8', 'Origin': 'https://accounts.pixiv.net', 'Referer': 'https://accounts.pixiv.net/login?lang=zh&source=pc&view_type=page&ref=wwwtop_accounts_index', 'X-Requested-With': 'XMLHttpRequest', 'Cookie': cookie }, data: querystring.stringify({ pixiv_id: '你的用户名', password: '你的密码', captcha: '', g_recaptcha_response: '', post_key: post_key, source: 'pc', ref: 'wwwtop_accounts_index', return_to: 'http://www.pixiv.net/' }) }).then((response) => { if (response.headers['set-cookie']) { const cookie = response.headers['set-cookie'].join(' ;'); console.log(cookie); } else { return Promise.reject(new Error(\"no cookie\")); } }).catch((error) => { console.log(error); }); }); 封装成一个npm包 登陆P站获取cookie这个功能，如果我们想让其他开发者也能方便调用，就可以考虑其封装为一个npm包发布出去 目录结构： 发布npm包 README.md 每个npm包，一般都需要配一段介绍文字，来告诉使用者如何安装使用，比如lodash的首页 新建一个README.md，填写相关信息 有时，我们会看到一些npm包有很漂亮的版本号图标： 这些图标shieds上制作 登陆该网站，下拉到最下面 输入你想要的文字，版本号，颜色，然后点击按钮 就可以得到图片的访问地址了 修改刚才的README.md，加上我们的版本号 .gitignore node_modules以及.vscode是完全不用上传的，所以为了防止发布时带上这些文件夹，我们要新建一个.gitignore .vscode/ node_modules/ 包发布 在终端输入 npm adduser 输入用户名，密码，邮箱即可登陆成功 这里还有一个坑！ 如果你的npm使用的是淘宝镜像，那么是无法登陆成功的 最简单的解决方法： npm i nrm -g nrm use npm nrm是个npm镜像管理工具，可以很方便的切换镜像源 登陆成功后，输入 npm whoami 如果出现了你的用户名，说明你已经成功登陆了 特别 因为包名字唯一，否则发布不了 修改pacakge.json文件的name字段 npm publish 即可发布成功啦！ 更新已发布的包 更新包和发布包的命令是一样的，更新包只需修改package.json里面的version字段,也可以使用npm自带的版本控制命令修改版本号，更新的步骤为 修改版本号 npm publish npm version npm 官方提供了npm version来进行版本控制，其效果跟手动修改package.json里面的version字段是一样的，好处在于，可以在构建过程中用命令自动修改，而且具有语义化即Semantic versioning npm version [ | major | minor | patch | premajor | preminor | prepatch | prerelease | from-git] 其语义为： major：主版本号（大版本） minor：次版本号（小更新） patch：补丁号（补丁） premajor：预备主版本 preminor: 预备次版本 prepatch：预备补丁版本 prerelease：预发布版本 如初始版本为1.0.0，执行相关类型命令后，对应的语义为： npm version patch // 1.0.1 表示小的bug修复 npm version minor // 1.1.0 表示新增一些小功能 npm version mmajor // 2.0.0 表示大的版本或大升级 npm version preminor // 1.1.0-0 后面多了个0，表示预发布 可以在当前模块的package.json里面看到相应的版本变化 撤销发布 由于撤销发布会让把要撤销的包作为依赖的包不能正常工作，所以npm官方对包的撤销是有限制的： 不允许撤销发布已经超过24小时的包 如果在24小时内确实要撤销，需要--force参数 即使撤销了发布的包，再次发布的时候也不能与之前被撤销的包的名称/版本其中之一相同，因为这两者构成的唯一性已经被占用，官方并没有随着测笑而删除 npm publish 撤销发布的命令为 npm unpublish npm unpublish my-test-project // 报错 npm ERR! Refusing to delete entire project. npm ERR! Run with --force to do this. npm ERR! npm unpublish [/][@] // 加 --force参数重新撤销发布 npm unpublish my-test-project --force npm WARN using --force I sure hope you know what you are doing. - my-test-project npm deprecate npm unpublish的推荐替代命令： npm deprecate [@] 这个命令，并不会在npm上里撤销已有的包，但会在任何人尝试安装这个包的时候得到deprecated的警告，例如： npm deprecate my-test-project 'this package is no longer maintained' By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/node版本管理工具.html":{"url":"Node/node版本管理工具.html","title":"Node版本管理工具","keywords":"","body":"node版本管理工具 https://zhuanlan.zhihu.com/p/63403762 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/clinic快速定位性能问题.html":{"url":"Node/clinic快速定位性能问题.html","title":"使用node-clinic快速定位性能问题","keywords":"","body":"使用node-clinic快速定位性能问题 安装 npm install -g clinic 启动服务 clinc doctor -- node app.js 跑一次压测 ab -c 10 -n 10000 http://127.0.0.1:7001/DouyinApp By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/cluster：扩展你的node应用.html":{"url":"Node/cluster：扩展你的node应用.html","title":"cluster：扩展你的node应用","keywords":"","body":"cluster：扩展你的node应用 负载均衡一个HTTP服务器 让我们使用cluster模块克隆和负载一个简单的HTTP服务器。这里有一个很简单的例子，稍微改动了，模拟在响应之前CPU的工作： // server.js const http = require('http'); const pid = process.pid; http.createServer((req, res) => { for (let i = 0, i { console.log(`started process`, pid); }) 为了核实我们创建的均衡器是在工作，我在返回结果里面加了进程的pid，确定是应用的拿一个实例处理该请求。 在我们创建的集群将这个服务器克隆为多工作进程之前，让我们先做一个简单的基准测试-这个服务器每秒中可以处理多少个请求。我们可以使用。运行哪个简单的服务器server.js之后，运行ab命令： ab -c200 -t10 http://localhost:8080/ 这个命令会每10秒钟发送200个并发请求来测试负载服务器。 现在我们有一个基准性能参照，我们可以使用cluster模块通过克隆策略扩展一个应用。 依据上面的例子-server.js，我们可以用这个内容为主进程创建一个新的文件（cluster.js）: // cluter.js const cluster = require('cluster'); const os = require('os'); if (cluster.isMaster) { const cpus = os.cpus().length; console.log('forking for ', cpus, ' CPUS'); for (let i =0; i 在 cluster.js 文件中，我们首先导入 cluster 和 os 模块。我们使用 os 模块通过 os.cpus() 获取 CPU 的内核数量。 cluster 模块给我们一个易得的 Boolean 标识 isMaster 判断 cluster.js 文件是否在主进程上面加载的。第一次执行这个文件，我们将会在主进程上面执行并且 isMaster 标识将会被设置为 true。在这个例子中，我们可以指导主进程根据 CPU 核数多次衍生我们的服务器。 现在我们仅仅通过 os 模块获取了 CPU 核的数量，然后用一个循环遍历这个数字，调用 cluster.fork 方法。for 循环根据系统中 cpu 的数量创建尽可能多的工作进程，充分利用多核的优势。 当在主进程里执行 cluster.fork 之后，当前的文件 cluster.js 将会被再次执行，不过这次是在 worker 模式执行的 isMaster 被设置为了 false。这里有另外一个标识被设置为了 true，如果你用到的话，是 isWorker。 当应用作为一个 worker 运行时，它可以做些实际的工作。这是我们需要定义我们服务逻辑的地方，例如，我们可以通过导入之前就已经存在的 server.js 文件。 基本上就是这样。在一个机器里面充分利用处理能力就是这么简单。为了测试 cluster，运行 cluster.js 文件： node cluster.js 当我们多次请求 web server，请求将会被带有不同进程 id 的不同工作进程所处理。工作子进程并不完全是按顺序轮流处理的，因为 cluster 模块在选择了下一个工作进程的时候做了很多优化，但是负载将会分布在不同的工作进程之间。 我们可以用上面同样的 ab 命令测试集群负载测试： 向所有的工作进程广播信息 在主进程和工作进程之间交流很容易，因为在cluster模块底层使用的是child_process.fork API，意味着在主进程和工作进程之间有通信通道。 基于上面server.js/cluster.js的例子，我们可以使用cluster。works获取工作进程对象的列表，是一个包含所有工作进程的引用的对象，并且可以被用来读取工作进程的信息。既然我们在主进程与所有工作进程之间有通信通道，仅仅使用一个循环就可以给它们广播信息，例如： Object.values(cluster.works).forEach(work => { worker.send('hello work', worker.id); }) 我们简单地使用Object.values来从cluster.workers对象获取所有工作进程的一个数组。然后，对于每个工作进程，我们可以使用send函数发送任何我们想要发送的值。 这个工作进程文件里面，我们的例子是server.js，为了从主进程读取收到的信息，我们可以在全局process对象上面注册一个message事件，例如： process.on('message', msg => { console.log(`Messge from master: ${msg}`); }) 增加服务器的可用性 在一个node应用内运行一个单一实例，其中一个问题就是当实例崩溃时，应用不得不重启，这意味这及时这个过程时自动的，这两个行为之间应该也有一些停机的时间。 这种情况业适用于但服务器部署新的代码必须重新启动。使用一个实例，将会有停机时间影响系统的可用性。 当我们有多个实例时，用一些额外少量的代码很容易增加系统的可用性。 为了在服务器进程中随机的模拟进程崩溃，我们可以在一个随机时间的定时器内出发process.exit: // in server.js setTimeout(() => { process.exit(1); // death by random timeout }, Math.random() * 10000); 当一个工作进程像这样退出时，主进程将会在cluster对象上使用exit事件被通知到。我们可以为这个时间注册一个处理器当任何一个工作进程退出时衍生出一个新的工作进程。 // 在 isMaster=true 块里面的 for 循环后面 cluster.on('exit', (worker, code, signal) => { if(code !== 0 && !worker.exitedAfterDisconnect) { console.log(`工作进程 ${worker.id} 崩溃了，正在开始一个新的工作进程`); cluster.fork(); } }) 上面添加了一个很好的判断条件，确保工作进程崩溃了，而不是手动断掉连接或者被主进程杀掉了。例如，主进程可能根据负载模式判断我们使用了太多的资源，它认为在这种情况下需要杀掉一些工作进程。就这样做，我们可以在任何一个工作进程上面使用disconnect方法，在这种请框下，exitAfterDisconnect标示将被设置为true。上面的判断保证在这些情况下不会衍生一个心的工作进程。 如果我们运行 cluster（在 server.js 中随机让一些工作进程崩溃），随机的几秒之后，工作进程将会崩溃掉并且主进程马上衍生一个新的工作进程增加系统的可用性。你可以使用同样的 ab 命令测量集群的可用性并且看下有多少请求，服务器将不能处理（因为一些不幸的请求将不得不面对崩溃的情形并且很难避免）。 当我测试的时候，10 秒，测试间隔 200 个并发请求，超过 1800 个请求中只有 17 个请求失败了。 超过了 99% 的可用性。仅仅添加了几行代码，现在再也不需要担心进程崩溃了。主进程将会帮我们留意这些流程。 不停机重启 当我们想要重启所有的工作进程，比如，我们需要部署一些新的代码。这种情况呢？ 我们有多个实例在运行，因此我们可以一次只启动他们之中的一个，而不是全部重启，允许当一个工作进程重启的时候，其他的工作进程继续服务请求。 用集群模块实现这个很容易。因为我们不想重启主进程，我们需要一个方法发送给主进程一个命令指导它去重启所有的工作进程。这个在 Linux 系统上面容易，我们可以简单的监听像 SIGUSR2 事件，这个事件在我们在进程 id 上面使用 kill 命令时，会触发并且将那个标志发送过去： // In Node process.on('SIGUSR2', () => { //... }); //触发 $ kill -SIGUSR2 PID 这种方式，主进程不会被杀掉并且有了一个我们可以指导它做一些事情的方式。在这里使用的 SIGUSR2 是一个合适的标志，因为这将会是一个用户命令。如果你在疑惑为什么不是 SIGUSR1，因为被 Node 用来调试（debugger）用了，需要避免冲突。 不幸的是，在 windows 上面，这些进程标志（signal）不被支持，我们需要找到另一种方式命令主进程做一些事情。这里有一些替代的方案。例如，我们可以使用标准的输入或者 socket 输入。或者我们可以模拟一个 process.pid 文件的退出并且监视它的删除事件。但是为了让个示例简单，我们假设这个服务器在 Linux 上面运行。 Node 在 windows 上面工作的非常好，但是我认为在 Linux 平台托管生产环境的 Node 应用更加安全。并不是 Node 本身的问题，但是很多其他的生产工具在 Linux 上面更加稳定。这是我个人的观点，你可以忽略掉。 顺便说下，在最近版本的 windows 上面，你可以使用一个真正的 Linux 子系统并且它工作的非常好。我自己测试过，真是令人印象深刻。如果你在 window 上面开发一个 Node 应用，试一下在 windows 上面的 bash 命令。 在我们的例子中，当主进程接受到一个 SIGUSR2 标志，意味着是时候重启它的工作进程了，但是我们需要一次只重启一个工作进程。这也简单地意味着主进程在重启完当前的工作进程后应该重启下一个工作进程。 为了开始这个任务，我们需要使用 cluster.workers 对象得到一个所有当前进程地引用，我们可以在一个数组中存储下来。 const workers = Object.values(cluster.workers); 然后，我们可以创建一个接受一个需要重启的工作进程的索引的 restartWorker 函数。我们在它已经为下个工作进程准备好了，然后调用函数，在序列里面做重启的工作。下面有一个我们可以在示例中使用的 restartWorker 函数： const restartWorker = (workerIndex) => { const worker = workers[workerIndex]; if(!worker) return; worker.on('exit', () => { if (!worker.exitedAfterDisconnect) return; console.log('退出的进程', worker.process.pid); cluster.fork().on('listening', () => { restartWorker(workerIndex + 1); }); }); worker.disconnect(); }; restartWorker(0); 在 restartWorker 函数内部，我们获取到一个需要重启的工作进程的引用，我们会从序列里面递归地调用这个函数，我们需要一个停止的条件。当我们不再需要重启一个工作进程时，我们可以返回（return）。我们基本上想要断掉（disconnect）这个工作进程（使用 worker.disconnect）时，但是在重启下一个工作进程之前，我们需要衍生一个新的工作进程代替当前这个我们我想要断掉的工作进程。 我们可以使用工作进程上面的 exit 事件，当前的进程退出后，衍生一个新的工作进程，但是我们不得不保证这个 exit 行为实际上是被一个正常 disconnect 调用之后触发的。我们可以使用 exitedAfterDisconnect 标识。如果这个标识不是 true，那么这个退出行为是被其他一些事情引起的而不是我们的 disconnect 调用，我们应该立马退出函数。但是如果这个标识是 true，我们可以继续并且衍生一个新的工作进程代替刚刚我们断掉的那个工作进程。 当这个新的衍生的工作进程准备好后，我们再重启下一个。然后记住衍生的进程不是同步的，因此我们不能再衍生调用之后直接重启下一个工作进程。应该是，我们在一个新衍生的工作进程上面模拟一个 listening 事件，告诉我们这个工作进程连接上了并且准备好了。当这个事件触发时，我们可以安全地重启序列中的下一个工作进程。 这就是我们需要的不停机重启。为了测试，你需要读取主进程的process id 并且将它发送到 SIGUSR2 标志： console.log(`Master PID: ${process.pid}`); 开启这个集群，复制主进程 id，并且使用 kill -SIGUSR2 PID 命令重启这个集群。当重启集群时，为了看下重启进程对可用性造成的影响，你也可以使用同样的 ab 命令。剧透警告，你应该会有 0 个失败的请求： 进程监视器，如 PM2，我在生产中使用，我们经历的到目前为止，所有的任务都非常简单，提供很多监视 Node.js 应用健康状况的特性。例如，使用 PM2，为任何一个应用启动一个集群，你所需要做的使用 -i 参数： pm2 start server.js -i max 并且不停机重启，仅仅使用这个有魔法的命令： pm2 reload all 然而，我发现当你使用这些命令时，有助于你首先理解背后发生了什么。 共享状态和粘性负载均衡（Sticky Load Balancing） 美好的事情总实有代价的。当我们负载均衡一个 Node 应用时，我们失去了一些仅仅适用于单进程的特性。这个问题某种程度上和我们所知道的在其他语言中的关于线程之间共享数据线程安全相似。在我们的示例中，是在工作进程之间共享数据。 例如，设置过集群，我们不在内存中缓存一些东西，因为每一个工作进程都有自己的内存空间。如果我们在一个工作进程的内存里面缓存东西，其他的工作进程将访问不到缓存的东西。 如果我们需要在设置过集群的应用中缓存东西，我们不得不使用一个分开的实体（entity）并且从所有的工作进程里面读取那个实体的 API。这个实体可以是个一个数据库服务器或者如果想使用内存中的缓存，你可以使用一个像 Redis 的服务器或者创建一个专有的带有读写 API 的 Node 进程，用来让所有其他的进程之间相互交流。 不要因为为你需要缓存的应用使用一个分离的实体是分解你应用的可伸缩性的一部分，把这个看成一个缺点。即使你在一个单一的核心机器上面运行，也是应该做的事情。 不同于缓存，当我们在一个集群上面运行时，通常有状态的沟通成为了一个问题。因为不保证交流的一方是同一个工作进程，在任何一个工作进程上创建一个有状态的通道不是一个选择。 最常见的例子是对用户进行身份验证。 使用集群，身份验证的请求来自于主进程均衡器，被发送给工作进程，假设在这个例子中是 A。 工作进程 A 现在识别到了用户的状态。然而，相同的用户发出另一个请求时，负载均衡器最终把请求发送到其他的工作进程中去了，而那些工作进程并没有验证他们的身份。在一个实例内存中保持一个对经过验证的用户会话的引用不再工作了。 这个问题有多种解决办法。我们在一个共享的数据库或者一个 Redis 节点通过存储用户的会话信息，在很多工作进程之间共享状态。然而，应用这个策略需要一些代码的变动，并不总是一个选择，对吗？ 如果你不能做一些代码的变动来实现一个会话的共享存储，这里有一个变动很小但很高效的策略。你可以使用所谓的粘性负载均衡。这是一种更为简单的实现，很多负载均衡器开箱即用地支持这一策略。理念很简单。当一个用户使用一个工作进程实例验证用户身份，我们可以在负载均衡器的水平保留一个那个关系的记录。 然后同样的用户发送一个新的请求，我们可以在记录中查找识别出哪一个工作进程有他们的身份验证信息，然后将请求发送给他们，而不是正常的分布式行为。这种方式，在服务器端的代码不需要改变，但是对于已经身份验证的用户，不能得到负载均衡的优势，因此如果你没有其他选择的话就使用粘性负载均衡。 集群模块实际上不支持粘性负载均衡，但是一些其他的负载均衡器可以配置默认支持粘性负载均衡。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/如何保存价值上千万的Node源代码.html":{"url":"Node/如何保存价值上千万的Node源代码.html","title":"如何保存价值上千万的Node源代码","keywords":"","body":"如何保存价值上千万的Node源代码 甲方突然要求做私有化部署 项目用node做的，node代码需要保护吗？ 一般情况下不需要，因为代码跑在云端服务器上，只要服务器安全，node代码哪怕是铭文，也是安全的 假如某天甲方突然要求做私有化部署，而你老板爽快地答应了，并把这个问题丢给你。 好吧，现在情况改变了，你需要代码部署在不信任的环境。一旦把Node.js源代码在甲方的服务器进行打包或上线，那甲方就可以轻易地查看，分析，篡改和复制你的代码。 于是开始谷歌 有哪些方法可以保护js代码？ 1. 加密 我们可以对js代码进行加密，每次执行前进行解密，实际运行的是解密后的代码，但是加密对执行效率会有影响，而且一旦密码被破解，源码就泄露了 2. 混淆 对代码进口行混淆，尽量地让代码变得不可读，据说这个思路是从代码写的很差的程序员身上学来的。常见的做法有：分离常量、打乱控制流、增加无意义代码、域名锁定、混淆字符串，禁用调试方法。 3. 编译 node代码是通过V8引擎来执行的，而V8引擎会将node代码编译成字节码（bytenode）之后再解释执行。如果我们能将node代码编译成字节码，将会进一步提高代码的安全性。毕竟反编译V8的字节码并非易事 4. 打包 把node代码和依赖，打包成一个可执行文件。这样可以增加一点破解的难度 然后从GitHub上抄代码 这里列出几个与js混淆，编译，打包相关的库或网站： 1. 混淆 一个强大的JS混淆器 Gihttps://github.com/javascript-obfuscator/javascript-obfuscator thub 一套JS代码安全问题解决方案 http://www.jshaman.com/ 2. 编译 一个极简的Node字节码编译器 https://github.com/OsamaAbbas/bytenode 3. 打包 -ncc可以把node项目打包成一个JS但文件，支持TypeScript，动态导入。 https://github.com/zeit/ncc [pkg]可以把node项目打包成一个二进制的可执行文件，pkg不支持动态导入，但是会将Node本身一起打包，可以实现再没安装node的环境运行 https://github.com/zeit/pkg [pmq20/node-packer]也能打包成一个二进制的可执行文件，它的优势在于支持各种形式的require, 也支持C++模块。可惜项目已经两年没更新了，只支持到Node.js 8.3.0, 对于更高版本的支持，请移步slee047/node-packer。 https://github.com/pmq20/node-packer 选出一个方法交差 没有绝对的安全可言，以上的方法只能是加大破解的难度。有道是世上无难事只怕有心人，只要有足够的技术和耐心，还是有可能还原出相应的代码。 作为防守方，我们能做是尽可能地多采用一些保护手段。在这里，提供一个JavaScript obfuscator + bytenode + node-packer的解决方案。 为什么选用node-packer而不是ncc，pkg呢？因为ncc和pkg无法处理混淆过的代码。 保护价值上千万的代码 假设我们需要保护的是这样的一个项目： // index.js console.log(\"以下是价值上千万的代码:\"); var a = 1; var b = 1; var c = a + b; console.log(\"1 + 1 = \" + c) 首先我们通过npm安装javascript-obfuscator 和 bytenode。然后在当前系统下安装node-packer。 由于node-packer无法打包由bytenode生成的.jsc字节码文件，所以需要添加以下文件来导入.jsc文件： // build.js require('bytenode'); require('./index-obfuscated.jsc'); 接着将以下指令加到package.json: \"scripts\": { \"obfuscate\": \"javascript-obfuscator index.js --string-array-encoding rc4\", \"bytenode\": \"bytenode --compile index-obfuscated.js\", \"nodec\": \"nodec build.js --skip-npm-install\", \"build\": \"npm run obfuscate && npm run bytenode && npm run nodec\", } 最后运行npm run build，可以得到一个a.out（windows系统下是a.exe）的可执行文件。运行一下，价值上千万的代码就跑起来了。 此外你会发现项目目录下多了两个文件：index-obfuscated.js, index-obfuscated.jsc。它们分别是混淆之后的代码以及字节码文件。javascript-obfuscator支持多种混淆技术，可能通过查看文档，按照自己的项目需求，调配选用合适的选项。 结语 除了以上技术手段，别忘了加上license，注明版权信息。 感谢看到最后，最后我将为你提供这份价值上千万的代码示例。项目地址是 https://github.com/iWinston/javascript-code-protection-example By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/6个Async、Await优于Promise的方面.html":{"url":"Node/6个Async、Await优于Promise的方面.html","title":"6个Async、Await优于Promise的方面","keywords":"","body":"6个Async、Await优于Promise的方面 async/await快速入门 async/await是一种编写异步代码的新方法，在着之前便携异步代码使用的是回调函数和promise async/await实际是建立在promise之上的。因此你不能把它和回调函数搭配使用 async/await和promise一样，是非阻塞的 async/await可以使异步代码在形式上更接近于同步代码 语法 假设有一个getJSON方法，它返回一个promise，该promise会被resolve为一个JSON对象。我们想要调用该方法，输出得到JSON对象，最后返回“done” 以下是使用promise的实现方式： const makeRequest = () => { getJSON() .then(data => { console.log(data) return 'done' }) } makeRequest() 使用async/await则是这样 const makeRequest = async() => { console.log(await getJSON()) return \"done\" } makeRequest() 使用async/await时有以下几个区别： 在定义函数时我们使用了async关键字。await关键字只能在使用async定义的函数的内部使用。所有async函数都会返回一个promise，该promise最终resolve的值就是你在函数中return的内容 由于第一点中的原因，你不能在顶级作用域中await一个函数。因为顶级作用域不是一个async方法。 await getJSON()意味着直到getJSON()返回的promise在resolve之后，console.log才会执行并输出resolove的值。 为何使用async/await编写出来的代码更好呢？ 1.简洁 看看我们节省了多少代码吧。即使是在这么一个简单的例子中，我们也节省了可观的代码。我们不需要为.then编写一个匿名函数来处理返回结果，也不需要创建一个data变量来保存我们实际用不到的值。我们还避免了代码嵌套。这些小优点会在真实项目中变得更加明显。 2. 错误处理 async/await终于使得用同一种构造(古老而好用的try/catch) 处理同步和异步错误成为可能。在下面这段使用promise的代码中，try/catch不能捕获JSON.parse抛出的异常，因为该操作是在promise中进行的。要处理JSON.parse抛出的异常，你需要在promise上调用.catch并重复一遍异常处理的逻辑。通常在生产环境中异常处理逻辑都远比console.log要复杂，因此这会导致大量的冗余代码 const makeRequest = () => { try { getJSON() .then(result => { // this parse may fail const data = JSON.parse(result) console.log(data) }) // uncomment this block to handle asynchronous errors // .catch((err) => { // console.log(err) // }) } catch (err) { console.log(err) } } 现在看看使用了async/await的情况，catch代码块现在可以捕获JSON.parse抛出的异常了： const makeRequest = async () => { try { // this parse may fail const data = JSON.parse(await getJSON()) console.log(data) } catch (err) { console.log(err) } } 3. 条件分支 假设有如下逻辑的代码。请求数据，然后根据返回数据中的某些内容决定是直接返回这些数据还是继续请求更多数据： const makeRequest = () => { return getJSON() .then(data => { if (data.needsAnotherRequest) { return makeAnotherRequest(data) .then(moreData => { console.log(moreData) return moreData }) } else { console.log(data) return data } }) } 只是阅读这些代码已经够让你头疼的了。一不小心你就会迷失在这些嵌套(6层)，空格，返回语句中。 在使用async/await改写后，这段代码的可读性大大提高了： const makeRequest = async () => { const data = await getJSON() if (data.needsAnotherRequest) { const moreData = await makeAnotherRequest(data); console.log(moreData) return moreData } else { console.log(data) return data } 4. 中间值 你可能会遇到这种情况，请求promise1，使用它的返回值请求promise2，最后使用这两个promise的值请求promise3。对应的代码看起来是这样的： const makeRequest = () => { return promise1() .then(value1 => { // do something return promise2(value1) .then(value2 => { // do something return promise3(value1, value2) }) }) } 如果promise3没有用到value1，那么我们就可以把这几个promise改成嵌套的模式。如果你不喜欢这种编码方式，你也可以把value1和value2封装在一个Promsie.all调用中以避免深层次的嵌套： const makeRequest = () => { return promise1() .then(value1 => { // do something return Promise.all([value1, promise2(value1)]) }) .then(([value1, value2]) => { // do something return promise3(value1, value2) }) } 这种方式为了保证可读性而牺牲了语义。除了避免嵌套的promise，没有其它理由要把value1和value2放到一个数组里。 同样的逻辑如果换用async/await编写就会非常简单，直观。 const makeRequest = async () => { const value1 = await promise1() const value2 = await promise2(value1) return promise3(value1, value2) } 5. 异常堆栈 假设有一段串行调用多个promise的代码，在promise串中的某一点抛出了异常： const makeRequest = () => { return callAPromise() .then(() => callAPromise()) .then(() => callAPromise()) .then(() => callAPromise()) .then(() => callAPromise()) .then(() => { throw new Error(\"oops\"); }) } makeRequest() .catch(err => { console.log(err); // output // Error: oops at callAPromise.then.then.then.then.then (index.js:8:13) }) 从promise串返回的异常堆栈中没有包含关于异常是从哪一个环节抛出的信息。更糟糕的是，它还会误导你，它包含的唯一的函数名是callAPromise，然而该函数与此异常并无关系。（这种情况下文件名和行号还是有参考价值的）。 然而，在使用了async/await的代码中，异常堆栈指向了正确的函数： const makeRequest = async () => { await callAPromise() await callAPromise() await callAPromise() await callAPromise() await callAPromise() throw new Error(\"oops\"); } makeRequest() .catch(err => { console.log(err); // output // Error: oops at makeRequest (index.js:7:9) }) 这带来的好处在本地开发环境中可能并不明显，但当你想要在生产环境的服务器上获取有意义的异常信息时，这会非常有用。在这种情况下，知道异常来自makeRequest而不是一连串的then调用会有意义的多。 6. 调试 最后压轴的一点，使用async/await最大的优势在于它很容易被调试。由于以下两个原因，调试promise一直以来都是很痛苦的。 你不能在一个返回表达式的箭头函数中设置断点（因为没有代码块） 如果你在一个.then代码块中使用调试器的步进(step-over)功能，调试器并不会进入后续的.then代码块，因为调试器只能跟踪同步代码的『每一步』。 通过使用async/await，你不必再使用箭头函数。你可以对await语句执行步进操作，就好像他们都是普通的同步调用一样。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/npm安装包时的几个命令区别.html":{"url":"Node/npm安装包时的几个命令区别.html","title":"npm安装包时的几个命令区别","keywords":"","body":"npm安装包时的几个命令区别 -D：是--save-dev的简写npm i XXX -D By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/Moment进入维护状态.html":{"url":"Node/Moment进入维护状态.html","title":"Moment进入维护状态","keywords":"","body":"Moment 进入维护状态 Moment.js 进入维护状态意味着： 不会再添加新功能； 不会将 API 更改为 immutable； 不会解决 Tree-shaking 或包体积的问题； 不会对其进行任何重大更改 可能选择不修复 bug，特别是长期存在的已知 bug Moment 官方表示，不鼓励开发者在以后的新项目中使用它，但他们考虑到现有项目仍在使用 Moment.js，当遇到以下情况时，Moment 团队会进行处理： 当出现严重的安全问题时，团队会给予解决； 在 IANA 时区数据库更新后，团队回味 Moment-Timezone 发布更新 仍需使用的 changing Moment 官方列举了需要继续使用 Moment.js 的场景： 浏览器的支持 Moment 能在 IE8 下完美运行。相比之下，Luxon只能在 IE10 及更高版本上运行，并且需要使用 polyfill 进行操作。如果存在一定要支持旧版本浏览器的情况，则可以使用 Moment.js。但是 Day.js 也可以在 IE8 及更高版本运行，开发者可以尝试使用 Day.js 其他库的依赖 有些库，尤其是日期选择器和图形库，很多都将 Moment.js 作为依赖项，如果正在使用这样的组件而找不到替代组件，开发者可以在整个项目中继续使用 Moment.js，而不用再引用另外一个库。 替代方案 如果开发者不在上述情况当中，Moment 团队也给出了一些替代方案 1、不使用库 一些简单的时间处理需求，可以使用 JavaScript 自带的 Date 和 Intl。Intl 对象可以展示不同时区不同语言的日期格式，同时 Intl 在多数浏览器上都已经有了很好的支持 2、Temporal Temporal 被看作是未来内置的时间日期方案。它是 javaScript 与语言内置的时间和日期 API。现阶段可通过试验性的 polyfil 来尝试。Moment.js 团队表示，希望未来能够完全不需要 JavaScript 的日期和时间库，而是使用语言本身的额功能。 3、其他替代库 除了 Temporal 之外，官方还推荐了 Luxon、Day.js、js-Joda 和 date-fns 等设计更优秀的时间日期库。自重 Day.js 凭借着拥有与 Moment.js 几乎相同的 API 而深受广大开发者的喜爱 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/package.json中的workspaces.html":{"url":"Node/package.json中的workspaces.html","title":"package.json中的workspaces","keywords":"","body":"package.json中的workspaces 官网 https://yarnpkg.com/blog/2017/08/02/introducing-workspaces/ package.json中的有个workspace设置，如下： \"workspace\": [ \"packages/*\" ] 不知道是什么意思，Google一下，原来是yarn特性。为了方便monorepo的管理，yarn提供了这个选项 为了方便代码的管理和维护，许多库都在使用monorepo的结构，比如 Babel, React, Jest, Vue, Angular。monorepo管理存在的一个共同难题是依赖的管理。如果为每个repo都独立安装依赖，当多个repo之间有共同的依赖时会造成重复安装，既占用空间又影响速度。yarn的workspaces功能就是为了解决这个问题。开启后，yarn会统一分析多个repo的依赖，如果有共同的依赖(同一个库，版本需求一致)，那么就会把依赖从该repo的node_modules提升到外部的node_modules中。这样避免重复安装。 实践: 在以workspaces的工作方式添加了多个repo后，在其中任意一个中执行yarn install xxx，则会在整个工程的根目录下创建该依赖。同时创建了指向各个repo的symlink。如此各个repo中就不会再报找不到依赖的错误了。 注意: 使用此feature需要工程为private，即在最外层的package.json中写入: \"private\": true 目前应该只有yarn支持此功能，npm还需要依赖第三发插件实现。 https://stackoverflow.com/questions/46947557/do-yarn-workspaces-work-with-npm-too 好文推荐 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/项目开发中的规范.html":{"url":"Node/项目开发中的规范.html","title":"项目开发中的规范","keywords":"","body":"形成良好统一的代码规范，有利于提高代码的可读性，减少潜在的错误，便于团队写作开发。本问简单介绍JS、CSS、Git Commit的规范工具和用法。 Lint\u001c JS 我们使用Eslint 1. 安装 你可以全局安装 npm install eslint -g 或者也可以在你的项目安装 npm install eslint --save-dev 安装完成后，可在命令行检查下你的代码是否否和规范。如果是全局安装则可以使用 eslint your-file 2. 配置 通过eslint --init命令可以生成一个配置文件。你也可以自己穿件.eslintrc.*文件，或者使用eslint命令检查你的代码时，它会从当前目录开始一层层向上查找是否存在.eslintrc文件，直到找到最近的一个.eslintrc文件，做诶此次检查的规则。 你可以在Eslint官网查看所有配置项 目前已经有很多大厂公开了他们的代码规范，也有很多相对应的Eslint插件，我们可以在.eslintrc中配置相对应的插件，这样就不用我们动手去添加一个个规则了。 以我们目前使用的Airbnb的代码规范为例，他提供了eslint-config-airbnb-base插件，因此我只需要在项目安装本插件： npx install-peerdeps --save-dev eslint-config-airbnb-base or npm install --save-dev eslint-config-airbnb-base 并且在.eslintrc中配置上这个插件，大功告成！ { \"extends\": [\"airbnb-base\"] } 需要⚠️的一点是，如果你是使用全局命令eslint你的代码，在相应的.eslintrc中的extends，plugins都需要在全局安装。否则eslint会找不到相对应的插件。 最后，如果你还想对现有的airbnb或者现有其他规则进行配置，则可以在.eslintrc中的rules加上相应的配置。 { \"extends\": [\"airbnb-base\"], \"rules\": { // 你的个性化配置 \"rule-name\": \"\" // 0-off, 1-warn, 2-error } } 还有一个比较例外的是可以使用以下方式，针对某些文件，重新修改相应规则： \"overrides\": [ { \"files\": [\"*-test.js\",\"*.spec.js\"], \"rules\": { \"no-unused-expressions\": \"off\" } } ] 3. 禁用 但是有些时候有些地方你可能真的需要禁用某些规则，eslint提供了几种禁用方式： /* eslint-disable [rules] */：这行之后的所有代码禁用eslint规则。 /* eslint-disable-line [rules] */：这一行禁用eslint规则 /* eslint-disable-next-line [rules] */： 下一行禁用eslint规则。 其中[rules]是可选的，如果没有[rules]则禁用所有规则，如果有[rules]则禁用指定规则 如 /* eslint-disable */则会禁用掉所有规则，/* eslint-disable no-console*/ 则只会禁用掉no-console这条规则。 Lint CSS 我选择了StyleLint来规范我的CSS。它可以说和eslint非常像了。 1. 安装 同样的，全局或者项目下安装stylelint。 npm install stylelint -g 安装完成后，如果是全局安装则可以使用 stylelint your-css-file 检查你的文件。如果是项目内安装则使用：./node_modules/.bin/stylelint your-css-file。 2. 配置 可以通过三种方式对stylelint进行配置： package.json中的stylelint属性 .stylelintrc 文件 stylelint.config.js文件导出一个 JS 对象 和 ESLint一样，我们可以在extends中指定第三方插件，rules来配置对应的规则。这里我们还是继续使用Airbnb CSS 规范。 安装stylelint-config-airbnb： npm install stylelint-config-airbnb 在配置文件中声明： { \"extends\": \"stylelint-config-airbnb\"} 注意，如果你的.stylelintrc文件是在根目录下，则extends的路径需要写成绝对路径，比如： { \"extends\": \"/usr/local/lib/node_modules/stylelint-config-airbnb\" } 最后，运行stylelint your-css-file就可以出现规范检查结果啦！stylelint默认会支持css,scss,less所以你也不用担心哦~ 禁用 stylelint 的规则也和 ESLint一样。所以如果熟悉了ESLint， stylelint真的可是说是无缝上手哦~ Lint Commit 在这一步我会进行两步操作： 检查前2步的ESLint，stylelint 是否全部通过； 提交的commit信息是否符合规范。 1. 使用githooks检查代码规范是否通过 我们使用husky来管理我们的githooks。在安装husky之前，请确保你的项目已经git init了。 安装 husky:npm install husky --save-dev 在package.json中定义我们需要的钩子及执行的命令： { \"scripts\": { \"lint:es\": \"eslint\", // lint js \"lint:css\": \"stylelint src/**/*.css\", // lint css \"lint:all\": \"npm-run-all lint:es lint:css\" // lint es, css }, \"husky\": { \"hooks\": { \"pre-commit\": \"npm run lint:all\", } } } 在这里我们分别定义了lint:es和lint:css两个命令来检测代码规范。你可以分别运行这两个命令。也可以定义一个命令同时运行这两个命令，我在这里使用了npm-run-all： npm install npm-run-all --save-dev 我们定义了在pre-commit钩子触发时会执行npm run lint:all命令。pre-commit钩子会在git commit时触发，如果lint:all没有通过，则本次commit会失败。 2. 使用commintlint检查 commit 信息是否符合规范 官网 在这里，我们使用阮老师这篇文章中提到的 git 提交规范， 大致是： (): // 空一行 // 空一行 其中，type 可选项为： feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 安装commitlint， 以及相对应的commit规范。和eslint一样，commitlint为我们提供检测功能，同时他也有不同的插件来对应不同的规范风格。你可以在这里查看大家分享出来的相应规范的配置。 npm install --save-dev @commitlint/{config-conventional,cli} 生成配置文件： echo \"module.exports = {extends: ['@commitlint/config-conventional']}\" > commitlint.config.js 它也支持多种文件格式的配置文件: Configuration is picked up from commitlint.config.js, .commitlintrc.js, .commitlintrc.json, or .commitlintrc.yml file or a commitlint field in package.json 并且常用的配置项也与ESlint很相似： { \"extends\": ['@commitlint/config-conventional'], // 扩展的规则集 \"rules\": { // commitmsg 的自定义规则 } } 这时候你就可以检查你要提交的commit信息是否符合规范了： echo \"foo\" | npx commitlint 不过这样很鸡肋，我要先commit一次要提交的信息，通过了，再用这条消息提交一次。我们完全可以在githooks时来解决这个问题： { \"scripts\": { \"commitmsg\": \"commitlint -e GIT_PARAMS\" }, \"husky\": { \"hooks\": { \"pre-commit\": \"npm run lint:all\", \"commit-msg\": \"npm run commitmsg\" } } } 在这里和githooks同时使用时需要加上GIT_PARAMS这个环境变量。我们在commit-msg这个钩子时调用npm run commitmsg 来判断commit信息是否符合规范。 3. 使用commitizen来填写commit msg 安装commitizen及其adapater : npm install -g commitizen cz-conventional-changelog 全局安装adapater: echo '{ \"path\": \"cz-conventional-changelog\" }' > ~/.czr 安装完成后，使用 git cz 代替 git commit -m 来填写 commit msg，会出现一个交互式工具： OK。完成以上三步之后我们的git 工作流变成了： git add . git cz 然后就会检查我们的eslint, stylelint, commitlint。这样，当你提交成功时，你的JS, CSS , Commit Msg 也是完全符合规范的哦~ By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/npm库：AJV，JSON模式验证.html":{"url":"Node/npm库：AJV，JSON模式验证.html","title":"npm库：AJV，JSON模式验证","keywords":"","body":"NPM酷库042：ajv，JSON 模式验证 schema npm node.js javascript 4.4k 次阅读 · 读完需要 5 分钟 NPM酷库，每天两分钟，了解一个流行NPM库。· 在NPM酷库041中，我们学习了如何用validator库来验证字符串是否是Email、URL等，在开发中，除了字符串，我们还需要对更复杂的数据进行验证。比如需要Object满足那些属性，每个属性都是什么类型的，这些条件称之为数据模式验证。 在对数据的模式验证领域有专门的标准，叫做 JSON Schema。就是按照JSON Schema标准声明一个模式对象，然后使用模式验证工具去验证目标数据。 ajv ajv 是一个非常流行的JSON Schema验证工具，并且拥有非常出众的性能表现。下方的例子中，我们使用ajv来验证用户输入的表单数据是否合法。 const Ajv = require('ajv'); let schema = { type: 'object', required: ['username', 'email', 'password'], properties: { username: { type: 'string', minLength: 4 }, email: { type: 'string', format: 'email' }, password: { type: 'string', minLength: 6 }, age: { type: 'integer', minimum: 0 }, sex: { enum: ['boy', 'girl', 'secret'], default: 'secret' }, } }; let ajv = new Ajv(); let validate = ajv.compile(schema); let valid = validate(data); if (!valid) console.log(validate.errors); 在上述代码中，我们声明了一个数据模式schema ，这个模式要求目标数据为一个对象，对象可以有五个字段 username、email、password、age、sex，并分别定义了五个字段的类型和数据格式要求，并且其中 username、email、password 必填。然后我们使用这个模式去验证用户输入的数据 data 是否满足我们的需求。 注意： JSON Schema 是一个声明模式描述对象的标准，并非一个库 ajv 是一个JSON Schema标准验证器的实现，除了ajv还有很多其他的库 代码中的 schema 是使用 JSON Schema生成的模式描述对象 代码中 data 是我们要进行检查的数据 参考资料： JSON Schema http://json-schema.org AJV https://github.com/epoberezki... By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/pm2深入学习.html":{"url":"Node/pm2深入学习.html","title":"pm2深入学习","keywords":"","body":"pm2深入学习 背景 对于线上项目，如果直接通过node app来启动，如果报错了可能直接停止导致整个服务崩溃，一般监控node有几个方案。 supervisor：一般用作开发环境的使用 forever：管理多个站点，一般每个站点的访问量不大的情况，不需要监控 PM2: 网站的访问量比较大，需要完整的监控页面 PM2的主要特性 内建负载均衡（使用node cluster集群模块） 监听文件变化，自动重启程序 支持性能监控 程序崩溃自动重启 服务器重新启动时自动重新启动 自动话部署项目 常用命令 pm2 start app.js --name application pm2 start app.js --name start --watch // 添加进程监视 pm2 show (appname|id) pm2 list pm2 monit pm2 logs pm2 web // 监控运行这些进程的机器状态 pm2 stop (id|all) pm2 restart (id| all) pm2 delete (id|all) // 杀死指定/所有进程 pm2 start centos // 设置pm2开机自启（可选项：ubuntu, centos, rehat, gentoo, systemd, darwin, amazon） pm2 save // 最后保存设置 配置PM2启动文件 pm2启动的方式可以进行很多的扩展，比如设置环境，设置错误信息打印，设置输入信息打印等等高级功能。那么命令就不能完成这些任务，所有pm2提供了配置文件的方式来启动～ pm2.config.js // 名称任意，按照个人习惯来 module.exports = { apps: [ { name: 'kaifazhe', // 应用名称 script: './build/server.js', // 启动文件地址 cwd: './', // 当前工作路径 watch: [ // 监控变化的目录，一旦变化，自动重启 'src', 'build', ], ignore_watch: [ // 忽视这些目录的变化 'node_modules', 'logs', 'public', ], node_args: '--harmony', // node的启动模式 env: { NODE_ENV: 'development', // 设置运行环境，此时process.env.NODE_ENV的值就是development ORIGIN_ADDR: 'http://www.yoduao.com' }, env_production: { NODE_ENV: 'production', }, out_file: './logs/out.log', // 普通日志路径 error_file: './logs/err.log', // 错误日志路径 merge_logs: true, log_date_format: 'YYYY-MM-DD HH:mm Z', }, ], }; 对于上面的 env，我们可以在内部添加很多个参数变量，这样我们所使用的 process.env.XXX 就会对应发生变化,例如上面，我们 process.env.ORIGIN_ADDR 的值就是http://www.youdao.com～ 日志 pm2日志 上面配置文件可以看出来，我们可以配置logs，包括普通的out和错误的err日志。其实业不需要我们做什么，我们只需要在config里面配置好就行，他就会自动往里面写日志： 日志分割 我们正常意义上的日志，以node为例，应该都是使用的log4js来进行按日期写入的，那么pm2可不可以按日期写入呢？答案时肯定的 pm2为我们提供了插件系统，而日期分割功能就正好用到了插件系统，！ pm2 install pm2-logrotate // 看好了，是pm2 install而不是npm install 复 你装完之后它就自动启动，然后你还可以配置各种参数 然后就完成了日志按日期分割～ 细心的小伙伴可能发现了，你上面让我安装的是pm2-logrotate，为啥你安装的是pm2-logrotate-ext，嗯，因为据说官方的pm2-logrotate存在一个bug，就是日期会正常分割，但是如果你前一天的文件没有写满比如你设置了1M但只写了500K那么第二天的日志还是会插入到原来的out.log(err.log)，所以大牛就写了这个解决了这个问题 负载均衡 pm2 start server.js -i (number|max) # 开启三个进程运行项目 pm2 start app.js -i 3 # 根据机器CPU核数，开启对应数目的进程运行项目 pm2 start app.js -i max 配置文件里对应的：\"instance\": (number|max) // pm2.config.js \"instances\": 2, // 启动两个实例 通过pm2配置文件来自动部署项目 在项目根目录下新建一个deploy.yml文件 # deploy.yaml apps: - script: ./start.js # 入口文件 name: 'app' # 程序名称 env: # 环境变量 COMMON_VARIABLE: true env_production: NODE_ENV: production deploy: # 部署脚本 production: # 生产环境 user: lentoo # 服务器的用户名 host: 192.168.2.166 # 服务器的ip地址 port: 22 # ssh端口 ref: origin/master # 要拉取的git分支 ssh_options: StrictHostKeyChecking=no # SSH 公钥检查 repo: https://github.com/**.git # 远程仓库地址 path: /home # 拉取到服务器某个目录下 pre-deploy: git fetch --all # 部署前执行 post-deploy: npm install && pm2 reload deploy.yaml --env production # 部署后执行 env: NODE_ENV: production 使用pm2部署项目 首次部署 pm2 deploy deploy.yaml production setup 再次部署 pm2 deploy deploy.yaml production update 该部署流程同样适用前端项目 如vue-cli的项目，自动部署到服务器，自动执行npm run build 命令，生成的dist目录，指定到nginx的静态文件目录下。 配合pm2-web实现监控可视化 可能很多人不喜欢控制台，喜欢把监控进行可视化更方便查看和分析。pm2-web npm install -g pm2-web 默认pm2-web会自动启动一个端口8080，但是我们还是喜欢可控状态的， 因此按照配置文件的方式启动。 $ pm2-web --config pm2-web-config.json // pm2-web-config.json { \"www\": { \"host\": \"localhost\", \"address\": \"0.0.0.0\", \"port\": 6688 } } 这样你就可以在浏览器查看可视化的监控状态了～ pm2-web依赖node-inspector,而node-inspector对于高版本node无法安装，很多人提了issue，但是感觉开发者也处于放弃了的状态。我也不打算在本地安装低版本node，所以感兴趣的大家可以安装个低版本的试试～ By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/pm2的cluster模式与fork模式的区别.html":{"url":"Node/pm2的cluster模式与fork模式的区别.html","title":"pm2的cluster模式与fork模式的区别","keywords":"","body":"pm2的cluster模式与fork模式的区别 fork模式 fork模式使用最基本的进程运行方式，知识但实例运行server，无法实现TCP连接共享；好处时可以修改exec_interpreter，使用pm2运行js之外的语言，例如PHP或python pm2 --interpreter [bash|python|...] cluster模式 利用node Cluster模块实现，只能用于启动node进程，无法应用于其他语言，可以启动多个server实例，并在各个实例之间实现负载均衡而且共享TCP连接，可以提升服务器的响应性能。 cluster用于运行一些可以共享TCP联机的worker进程，首先创建一个master进程，虽有根据设置的次数fork处响应用的workers，master和workers通过进程间通行（IPC）实现TCP句柄等数据交换，cluster模块自带负载均衡，默认使用Round-Robin算法实现，master监听TCP端口，并根据RR算法将请求交给钦定的worker进行处理。 cluster代码实现： const cluster = require('cluster'); const http = require('http'); const numCPUs = require('os').cpus().length; if (cluster.isMaster) { console.log(`Master ${process.pid} is running`); // Fork workers. for (let i = 0; i { console.log(`worker ${worker.process.pid} died`); }); } else { // Workers can share any TCP connection // In this case it is an HTTP server http.createServer((req, res) => { res.writeHead(200); res.end('hello world\\n'); }).listen(8000); console.log(`Worker ${process.pid} started`); } pm2内置了一cluster模式启动server的方式，不需要对代码进行改动，只需要生成自己的http server： var http = require('http'); http.createServer(function(req, res) { res.writeHead(200); res.end(\"hello world\"); }).listen(8080); 随后： pm2 start app.js -i 4 4表示启动4个server实例，输入0可以以机器CPU核心数来启动server实例。 及时调整集群 pm2通过scale命令及时调整集群数，如果线上运行时发现worker数不足以支撑请求，可以使用 pm2 scale +3 添加三个server实例 无缝重启 pm2 reload 可以重启app对应的所有worker，重启对于每一个worker，会在一个新的worker生成之后再kill掉之前的worker，这样即便是对服务器进行更新时，也可以正常处理用户的请求，做到无缝升级重启。同时可以参考pm2的gracefulReload命令，实现在实例推出之前关闭数据库连接需要处理的操作： process.on('message', function (msg) { if (msg === 'shutdown') { close_all_connections(); delete_cache(); server.close(); process.exit(0); } }); By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/Node原生模块整理.html":{"url":"Node/Node原生模块整理.html","title":"Node原生模块整理","keywords":"","body":"Koa深入学习笔记一 Node原生模块整理 1）assert： 断言模块为不变量的验证提供了一组断言函数。 该模块提供了推荐的严格模式和较为宽松的遗留模式。 2）async_hooks： 异步钩子模块提供了一个 API 来注册回调，它跟踪在 Node.js 应用程序中创建的异步资源的生存期。 3） Buffer：（Buffer 类位于全局范围内，因此不太可能需要使用 require (‘ Buffer')。 缓冲区 ） 在 TypedArray 引入之前，JavaScript 语言没有读取或操作二进制数据流的机制。 Buffer 类作为 Node.js API 的一部分被引入，用于在 TCP 流、文件系统操作和其他上下文中与 octet 流进行交互。 使用 TypedArray，Buffer 类实现了 Uint8Array API，这种方式更加优化，更适合 Node.js。 4）child_process: 子进程模块提供以与 popen (3)类似但不完全相同的方式生成子进程的能力。 此功能主要由子进程提供。 函数: 5) Cluster: Js 的一个实例在一个线程中运行。 为了利用多核系统的优势，用户有时会希望启动一个 Node.js 进程集群来处理负载。 集群模块允许简单地创建所有共享服务器端口的子进程。 6) console: 控制台模块提供了一个简单的调试控制台，类似于 web 浏览器提供的 JavaScript 控制台机制。 模块导出两个特定的组件: 一个 Console 类，其方法包括 Console.log ()、 Console.error ()和 Console.warn () ，可用于写入任何 Node.js 流。 配置为写入 process.stdout 和 process.stderr 的全局控制台实例。 可以使用全局控制台而无需调用 require (“ console”)。 7) crypto: 加密模块提供了加密功能，包括一组用于 OpenSSL 的散列、 HMAC、密码、解密、签名和验证函数的包装器。 8) debugger: Js 包含一个可通过 V8 Inspector 和内置调试客户端访问的进程外调试工具。 要使用它，启动 Node.js，带有 inspect 参数，然后是调试脚本的路径; 将显示一个提示，表明调试器的成功启动: 9) dns: Dns 模块包含属于两个不同类别的函数: 使用基础操作系统设施执行名称解析的函数，以及不一定执行任何网络通信的函数。 此类别只包含一个函数: dns.lookup ()。 希望以与同一操作系统上的其他应用程序相同的方式执行名称解析的开发人员应该使用 dns.lookup ()。 连接到实际 DNS 服务器以执行名称解析的函数，并且始终使用网络执行 DNS 查询。 此类别包含 dns 模块中除 dns.lookup ()以外的所有函数。 这些函数不使用 dns.lookup ()(例如 / etc / hosts)使用的同一组配置文件。 如果开发人员不想使用底层操作系统的设施进行名称解析，而是希望始终执行 DNS 查询，那么他们应该使用这些函数。 下面是一个解析‘ archive. org'的示例，然后反向解析返回的 IP 地址。 10)events: Js 的大部分核心 API 都是围绕惯用的异步事件驱动架构构建的，在这种异步通道中，某些类型的对象(称为“发射器”)发出命名事件，从而导致调用 Function 对象(“ listeners”)。 例如: 一张网。 服务器对象在每次对等点连接到它时发出一个事件; 一个 fs。 Readstream 在打开文件时发出事件; 流在可以读取数据时发出事件。 发出事件的所有对象都是 EventEmitter 类的实例。 这些对象公开一个 eventEmitter.on ()函数，该函数允许将一个或多个函数附加到对象发出的命名事件。 通常，事件名称采用大小写混合格式，但可以使用任何有效的 JavaScript 属性键。 当 EventEmitter 对象发出事件时，将同步调用附加到该特定事件的所有函数。 被调用的侦听器返回的任何值都将被忽略，并将被丢弃 下面的示例显示一个带有单个侦听器的简单 EventEmitter 实例。 方法用于注册侦听器，而 eventEmitter.emit ()方法用于触发事件。 11)fs: Fs 模块提供了一个 API，用于以类似于标准 POSIX 函数的方式与文件系统进行交互。 所有文件系统操作都有同步和异步的形式。 异步窗体总是将完成回调作为它的最后一个参数。 传递给完成回调的参数依赖于该方法，但第一个参数始终保留用于异常。 如果操作成功完成，则第一个参数将为 null 或未定义。 12)http:/http2: Coreapi 提供了一个专门围绕 http / 2协议特性设计的低级接口。 它不是专门为与现有的 http / 1模块 API 兼容而设计的。 但是，兼容性 API 是。 The http2 Core API is much more symmetric between client and server than the http API. For instance, most events, like 'error', 'connect' and 'stream', can be emitted either by client-side code or server-side code. 13)https: Https 是基于 tls / ssl 的 HTTP 协议。 在 Node.js 中，这是作为一个单独的模块实现的。 14) inspector: 检查器模块提供了与 V8检查器交互的 API。 15)net: Net 模块提供了一个异步网络 API，用于创建基于流的 TCP 或 IPC 服务器(net.createServer ())和客户机(net.createConnection ()) 16)os: Os 模块提供了操作系统相关的实用程序方法和属性。 17)path: 路径模块提供了处理文件和目录路径的实用程序。 18)process: 流程对象是一个全局对象，它提供关于当前 Node.js 流程的信息并对其进行控制。 作为一个全局，Node.js 应用程序始终可以使用 require ()。 也可以使用 require ()显式地访问它: 19)querystring: Querystring 模块提供了解析和格式化 URL 查询字符串的实用工具。 20)readline: Readline 模块提供了一个接口，用于一次从可读流(如 process.stdin)中读取一行数据 21)repl: 模块提供了一个 Read-Eval-Print-Loop (REPL)实现，可以作为独立程序使用，也可以在其他应用程序中包含。 22)stream: 流是一个用于在 Node.js 中处理流数据的抽象接口。 流模块提供了实现流接口的 API。 有很多由 Node.js 提供的流对象。 例如，对 HTTP 服务器的请求和 process.stdout 都是流实例。 流可以是可读的、可写的，或者两者兼而有之。 所有流都是 EventEmitter 的实例。 23)string_decoder: 字符串解码器模块提供了一个 API，用于以保留编码的多字节 UTF-8和 UTF-16字符的方式将 Buffer 对象解码为字符串, const { StringDecoder } = require('string_decoder’); 24)timer: 定时器模块公开一个全局 API，用于调度将在未来某个时间段调用的函数。 因为计时器函数是全局的，所以不需要调用 require (‘ timers')来使用 API。 25)TLS(SSL): Tls 模块提供了构建在 OpenSSL 之上的传输层安全性(TLS)和安全套接字层(SSL)协议的实现。 该模块可以通过以下方式访问: const tls = require('tls’); 26)tty: Tty 模块提供 tty. ReadStream 和 tty. WriteStream 类。 在大多数情况下，不需要或不可能直接使用这个模块 27)dgram: 模块提供了 UDP 数据报套接字的实现。 28)url: 模块提供 URL 解析和解析的工具 29)util: 模块主要是为了支持 Node.js 自己的内部 api 而设计的。 但是，许多实用程序对应用程序和模块开发人员也很有用。 30)v8: 模块公开了特定于 Node.js 二进制文件中内置的 V8版本的 api。 31)vm: 模块提供了用于在 V8虚拟机上下文中编译和运行代码的 api。 Vm 模块不是安全机制。 不要使用它来运行不受信任的代码。 “沙箱”这个术语在这些文档中的使用仅仅是指一个单独的上下文，并不提供任何安全保证。 32)work_threads: 允许使用并行执行 JavaScript 的线程。 33)zlib: 模块提供了使用 Gzip 和 deflate / flate 以及 Brotli 实现的压缩功能。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/koa-bodyparser中间件.html":{"url":"Node/koa-bodyparser中间件.html","title":"koa-bodyparser中间件","keywords":"","body":"koa-bodyparser中间件 使用koa-body代替koa-bodyparser和koa-multer 之前使用koa2的时候，处理post请求使用的是koa-bodyparser，同时如果是图片上传使用的是koa-multer 这两者的组合没什么问题，不过koa-multer和koa-route（注意不是koa-router）存在不兼容的问题。 koa-bodyparser koa处理post请求的时候，需要对body传递过来的数据进行处理。 实际上如果要手动实现一个能够处理 application/x-www-from-urlencoded 的中间件，还是借助的原生 node.js 的方法进行处理。 koa封装了原生了node.js的request对象的ctx.req中。 而借助原生node.js的request对象，监听data事件及end事件，进行处理。 实现中间件 实现listen监听body数据传输 当body数据传输结束后，将拼接的字符串进行解析，解析之后，将Promise.resolve即可 function listen(ctx){ let str = ''; return new Promise((resolve,reject) => { ctx.req.addListener('data',(data)=>{ str += data; }); ctx.req.addListener('end',()=>{ const res = jsonBodyparser(str); resolve(res); }); }); } 将字符串解析成对象 处理拼接的字符串很简单，我用了最简单的方式，就是拆分遍历字符串 function jsonBodyparser(str){ let parseBody = {}; let strArr = str.split('&'); for(let [index,item] of strArr.entries()){ const itemArr = item.split(\"=\"); parseBody[itemArr[0]] = itemArr[1]; } return parseBody; } exports中间件 参照了koa-bodypaser中间件实现方式，将结果封装进了ctx.request.body中 module.exports = () => { return async (ctx, next) => { bodyPaser = await listen(ctx);; ctx.request.body = bodyPaser; await next(); } } 使用中间件 const Router = require('koa-router'); const fs = require('fs'); const path = require('path'); const postBodyParser= require('../middlewares/postBodyParser'); const router = new Router(); router.use(postBodyParser()); router.get('/',async (ctx)=>{ const htm = fs.readFileSync(path.resolve('./views/login.html')).toString(); ctx.body = htm; }).post('/',async (ctx)=>{ ctx.body = ctx.request.body; }); module.exports = router; 使用koa-bodypaser 如果使用koa-bodypaser，则非常方便而且考虑的方面非常多 const Koa = require('koa'); const Router = require('koa-router'); const bodyParser = require('koa-bodyparser'); const router = require('./routes/index'); const app = new Koa(); app.use(bodyParser()); app.use(router.routes()).use(router.allowedMethods()); app.listen(5000,()=>{ console.log('start : 5000'); }); koa-body koa-body主要是下面两个依赖： \"co-body\": \"^5.1.1\", \"formidable\": \"^1.1.1\" 在koa2中使用koa-body，我使用的全局引入，而不是路由级别的引入，因为考虑到很多地方都有post请求或者，没必要只在路由级别引入。 安装依赖 yarn add koa-body npm i koa-body -S app.js 省略了 koa的一些基本的代码 const koaBody = require('koa-body') const app = new Koa(); app.use(koaBody({ multipart: true, //支持文件上传 encoding: 'gzip', formidable: { uploadDir: path.join(__dirname, 'public/upload/'), // 设置文件上传目录 keepExtension: true, //保存文件的后缀 maxFieldsSize: 2 * 1024 * 1024, // 文件上传大小限制 onFileBegin: (name, file) => { // 文件上传前的设置 // console.log(`name: ${name}`); // console.log(file); } } })); 有用的参数 1）koa-body的基本参数 参数名 描述 类型 默认值 patchNode 将请求体打到原生node的ctx.req中 Boolean false patchKoa 将请求体打到原生koa的ctx.req中 Boolean false jsonLimit JSON数据体的大小限制 String/Integer 1mb formLimit 限制表单请求大小限制 String/Integer 56kb textLimit 限制 text body 的大小 String/Integer 56kb encoding 表单的默认编码 String utf-8 multipart 是否支持multipart-formdate的表单 Boolean false urlencoded 是否支持 urlencoded Boolean true text 是否解析text/plain的表单 Boolean true json 是否解析json Boolean true jsonStrict 是否使用json严格模式，true会只处理数组和对象 Boolean true formidable 配置更多的关于multipart的选项 Object {} onError 错误处理 Function function() {} stict 严格模式 启用后不会解析GET、HEAD、DELETE请求 Boolean true 2） formidable的相关配置参数 参数名 描述 类型 默认值 maxFields 限制字段的数量 Integer 1000 maxFieldsSize 限制字段的最大大小 Integer 2 1024 1024 uploadDir 文件上传的文件夹 String os.tmpDir() keepExtensions 保留原来的文件后缀 Boolean false hash 如果要计算文件的 hash，则可以选择 md5/sha1 String false multipart 是否支持多文件上传 Boolean true onFileBegin 文件上传前的一些设置操作 Function function(name,file){} 获取文件上传后的信息 这些代码是在路由中体现的 需要注意的是，如果是获取上传后文件的信息，则需要在ctx.request.files 中获取。 如果是获取其他的表单字段，则需要在 ctx.request.body 中获取，这是由 co-body 决定的（默认情况）。 router.get('/', async (ctx) => { await ctx.render('index'); }); router.post('/',async (ctx)=>{ console.log(ctx.request.files); console.log(ctx.request.body); ctx.body = JSON.stringify(ctx.request.files); }); 因为默认开启多个文件上传，因此ctx.request.files是一个对象，而且是通过表单的name=photo属性作为对象的key,值便是一个File对象，有用的字段如下： 参数名 描述 size 文件大小 path 文件上传后的目录 name 文件的原始名称 type 文件类型 lastModifiedDate 上次更新的时间 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/koa-multer实现文件上传并自定义文件名和目录.html":{"url":"Node/koa-multer实现文件上传并自定义文件名和目录.html","title":"koa-multer实现文件上传并自定义文件名和目录","keywords":"","body":"koa-multer实现文件上传并自定义文件名和目录 之前使用express的时候，使用multer进行文件上传，而koa-multer是koa-modules提供的文件上传中间件 使用multer最显著的特点就是要通过multer生成upload中间件。 upload.js 借助multer.diskStorage({})方法，实现自定义上传目录和文件名 文件路径./utils.upload.js const multer = require('koa-multer') const path = require('path'); const storage = multer.diskStorage({ desctination: 'public/uploads/' + new Date().getFullYear() + (new Date().getMonth()+1) + new Date().getDate(), filename(ctx, file, cb) { const filenameArr = file.origininalname.split('.'); cb(null, Date.now() + '.' + filenameArr[filenameArr.length-1]) } }) const upload = multer({storage}); module.exports = upload; 在路由中使用upload 导出upload之后，就可以在路由中使用upload: router.gete('upload', async(ctx) => { await ctx.render('upload') }) router.post('upload', upload.single('file'), async(ctx) => { console.log(ctx.req.file); ctx.body = ctx.request.body; }) 同样的，upload之后的文件信息，仍就存储在ctx.req.file中，用来提供相关的信息。 filename 是存储的文件名称，而可以通过 destination 切割出文件的上传文件夹 201864，path 没什么大的用处，因为 linux 上和 windows 是不同的，直接使用 path 不利于跨平台移植项目。 filename 是存储的文件名称，而可以通过 destination 切割出文件的上传文件夹 201864，path 没什么大的用处，因为 linux 上和 windows 是不同的，直接使用 path 不利于跨平台移植项目。 问题 koa-multer和koa-route并不能完美的结合，因此不建议使用koa-route，而是建议使用koa-router（两者是不同的） By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg体系.html":{"url":"Node/egg体系.html","title":"egg体系","keywords":"","body":"egg体系 Egg.js官方的定位是为企业级框架和应用而生和孕育出更多上层框架，帮助开发团队人员降低开发和维护成本。 从Egg.js目前的定位和现有的生态周边，我们可以分成以下三个体系： 核心体系 辅助体系 生态体系 核心体系 顾名思义就是egg的核心能例，其实Egg的核心就只有一盒模块egg-core模块，虽然egg-core模块代码有十几个文件，但是核心思想可以抽象出一下两点： 以koa.js为基类，利用了其中间件机制和HTTP服务机制作为框架基础 以Loader机制作为Egg各分层机制的约定基础 辅助体系 就是再开发和生产过程中，提供相关的支持能力，例如脚手架初始化、开发热更新、开发热部署、多线程使用和多线程守护等能力支持。在目前官方提供的辅助体系中，这里主要讲解的是以下几个能力 开发模式支持 生产模式支持 多线程利用 主要涉及的模块有egg-script、egg-bin等 生态体系 一个好框架必备的条件有两个，其一是友好的开发体验，其二是生机勃勃的生态体系。目前来讲Egg.js做的更出色的是友好的开发体验，也就是说该框架的约定是很友好的 目前Egg生态分约定以下三种类型 中间件 插件 框架 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg应用自定义4xx和5xx的方案.html":{"url":"Node/egg应用自定义4xx和5xx的方案.html","title":"egg应用自定义4xx和5xx的方案","keywords":"","body":"转载 现在的错误处理插件是egg-onerror，但这个插件主要是优雅处理为捕获异常，也就是为了让应用不挂进行兜底，但是现在没有一种同意的业务员错误处理方案。 问题 业务校验 比如参数校验、业务演这个等等，这些并不术语异常，一般会在响应时专程对应的数据格式。常见的处理方式是接口返回错误，并在response转换 class User extends Controller { async show() { const error = this.check(this.params.id); if (error) { this.ctx.status = 422; this.ctx.body { message: error.message, }; return; } // 继续处理 } check(id) { if (!id) return { message: 'id is required' }; } } 但是业务场景是非常复杂的，可能在controller里面调用多层service，这样就必须把错误结果一层层传递。所以这种场景业务校验推荐使用异常的方式，类似上面的场景只需要抛出一个异常 class User extends Controller { async show() { this.check(this.params.id); // 继续处理 } check(id) { if (!id) throw new Error('id is required'); } } 然后再中间件处理这个异常 异常类型区分 上面的示例也同样抛出Error，如果不写中间件处理同样回走到oneror插件，根据惠泽会打印错误日志并返回500. 这不是我们期望的，开发者希望但会正确的格式，比如status是422，body是一个含错误信息的json。所以我们需要明确已知异常和为捕获异常，并对他们做差异处理 标准化响应 如果在写一个api server的时候，希望响应格式是规范的，而开发者一般都比较关注正常结果，异常时会返回格式，所以对于一个API Server来说这也是非常重要的。 内容协商 有些应用会根据content-type来返回对应数据，这种情况错误处理也需要根据这种场景来返回相应的结果 Spec 错误定义 种类 错误分为三种未捕获异常、系统异常、业务异常，以下是分类比较 定义 未捕获异常 系统异常 业务错误 类名 Error xxxException xxxBizError 说明 js内置错误，未做任何处理 自己抛出的系统异常 自己跑出的业务异常 错误处理方式 由onerror插件处理 业务可扩展处理 业务处理 可识别 否 是 是 属性扩展 否 是 是 类名只是用来区分三种错误，继承可以自定义 所有的类均继承自Error类，并定义BaseError类，集成子BaseError的错误是可以被识别的，而其他三方继承Error的类都发被识别。 class BaseError extends Error {} class HttpClientError extends BaseError {} class HttpServerError extends BaseError {} BaseError.check(BaseError); // true BaseError.check(Error); // false 如果业务跑出自定义的系统异常和业务错误，可直接在错误里面处理，为捕获异常在onerror中处理 即成的错误可增加额外的属性，比如HttpError可增加额外属性，比如HttpError可增加status属性作为处理函数的输入 字段 标准字段包括 name：一般为类名，如NotFoundError message: 错误的具体信息，可读的，如404 Not Found code：答谢的字符串，描述错误，如NOT_FOUND http扩展 status: http状态码，400 错误抛出 自行在代码里面引入对应的类 import { http } from 'egg-errors'; class User extends Controller { async show() { this.check(this.params.id); // 继续处理 } check(id) { if (!id) throw new http.UnprocessableEntityError('id is required'); } } 自定义类 import { BaseError } from 'egg-errors'; class CustomError extends BaseError { constructor(message) { super(message); this.code = 'CUSTOM_ERROR'; } } throw new CustomError('xxx'); 错误处理 错误处理是最核心的功能，有如下规则： 未捕获异常不错处理，向上抛 系统异常会打印错误日志，但是会按照标准格式format 业务异常根据标准格式format 根据内容协商，返回对应的format值 可自定义format 标准format { \"code\": \"\", \"message\": \"\" } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg的Http请求.html":{"url":"Node/egg的Http请求.html","title":"egg的Http请求","keywords":"","body":"官网 HTTP状态码 data: Object 需要发送的请求数据，根据 method 自动选择正确的数据处理方式。 GET，HEAD：通过 querystring.stringify(data) 处理后拼接到 url 的 query 参数上。 POST，PUT 和 DELETE 等：需要根据 contentType 做进一步判断处理。 contentType = json：通过 JSON.stringify(data) 处理，并设置为 body 发送。 其他：通过 querystring.stringify(data) 处理，并设置为 body 发送。 // GET + data ctx.curl(url, { data: { foo: 'bar' }, }); // POST + data ctx.curl(url, { method: 'POST', data: { foo: 'bar' }, }); // POST + JSON + data ctx.curl(url, { method: 'POST', contentType: 'json', data: { foo: 'bar' }, }); dataAsQueryString: Boolean 如果设置了 dataAsQueryString=true，那么即使在 POST 情况下， 也会强制将 options.data 以 querystring.stringify处理之后拼接到 url 的 query 参数上。 可以很好地解决以 stream 发送数据，且额外的请求参数以 url query 形式传递的应用场景： ctx.curl(url, { method: 'POST', dataAsQueryString: true, data: { // 一般来说都是 access token 之类的权限验证参数 accessToken: 'some access token value', }, stream: myFileStream, }); By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/怎么开发一个像egg-init的脚手架.html":{"url":"Node/怎么开发一个像egg-init的脚手架.html","title":"怎么开发一个像egg-init的脚手架","keywords":"","body":"想着自己怎么开发一个像egg-init, umi create的脚手架 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/结合源码揭秘egg运行原理.html":{"url":"Node/结合源码揭秘egg运行原理.html","title":"结合源码揭秘egg运行原理","keywords":"","body":"转载自 关于egg egg是阿里开源的一个框架，为企业级框架和应用而生，相较于express和koa，有更加严格的目录结构和规范，使得团队可以在基于egg定制化自己的需求或者根据egg封装出适合自己团队业务的更上层框架 egg所处的定位 天猪曾经在这篇优秀的博文中给出关于egg的定位，如下图： 可以看到egg处于的是一个中间层的角色，基于koa，不同于koa以middleware为主要生态，egg根据不同的业务需求和场景，加入了plugin,extends等这些功能，可以让开发者摆脱在使用middleware功能时无法控制使用顺序的被动状态，而且还可以增加一些请求无关的一些功能。除此之外，egg还有很多其他优秀的功能，在这里不详述。想了解更多可以移步这里 初始化项目 egg有直接生成整个项目的脚手架功能，只需要执行如下几条命令，就可以生成一个新的项目： $ npm i egg-init -g $ egg-init helloworld --type=simple $ cd egg-helloworld $ npm i 启动项目： $ npm run dev $ open localhost:7001 egg是如何运行起来的 下面通过追踪源码来讲解一下egg究竟是如何运行起来的： 查看egg-init脚手架生成的项目文件，可以看到整个项目文件是没有严格意义上的入口文件的，根据package.json中的script命令，可以看到执行的直接是egg-bin dev的命令。找到egg-bin文件夹中的dev.js,会看到里面会去执行start-cluster文件： //dev.js构造函数中 this.serverBin = path.join(__dirname, '../start-cluster'); // run成员函数 * run(context) { //省略 yield this.helper.forkNode(this.serverBin, devArgs, options); } 移步到start-cluster.js文件，可以看到关键的一行代码： require(options.framework).startCluster(options); 其中options.framework打印信息为: /Users/wyf/Project/egg-example/node_modules/egg 找到对应的egg目录中的index.js文件： exports.startCluster = require('egg-cluster').startCluster; 继续追踪可以看到最后运行的其实就是egg-cluster中的startCluster,并且会fork出agentWorker和appWorks，官方文档对于不同进程的fork顺序以及不同进程之间的IPC有比较清晰的说明, 主要的顺序如下： Master 启动后先 fork Agent 进程 Agent 初始化成功后，通过 IPC 通道通知 Master Master 再 fork 多个 App Worker App Worker 初始化成功，通知 Master 所有的进程初始化成功后，Master 通知 Agent 和 Worker 应用启动成功 通过代码逻辑也可以看出它的顺序: //在egg-ready状态的时候就会执行进程之间的通信 this.ready(() => { //省略代码 const action = 'egg-ready'; this.messenger.send({ action, to: 'parent' }); this.messenger.send({ action, to: 'app', data: this.options }); this.messenger.send({ action, to: 'agent', data: this.options }); }); this.on('agent-exit', this.onAgentExit.bind(this)); this.on('agent-start', this.onAgentStart.bind(this)); this.on('app-exit', this.onAppExit.bind(this)); this.on('app-start', this.onAppStart.bind(this)); this.on('reload-worker', this.onReload.bind(this)); // fork app workers after agent started this.once('agent-start', this.forkAppWorkers.bind(this)); 通过上面的代码可以看出，master进程会去监听当前的状态，比如在检测到agent-start的时候才去fork AppWorkers,在当前状态为egg-ready的时候，会去执行如下的进程之间的通信： master---> parent master ---> agent master ---> app fork出了appWorker之后，每一个进程就开始干活了，在app_worker.js文件中，可以看到进程启动了服务，具体代码： //省略代码 function startServer() { let server; if (options.https) { server = require('https').createServer({ key: fs.readFileSync(options.key), cert: fs.readFileSync(options.cert), }, app.callback()); } else { server = require('http').createServer(app.callback()); } //省略代码 } 然后就回归到koa中的入口文件干的事情了。 除此之外，每一个appWorker还实例化了一个Application： const Application = require(options.framework).Application; const app = new Application(options); 在实例化application(options)时，就会去执行node_modules->egg模块下面loader目录下面的逻辑，也就是agentWorker进程和多个appWorkers进程要去执行的加载逻辑，具体可以看到app_worker_loader.js文件中的load(): load() { // app > plugin > core this.loadApplicationExtend(); this.loadRequestExtend(); this.loadResponseExtend(); this.loadContextExtend(); this.loadHelperExtend(); // app > plugin this.loadCustomApp(); // app > plugin this.loadService(); // app > plugin > core this.loadMiddleware(); // app this.loadController(); // app this.loadRouter(); // 依赖 controller } } 这也是下面要讲的东西了 在真正执行业务代码之前，egg会先去干下面一些事情： 加载插件 egg中内置了如下一系列插件: onerror 统一异常处理 Session Session 实现 i18n 多语言 watcher 文件和文件夹监控 multipart 文件流式上传 security 安全 development 开发环境配置 logrotator 日志切分 schedule 定时任务 static 静态服务器 jsonp jsonp 支持 view 模板引擎 加载插件的逻辑是在egg-core里面的plugin.js文件，先看代码： loadPlugin() { //省略代码 //把本地插件，egg内置的插件以及app的框架全部集成到allplugin中 this._extendPlugins(this.allPlugins, eggPlugins); this._extendPlugins(this.allPlugins, appPlugins); this._extendPlugins(this.allPlugins, customPlugins); //省略代码 //遍历操作 for (const name in this.allPlugins) { const plugin = this.allPlugins[name]; //对插件名称进行一些校验 this.mergePluginConfig(plugin); //省略代码 } if (plugin.enable) { //整合所有开启的插件 enabledPluginNames.push(name); } } 如上代码（只是贴出了比较关键的地方），这段代码主要是将本地插件、egg中内置的插件以及应用的插件进行了整合。其中this.allPlugins的结果如下： 可以看出，this.allPlugins包含了所有内置的插件以及本地开发者自定义的插件。先获取所有插件的相关信息，然后将所有插件进行遍历，执行this.mergePluginConfig()函数，这个函数主要是对插件名称进行一些校验。之后还对项目中已经开启的插件进行整合。plugin.js文件还做了一些其他事情，比如获取插件路径，读取插件配置等等，这里不一一讲解。 扩展内置对象 包括插件里面定义的扩展以及开发者自己写的扩展，这也是这里讲的内容。 在对内置对象进行扩展的时候，实质上执行的是extend.js文件，扩展的对象包括如下几个： Application Context Request Response Helper 通过阅读extend.js文件可以知道，其实最后每个对象的扩展都是直接调用的loadExtends这个函数。拿Application这个内置对象进行举例： loadExtend(name, proto) { // All extend files const filepaths = this.getExtendFilePaths(name); // if use mm.env and serverEnv is not unittest const isAddUnittest = 'EGG_MOCK_SERVER_ENV' in process.env && this.serverEnv !== 'unittest'; for (let i = 0, l = filepaths.length; i 将filepaths进行打印,如下图： 可以看出，filepaths包含所有的对application扩展的文件路径，这里会首先将所有插件中扩展或者开发者自己自定义的扩展文件的路径获取到，然后进行遍历，并且对内置对象的一些原有属性和扩展属性进行合并，此时对内置对象扩展的一些属性就会添加到内置对象中。所以在执行业务代码的时候，就可以直接通过访问application.属性（或方法）进行调用。 加载中间件 对中间件的加载主要是执行的egg-core中的middleware.js文件，里面的代码思想也是和上面加载内置对象是一样的，也是将插件中的中间件和应用中的中间件路径全部获取到，然后进行遍历。 遍历完成之后执行中间件就和koa一样了，调用co进行包裹遍历。 加载控制器 对控制器的加载主要是执行的egg-core中的controller.js文件 egg的官方文档中，插件的开发这一节提到： 插件没有独立的 router 和 controller 所以在加载controller的时候，主要是load应用里面的controller即可。详见代码; loadController(opt) { opt = Object.assign({ caseStyle: 'lower', directory: path.join(this.options.baseDir, 'app/controller'), initializer: (obj, opt) => { if (is.function(obj) && !is.generatorFunction(obj) && !is.class(obj)) { obj = obj(this.app); } if (is.promise(obj)) { const displayPath = path.relative(this.app.baseDir, opt.path); throw new Error(`${displayPath} cannot be async function`); } if (is.class(obj)) { obj.prototype.pathName = opt.pathName; obj.prototype.fullPath = opt.path; return wrapClass(obj); } if (is.object(obj)) { return wrapObject(obj, opt.path); } if (is.generatorFunction(obj)) { return wrapObject({ 'module.exports': obj }, opt.path)['module.exports']; } return obj; }, }, opt); const controllerBase = opt.directory; this.loadToApp(controllerBase, 'controller', opt); this.options.logger.info('[egg:loader] Controller loaded: %s', controllerBase); }, 这里主要是针对controller的类型进行判断（是否是Object,class,promise,generator)，然后分别进行处理 加载service 加载service的逻辑是egg-core中的service.js,service.js这个文件比较简单，代码如下: loadService(opt) { // 载入到 app.serviceClasses opt = Object.assign({ call: true, caseStyle: 'lower', fieldClass: 'serviceClasses', directory: this.getLoadUnits().map(unit => path.join(unit.path, 'app/service')), }, opt); const servicePaths = opt.directory; this.loadToContext(servicePaths, 'service', opt); }, 首先也是先获取所有插件和应用中声明的service.js文件目录，然后执行this.loadToContext() loadToContext()定义在egg-loader.js文件中，继续追踪，可以看到在loadToContext()函数中实例化了ContextLoader并执行了load()，其中ContextLoader继承自FileLoader,而且load()是声明在FileLoader类中的。 通过查看load()代码可以发现里面的逻辑也是将属性添加到上下文(ctx)对象中的。也就是说加载context对象是在加载service的时候完成的。 而且值得一提的是：在每次刷新页面重新加载或者有新的请求的时候，都会去执行context_loader.js里面的逻辑，也就是说ctx上下文对象的内容会随着每次请求而发生改变，而且service对象是挂载在ctx对象下面的，对于service的更新，这里有一段代码： // define ctx.service Object.defineProperty(app.context, property, { get() { // distinguish property cache, // cache's lifecycle is the same with this context instance // e.x. ctx.service1 and ctx.service2 have different cache if (!this[CLASSLOADER]) { this[CLASSLOADER] = new Map(); } const classLoader = this[CLASSLOADER]; //先判断是否有使用 let instance = classLoader.get(property); if (!instance) { instance = getInstance(target, this); classLoader.set(property, instance); } return instance; }, }); 在更新service的时候，首先会去获取service是否挂载在ctx中，如果没有，则直接返回，否则实例化service，这也就是service模块中的延迟实例化 加载路由 加载路由的逻辑主要是egg-core中的router.js文件 loadRouter() { // 加载 router.js this.loadFile(path.join(this.options.baseDir, 'app/router.js')); }, 可以看出很简单，只是加载应用文件下的router.js文件 加载配置 直接加载配置文件并提供可配置的方法。 设置应用信息 对egg应用信息的设置逻辑是对应的egg-core中的egg-loader.js,里面主要是提供一些方法获取整个app的信息，包括appinfo，name,path等，比较简单，这里不一一列出 执行业务逻辑 然后就会去执行如渲染页面等的逻辑 总结 这里只是我个人针对源代码以及断点调试总结的一些东西，如有不同见解或者觉得有哪些错误的地方，可以私聊拍砖 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/脚手架之egg-init.html":{"url":"Node/脚手架之egg-init.html","title":"脚手架之egg-init","keywords":"","body":"转载自 egg-init是egg的一个脚手架，用于快速生成一个egg项目或者插件 在剖析egg-init之前，先介绍一下脚手架的相关知识 关于脚手架 脚手架主要是在项目启动过程中生成一些初始文件的，而且一旦生成了初始化文件之后，脚手架就没有用武之地了。但是在一个工程化体系过程中，脚手架的作用还是很大的： 规范团队中的协作开发 快速生成配置文件，节省开发时间 降低框架的学习成本 脚手架一般运行在本地，并且有一些配置选项可以选用。 目前比较流行的脚手架就是yeoman,具体怎么使用可以移步这里 如何开发脚手架 npm bin 需要在package.json中声明bin字段的命令\"bin\": { \"init\": \"bin/init.js\" }, 建立相应的init.js文件 #!/usr/bin/env node //首行必须加上 //具体逻辑 在init.js里面可以添加一些具体的逻辑，如读取文件路径，获取开发者的一些动态设置选项等 全局安装之后，通过npm link或者全局安装创建软链接，我们配置的init命令才能生效 egg-init egg-init提供的options有如下几种: 选项： --type boilerplate type [字符串] --dir target directory [字符串] --force, -f force to override directory [布尔] --template local path to boilerplate [字符串] --package boilerplate package name [字符串] --registry, -r npm registry, support china/npm/custom, default to auto detect [字符串] --silent do not ask, just use default value [布尔] --version 显示版本号 [布尔] -h, --help 显示帮助信息 [布尔] 可以根据个人情况选择相应的配置 egg-init实现的方式也和前面说的方法一样，入口文件就是bin/egg-init.js，但是这个文件很简单，最终调用的是lib/init-command.js。 首先是在构造函数中初始化了一些参数，然后在run主函数里面获取了注册地址，并在terminal中打印出了use registry:的信息，然后是根据用户输入的项目名来生成相应的项目dir。 egg-init支持四种项目类型，分别是： simple empty framework plugin 其中四种项目类型的一些定义都放在boilerplateMapping这个变量中。并且会通过交互式命令行工具yargs记录用户的选择，从而在指定的dir下面下载不同的项目类型文件包。并且会在控制台中打印出下载地址，比如当选择simple时，下载包地址为：http://registry.npm.taobao.org/egg-boilerplate-simple/download/egg-boilerplate-simple-3.0.0.tgz 在下载文件之前会通过askForVariable这个函数来收集用户的一些自定义信息，比如project name、project description等,当设置完了所有的这些配置项之后，就开始在指定的目录中生成文件了。 egg-init初始化的文件 当选择项目类型为simple时，生成的文件如下图所示: 对于app,config,test这些文件，就不说了，基本上也是初始化了egg的一些示例文件。 eslintrc里面声明了如下扩展: { \"extends\": \"eslint-config-egg\" } 对于eslintrc配置的一些说明，可以移步这里 eslint-config-egg这个npm包则是声明了一些书写egg时需要遵循的一些书写规范。 .autod.conf.js则是声明了autod的配置方式，对于autod的学习，可以参见这里和这里,主要是给用户提供自行升级依赖版本的便利 package.json，里面已经配备好了一些字段，比如dependencies、devDependencies以及script等命令。总结 egg-init脚手架的功能主要是生成项目的初始化文件，比较好的是为开发者生成了一些配置，比如说eslint以及gitignore等，会比较方便。相较于自己新建每个文件要快得多。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg框架中的参数校验.html":{"url":"Node/egg框架中的参数校验.html","title":"egg框架中的参数校验","keywords":"","body":"egg框架中的参数校验 egg-validate 对请求参数做校验 egg-sequelize 对存到数据库里面的数据做校验 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg的Controller最佳实践.html":{"url":"Node/egg的Controller最佳实践.html","title":"egg的Controller最佳实践","keywords":"","body":"egg 的 Controller 最佳实践 router描述了URL与Controller的对应关系。egg约定所有的路由都需要在app/router中申明，路由和对应的处理方法分开两个地方维护，开发时进场需要在router.js和Controller之间来回切换 前后协作时，后端需要为每个API都生成一份对应的API文档给前端 更优雅的实现 得益于JavaScript加入的decorator特性，可以是我们跟Java/C#一样，更加直观自然的做面向切面编程。 // 基础版 @route('/intro') async intro() { } // 定义 Method @route('/intro', { method: 'post' }) async intro() { } // 增加权限 @route('/intro', { method: 'post', role: xxxRole }) async intro() { } // Controller 级别中间件 @route('/intro', { method: 'post', role: xxxRole, beforeMiddleware: xxMiddleware }) async intro() { } 为什么是这样的方案 为什么如此复杂的功能，是不是在滥用Decorator? 先看看route的功能: 路由定义 参数校验 权限 Controller级别中间件 router 官方完整定义中包含的功能：路由定义、中间件、权限及文档中未直接写的“权限”： router.verb('router-name', 'path-match', middleware1, ..., middlewareN, app.controller.action); 比较下来会发现，只是多了参数校验的功能 参数校验 class PostController extends Controller { async create() { const ctx = this.ctx; try { // 校验参数 // 如果不传第二个参数会自动校验 `ctx.request.body` ctx.validate(createRule); } catch (err) { ctx.logger.warn(err.errors); ctx.body = { success: false }; return; } } }; 在我们的业务实践中这个方案会有两个问题： 参数漏校验 比如用户提交的数据为 { a: 'a', 'b': 'b', c: 'c' }，如果校验规则只定义了 a，那么 b、c就被漏掉了，并且后续业务中可能会使用这 2 个值。 egg 一个request生命周期内，可以随时随地通过 ctx.request拿到用户数据 因为“参数漏校验”问题的存在，导致后续业务变的不稳定，随时可能都会因为用户的异常数据导致业务崩溃，或者出现安全问题。 解决方案 为了解决“参数漏校验”问题，我们做了如下约定： Controller 也需要申明入参 class UserController extends Controller { @route('/api/user', { method: 'post' }) async updateUser(username) { // ... } } 上面的例子中，即使用户提交了海量数据，业务代码中也只能拿到 username Controller 之外的业务不应该直接访问 ctx.request 上的数据 也就是说，当某个 Service 方法依赖用户数据时，应该通过入参获取，而不是直接访问 ctx.request 基于以上约定，分别看看 JS、TypeScript 下我们如何解决参数校验问题： JS @route('/api/user', { method: 'post', rule: { username: { type: 'string', max: 20 }, } }) async updateUser(username) { // ... } 这里使用了 egg-validate 底层依赖的 parameter 作为校验库 TypeScript @route('/api/user', { method: 'post' }) async updateUser(username: R) { // ... } 没看错，手动调用 ctx.validate(createRule) 并捕获异常的逻辑确实被我们省略掉了。“懒惰”是提高生产力的第一要素。参数、规则都有了，为什么还要自己撸代码呢？ 新的前后端协作实践 传统的前后端开发协作方式中，后端提供 Api 给前端调用，代码类似这样： function updateUser() { request .post(`/api/user`, { username }) .then(ret => { }); } 前端同学需要关注路由、参数、返回值。而这些信息 Controller 都已经有了，直接生成前台 service 用起来是不是更方便呢： Controller 代码： export class UserController { @route({ url: '/api/user' }) async getUserInfo(id: number) { return { ... }; } } 生成Service export class UserService extends Base { /** 首页 */ async getUserInfo(id: number) { const __data = { id }; return await this.request({ method: `get`, url: `/api/user`, data: __data, }); } } export const metaService = new UserService(); export default new UserService(); 前台使用 import { userService } from 'service/user'; const userInfo = await userService.getUserInfo(id); 对比原来的写法： function updateUser() { return new Promise((resolve, reject) => { request .post(`/api/user`, { username }) .then(ret => { resolve(ret); }); }); } userService.getUserInfo 内部封装了 request 逻辑，前端不需要在关心调用过程。 如何在自己的项目中使用 我们已经把最佳实践抽象为了 egg-controller 插件，可以按下面的步骤安装使用： 安装 egg-controller npm i egg-controller 启用插件 打开 config/plugin.js，增加以下配置 aop: { enable: true, package: 'egg-aop', }, controller: { enable: true, package: 'egg-controller', }, 使用插件 详细用法参考egg-controller 文档 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg基于egg-validate的定制化升级.html":{"url":"Node/egg基于egg-validate的定制化升级.html","title":"egg基于egg-validate的定制化升级","keywords":"","body":"转载 egg基于egg-validate的定制化升级 简单讲一下这个egg-validate egg-validate是基于parameter的。安装: npm install --save egg-validate 启用 // config/plugin.js exports.validate = { enable: true, package: 'egg-validate', }; 配置 // config/config.default.js exports.validate = { // convert: false, // validateRoot: false, }; 用途嘛，就是对参数进行检验。比如检验一个用户名是不是字符串，可以这么写， ctx.validate({ userName: ‘string‘ }); 注意：默认就会ctx.request.body进行校验，你想校验ctx.query的话，那就ctx.validate({ userName: 'string}, ctx.query), params就ctx.params。它会在校验失败时抛出一个异常，没有捕获的话，会返回一个422错误。 let errs = app.validator.validate({ userName: ‘string‘ }, ctx.request.body); ctx.validate校验不通过会抛出异常，而app.validator.validate校验不通过回返回错误。你可以自己选择要对这个错误怎么处理，是不管呢还是返回给前端。 关于校验规则rule 完全和parameter的校验规则一样，建议看readme文档。 ctx.validate({ userName: 'string' });这里面的rule就是{ userName: 'string' },它回这么校验，首先判断这个userName有没有在ctx.request.body，没有就抛出参数不存在，然后就是userName不能为空、然后得是string。有一个不符合都会抛出错误。 你想判断一个用户的信息是否正确可以这么写： ctx.validate({ userName: 'userName', // 自定义的校验规则 password: 'password', // 自带的校验规则 sex: ['men', 'women'], // 性别是men或者women age: { type: 'number', // 年龄范围0-120 min: 0, max: 120 } }); 注意：这里有个坑，年龄怎么 填都会报格式错误，这是因为 配置的时候默认把参数类型转型关了，配置回来就好： config.validate = { // 配置参数校验器，基于parameter convert: true, // 对参数可以使用convertType规则进行类型转换 // validateRoot: false, // 限制被验证值必须是一个对象。 }; 主要使用的几个点，核心使用 使用自带的校验规则： password：‘password’，或者 password: { type: 'password', allowEmpty: true, } 校验规则有这些，文档很详细了： 自定义的校验规则 通过app.validator.addRule添加： // 校验用户名是否正确 app.validator.addRule('userName', (rule, value)=>{ // value就是待检验的数据 if (/^\\d+$/.test(value)) { return \"用户名应该是字符串\"; } else if (value.length 10) { console.log(\"用户名的长度应该在3-10之间\"); } }); 这个可以直接放在app.js里面。return的就是错误消息message，但是err不止这个，还有错误字段啊啥的、都会帮我们自动包装好。 app.validator.addRule(‘userName‘, (rule, value)=>{里面这个字符串‘userName‘ 添加了这个规则就可以直接使用ctx.validate({userName: ‘userName‘});。 或者 ctx.validate({userName: { type: 'userName', isAdmin: true' }); app.validator.addRule(‘userName‘, (rule, value)=>{里面这个rule 像3、那样的规则就是把 { type: 'userName', isAdmin: true' } 直接赋值给rule传过来 { type: ‘userName‘, isAdmin: true } 然后你可以自己加判断，比如如果 isAdmin的话，管理员用户名不能有中文啊，长度至少5位啊啥的。 在哪里addRule 上面写的是推荐大家在app.js里面addRule。为什么呢？ 当然你可以在任何你能取到app的地方调用app.validator.addRule(‘userName‘, (rule, value)=>{去addRule。 但是不建议在controller里面addRule。因为controller在每次路由匹配到之后都会进行实例化，所以请求了n遍，也就执行了这个addRule n遍。 而且代码会变的很臃肿。不易于管理。 在app.js addRule当然是很棒的，只在app加载时add一次。 但是问题又来了，随着rule变多，你在app.js里面写了很多代码都是关于addRule的，但是app.js又不止要写addRule、还写了一些别的，那看起来多乱啊。也不利于管理。 如果将来新的项目要用到相同的校验规则难道还有从app.js里面手动拷贝吗。 所以下面接着讲eggjs基于egg-validate的定制化升级 eggjs基于egg-validate的定制化升级 我们希望能达到怎么样的一个效果呢？ app.js里面少写一些代码，最好就写一两行，做个配置这样子 对于所有的自定义校验规则独立出文件夹，可以取名validate，就丢在app/下面 针对相似的校验规则进一步抽象成文件，就叫做user.js这样，丢在app/validate/下面 针对某一条特定的校验规则，如校验用户的userName就丢在app/validate/user.js里面 然后保持egg-validate的使用规则不变，原来是ctx.validate现在还是ctx.vallidate。同时其他的插件、配置不受影响。这叫做代码侵入性小。 首先把app.js里面导入模块写出来 我们使用Loader来加载validate下面的所有文件: const path = require('path'); module.exports = app => { // 你的其它代码，balabala // 加载所有的校验规则 const directory = path.join(app.config.baseDir, 'app/validate'); app.loader.loadToApp(directory, 'validate'); } 然后建立实际的校验规则文件 建立app/validate/user.js文件 module.exports = app =>{ let { validator } = app; // 校验用户名是否正确 validator.addRule('userName', (rule, value)=>{ console.log(rule); if (/^\\d+$/.test(value)) { return \"用户名应该是字符串\"; } else if (value.length 10) { console.log(\"用户名的长度应该在3-10之间\"); } }); // 添加自定义参数校验规则 validator.addRule('123', (rule, value) => { if (value !== '123'){ return 'must be 123'; } }); }; 这样定制化升级就完成了。之后需要再新建检验规则就写在validate里面，某一类相似校验规则要复用就直接拷贝文件就好了。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg中cookie与Session的使用笔记.html":{"url":"Node/egg中cookie与Session的使用笔记.html","title":"egg中cookie与Session的使用笔记","keywords":"","body":"egg中cookie与Session的使用笔记 HTTP 请求都是无状态的，但是我们的 Web 应用通常都需要知道发起请求的人是谁。为了解决这个问题，HTTP 协议设计了一个特殊的请求头：Cookie。服务端可以通过响应头（set-cookie）将少量数据响应给客户端，浏览器会遵循协议将数据保存，并在下次请求同一个服务的时候带上（浏览器也会遵循协议，只在访问符合 Cookie 指定规则的网站时带上对应的 Cookie 来保证安全性） Cookie的设置和获取 Cookie 设置语法：ctx.cookies.set(key, value, options) this.ctx.cookies.set('name','Andy'); Cookie 获取语法：ctx.cookies.get(key, options) this.ctx.cookies.get('name'); 在响应某一个 HTML 页面时，设置 Cookies： 'use strict'; const Controller = require('egg').Controller; class HomeController extends Controller { async index() { /* * * 设置 cookies * ctx.cookies.set(key, value, options) * 参数： * key : cookies 的名称 * value ： cookies 的值 * optons ： 配置 */ this.ctx.cookies.set('username', 'Andy', { maxAge: 1000 * 3600 * 24, // cookies 有效期：一天 httpOnly: true, signed: true, // 对 Cookies 进行签名，防止用户修改 encrypt: true, // 对 Cookies 进行加密，同时获取时ye需要解密 }); // 显示表单页面，加载并渲染 HTML 模版文件 // this.ctx.csrf 用户访问页面时生成的密钥 await this.ctx.render('home'); } } module.exports = HomeController; 然后打开其他页面时，可以获取这个页面中设置的 Cookies： 'use strict'; const Controller = require('egg').Controller; class NewsController extends Controller { // 获取数据显示到新闻页面 async index() { // 获取 cookies 值 const userinfo = this.ctx.cookies.get('username', { encrypt: true, // 对被加密的 Cookies 进行解密 }); console.log(userinfo); // 调用 Service 层方法 const listInfo = await this.service.news.getNewsList(); // 渲染模版引擎 await this.ctx.render('news', { list: listInfo }); } } module.exports = NewsController; 其他问题 如何持久化存储Cookie？ 默认情况下，当浏览器关闭后，Cookie自动销毁。可以设置Cookie的options参数（maxAge或者expires，两者选其一）设置Cookie有效期。 Cookie如何设置中文？ 默认情况下，egg中的Cookie无法设置中文。但是，如果对Cookie进行加密操作，则可以设置中文Cookie了。还有一种方式，也可以通过将Buffer转换为base64进行设置中文了。 Cookie如何设置对象？ 对对象进行序列化和反序列化操作即可设置对象： JSON.stringify() JSON.parse() 如何清除Cookie？ 将Cookie设置为空：this.ctx.cookies.set('name', null), 或者设置maxAge过期时间为0 Session的工作流程 当浏览器访问服务器并发送第一次请求时，服务器端会创建一个 session 对象，生成一个类似于 key，value 的键值对，然后将 key(cookie) 返回到浏览器（客户）端，浏览器下次再访问时，携带 key(cookie)，找到对应的 session（value）。 示例代码： 设置Session： 'use strict'; const Controller = require('egg').Controller; class HomeController extends Controller { async index() { // 设置 session this.ctx.session.username = 'Andy'; // 显示表单页面，加载并渲染 HTML 模版文件 // this.ctx.csrf 用户访问页面时生成的密钥 await this.ctx.render('home'); } } module.exports = HomeController; 获取 Session： 'use strict'; const Controller = require('egg').Controller; class NewsController extends Controller { // 获取数据显示到新闻页面 async index() { // 获取 session 值 const username = this.ctx.session.username; console.log(username); // 调用 Service 层方法 const listInfo = await this.service.news.getNewsList(); // 渲染模版引擎 await this.ctx.render('news', { list: listInfo }); } } module.exports = NewsController; 设置 Session 的有效期 方法一： 'use strict'; const Controller = require('egg').Controller; class HomeController extends Controller { async index() { // 设置 session this.ctx.session.username = 'Andy'; // 设置 Session 的过期时间（基于 Cookies），默认过期时间 24H. this.ctx.session.maxAge = 2000; // 显示表单页面，加载并渲染 HTML 模版文件 // this.ctx.csrf 用户访问页面时生成的密钥 await this.ctx.render('home'); } } module.exports = HomeController; 方法二（全局配置） 推荐在 config.default.js 文件中设置 Session。 因为 Session 是基于 Cookies 的，所以 Session 的配置和 Cookies 基本是一样的，可以使用 Cookies 里面的配置： // 配置 Session config.session = { key: 'SESSION_ID', // 设置 Session cookies 里面的 key maxAge: 24 * 3600 * 1000, // 1 天 httpOnly: true, encrypt: true, renew: true, // 每次刷新页面，Session 都会被延期。 }; By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg-bin源码解析笔记.html":{"url":"Node/egg-bin源码解析笔记.html","title":"egg-bin源码解析笔记","keywords":"","body":"egg-bin源码解析笔记（转载） egg-bin是一个本地开发者工具，集成到egg中，里面涵盖了很多功能，比如调试，单元测试和代码覆盖率等这些功能，可以说是比较强大了。 下面就egg-bin源码分析一些东西（针对的是4.13.2） egg-bin如何工作的： 在本地运行egg项目的时候，我们往往会根据不同的场景（调试、测试等）来选择不同的命令（egg-bin dev、egg-bin debug）启动项目，从而达到我们需要的效果，但是egg-bin是如何让命令运行起来的呢？ 比如在命令行中回车下面的命令： $ egg-bin dev --port 7001 开始进入node_modules/egg-bin/bin/egg-bin.js文件，文件代码比较简单： #!/usr/bin/env node 'use strict'; const Command = require('..'); new Command().start(); 其中，Command对应的是node_modules/egg-bin/bin.egg-bin.js中的EEggBin对象。首先理清以下egg-bin中对应的几个对象之间的关系，如下图： 其中最后导出的EggBin对象以及DevCommand、AutodCommand、TestCommand、PkgFilesCommand继承于egg-bin/lib/command.js里面导出的Command对象，而egg-bin/lib/command.js里面导出的Command又是继承于第三库command-bin，而command-bin中导出的CommandBin对象又是一个yards属性，该属性是目前比较流行的命令工具yargs。DebugCommand和CovCommand则分别继承自DevCommand和TestCommand。 进入index.js文件源代码，该文件至少定义了EggBin这个对象，并且将一些sub command挂载到EggBin这个导出对象中，有如下几个自命令： // load directory this.load(path.join(__dirname, 'lib/cmd')); Command --- 继承自 common-bin的基础命令对象 CovCommand --- 代码覆盖率命令对象 DevCommand --- 本地开发命令对象 TestCommand --- 测试命令对象 DebugCommand --- 调试命令对象 PkgfilesCommand --- 包文件对象 接着就是执行bin/egg-bin.js文件中的new Command().start()这一行，首先会先去执行EggBin构造函数中的内容： class EggBin extends Command { constructor(rawArgv) { // 获取用户输入的options super(rawArgv); this.usage = 'Usage: egg-bin [command] [options]'; // load 对应目录下的command文件 this.load(path.join(__dirname, 'lib/cmd')); } } 获取命令参数 由于上面的继承关系，第一行就会直接执行到Command-bin/lib/command.js中的第一行 /** * original argument * @type {Array} */ this.rawArgv = rawArgv || process.argv.slice(2); 此时 this.rawArgv的值如下： 0: \"dev\" 1: \"--port\" 2: \"7001\" load 配置文件 获取到这个参数之后就会直接将该参数传给yargs并将yyargs对象赋给自己的一个yargs属性 /** * yargs * @type {Object} */ this.yargs = yargs(this.rawArgv); 然后就开始load命令行文件了，通过追踪，也可以发现最后执行的也是common-bin中的load命令common-bin中的load成员函数，该函数要求参数是所需要获取的命令文件的绝对路径，其中common-bin/command.js中的load源码如下： load(fullPath) { // 省略对参数的校验 // load entire directory const files = fs.readdirSync(fullPath); const names = []; for (const file of files) { if (path.extname(file) === '.js') { const name = path.basename(file).replace(/\\.js$/, ''); names.push(name); this.add(name, path.join(fullPath, file)); } } // 省略 } 其中files文件的值为egg-bin/lib/cmd下文件名称： 0: \"autod.js\" 1: \"cov.js\" 2: \"debug.js\" 3: \"dev.js\" 4: \"pkgfiles.js\" 5: \"test.js\" 然后将files进行遍历，执行下面的add的操作: /** * add sub command * @param {String} name - a command name * @param {String|Class} target - special file path (must contains ext) or Command Class * @example `add('test', path.join(__dirname, 'test_command.js'))` */ add(name, target) { assert(name, `${name} is required`); if (!(target.prototype instanceof CommonBin)) { assert(fs.existsSync(target) && fs.statSync(target).isFile(), `${target} is not a file.`); debug('[%s] add command `%s` from `%s`', this.constructor.name, name, target); target = require(target); assert(target.prototype instanceof CommonBin, 'command class should be sub class of common-bin'); } this[COMMANDS].set(name, target); } 其中要求 参数target也是对应的文件的绝对路径。在进行条件判断之后直接使用set将该命令挂载在this[COMMANDS]变量中。遍历完成后this[COMMANDS]的值如下所示： 执行start() 最重要的start操作，追根溯源也是执行的common-bin里面的start(), start()里面主要使用co包了一个generator函数，并且在genertor函数中执行了this[DISPATCH],然后，重头戏来了，this[DISPATCH]的源码如下： /** * dispatch command, either `subCommand.exec` or `this.run` * @param {Object} context - context object * @param {String} context.cwd - process.cwd() * @param {Object} context.argv - argv parse result by yargs, `{ _: [ 'start' ], '$0': '/usr/local/bin/common-bin', baseDir: 'simple'}` * @param {Array} context.rawArgv - the raw argv, `[ \"--baseDir=simple\" ]` * @private */ * [DISPATCH]() { // define --help and --version by default this.yargs // .reset() .completion() .help() .version() .wrap(120) .alias('h', 'help') .alias('v', 'version') .group([ 'help', 'version' ], 'Global Options:'); // get parsed argument without handling helper and version const parsed = yield this[PARSE](this.rawArgv); const commandName = parsed._[0]; if (parsed.version && this.version) { console.log(this.version); return; } // if sub command exist if (this[COMMANDS].has(commandName)) { const Command = this[COMMANDS].get(commandName); const rawArgv = this.rawArgv.slice(); rawArgv.splice(rawArgv.indexOf(commandName), 1); debug('[%s] dispatch to subcommand `%s` -> `%s` with %j', this.constructor.name, commandName, Command.name, rawArgv); const command = this.getSubCommandInstance(Command, rawArgv); yield command[DISPATCH](); return; } // register command for printing for (const [ name, Command ] of this[COMMANDS].entries()) { this.yargs.command(name, Command.prototype.description || ''); } debug('[%s] exec run command', this.constructor.name); const context = this.context; // print completion for bash if (context.argv.AUTO_COMPLETIONS) { // slice to remove `--AUTO_COMPLETIONS=` which we append this.yargs.getCompletion(this.rawArgv.slice(1), completions => { // console.log('%s', completions) completions.forEach(x => console.log(x)); }); } else { // handle by self yield this.helper.callFn(this.run, [ context ], this); } } 首先会去执行yargs中的一些方法，这里common-bin只是保留了yargs中一些对自己有用的方法，比如completion()、wrap()、alias()等，具体关于yargs的API可以移步这里。接着是执行this[PARSE]将rawArgv进行处理，处理后的parse对象结构如下： 接着就是对获取到的命令行进行校验，如果存在this[COMMAND]对象中就执行。在当前例子中也就是去执行DevCommand。而由于DevCommand最终也是继承于common-bin的，然后执行yield command[DISPATCH]();又是递归开始执行this[DISPATCH]了，直到所有的子命令递归完毕，才会去使用helper（common-bin中支持的异步关键所在)类继续执行每个command文件中的*run()函数。 egg-bin中的子命令文件 dev.js 作为在egg项目中本地开发最为重要的开发命令，dev.js无疑肩负着比较重要的指责。在dev.js中，主要是定义了一些默认的端口号，以及入口命令等。*run的源码如下： * run(context) { const devArgs = yield this.formatArgs(context); const env = { NODE_ENV: 'development', EGG_MASTER_CLOSE_TIMEOUT: 1000, }; const options = { execArgv: context.execArgv, env: Object.assign(env, context.env), }; debug('%s %j %j, %j', this.serverBin, devArgs, options.execArgv, options.env.NODE_ENV); yield this.helper.forkNode(this.serverBin, devArgs, options); } 主要是对当前的上下文参数进行转化并对端口进行了一些处理，然后就开始调用helper的forkNode来执行入口命令，其中this.serverBin的值为：Users/uc/Project/egg-example/node_modules/egg-bin/lib/start-cluster,下面的事情可以异步这里进行了解： debug.js 有上分析可知，DebugCommand继承于DevCommand，所以在constructor的时候就会去执行dev中的一些options，而且在debug.js中的*run函数中直接调用的是dev.js中的formatArgs()参数处理。关键源码（有删减）如下： * run(context) { const proxyPort = context.argv.proxy; context.argv.proxy = undefined; const eggArgs = yield this.formatArgs(context); //省略部分 // start egg const child = cp.fork(this.serverBin, eggArgs, options); // start debug proxy const proxy = new InspectorProxy({ port: proxyPort }); // proxy to new worker child.on('message', msg => { if (msg && msg.action === 'debug' && msg.from === 'app') { const { debugPort, pid } = msg.data; debug(`recieve new worker#${pid} debugPort: ${debugPort}`); proxy.start({ debugPort }).then(() => { console.log(chalk.yellow(`Debug Proxy online, now you could attach to ${proxyPort} without worry about reload.`)); if (newDebugger) console.log(chalk.yellow(`DevTools → ${proxy.url}`)); }); } }); child.on('exit', () => proxy.end()); } 此处首先是开启egg,做的是和dev里面一样的东西，然后则是实例化InspectorProxy进行debug操作，在命令行打印出devtools的地址。 test.js 这个命令主要是用来运行egg项目中的test文件的，也就是跑我们自己写的测试用例，关于如何写单元测试，可以异步单元测试,在这个文件，*run形式也和上面类似，然后调用this.formatTestArgs()，formatTestArgs源码如下（有删减）： /** * format test args then change it to array style * @param {Object} context - { cwd, argv, ...} * @return {Array} [ '--require=xxx', 'xx.test.js' ] * @protected */ * formatTestArgs({ argv, debugOptions }) { const testArgv = Object.assign({}, argv); /* istanbul ignore next */ testArgv.timeout = testArgv.timeout || process.env.TEST_TIMEOUT || 60000; testArgv.reporter = testArgv.reporter || process.env.TEST_REPORTER; // force exit testArgv.exit = true; // whether is debug mode, if pass --inspect then `debugOptions` is valid // others like WebStorm 2019 will pass NODE_OPTIONS, and egg-bin itself will be debug, so could detect `process.env.JB_DEBUG_FILE`. if (debugOptions || process.env.JB_DEBUG_FILE) { // --no-timeout testArgv.timeout = false; } // collect require let requireArr = testArgv.require || testArgv.r || []; /* istanbul ignore next */ if (!Array.isArray(requireArr)) requireArr = [ requireArr ]; // clean mocha stack, inspired by https://github.com/rstacruz/mocha-clean // [mocha built-in](https://github.com/mochajs/mocha/blob/master/lib/utils.js#L738) don't work with `[npminstall](https://github.com/cnpm/npminstall)`, so we will override it. if (!testArgv.fullTrace) requireArr.unshift(require.resolve('../mocha-clean')); requireArr.push(require.resolve('co-mocha')); if (requireArr.includes('intelli-espower-loader')) { console.warn('[egg-bin] don\\'t need to manually require `intelli-espower-loader` anymore'); } else { requireArr.push(require.resolve('intelli-espower-loader')); } // for power-assert if (testArgv.typescript) { // remove ts-node in context getter on top. requireArr.push(require.resolve('espower-typescript/guess')); } testArgv.require = requireArr; let pattern; // changed if (testArgv.changed) { pattern = yield this._getChangedTestFiles(); if (!pattern.length) { console.log('No changed test files'); return; } } if (!pattern) { // specific test files pattern = testArgv._.slice(); } if (!pattern.length && process.env.TESTS) { pattern = process.env.TESTS.split(','); } // collect test files if (!pattern.length) { pattern = [ `test/**/*.test.${testArgv.typescript ? 'ts' : 'js'}` ]; } pattern = pattern.concat([ '!test/fixtures', '!test/node_modules' ]); // expand glob and skip node_modules and fixtures const files = globby.sync(pattern); files.sort(); if (files.length === 0) { console.log(`No test files found with ${pattern}`); return; } // auto add setup file as the first test file const setupFile = path.join(process.cwd(), 'test/.setup.js'); if (fs.existsSync(setupFile)) { files.unshift(setupFile); } testArgv._ = files; // remove alias testArgv.$0 = undefined; testArgv.r = undefined; testArgv.t = undefined; testArgv.g = undefined; testArgv.typescript = undefined; return this.helper.unparseArgv(testArgv); } 代码里面的注释很清楚了，就是将单元测试的一些库push进requireArr的值如下： 其中mocha-clean是清除上一次mocha遗留的堆栈了，后面两个就是egg选用的测试框架和断言库了。 然后就是加载egg项目中除掉node_modules和fixtures里面的test文件，即项目层面的*.test.js后面也就是开启进程进行单元测试。 cov.js cov.js是用来测试代码的覆盖率的。其中CovCommand继承自TestCommand，在cov的*run中主要定义了字段，比如exclude、nycCli、coverageDir、outputDir等。根据英文命名就知道是什么意思了。然后继续执行getCovArgs是对参数的一些处理，源码也就很简单，就不贴出来了，在getCovArgs中将上面test.js中的承诺书一起concat进来了，最后返回的covArgs的样子是这样的： 然后又是开启进程了。 autod.js和pkgfiles.js 这两个比较简单，这里就不再赘述了 总结 整个egg-bin看下来，还是很厉害的，涉及的都是我之前没听过或者听过但是没用过的高大尚的东西，比如commander.js,yargs,mocha,co-mocha,power-assert,istanbuljs,nyc， By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg框架与nest框架的对比.html":{"url":"Node/egg框架与nest框架的对比.html","title":"egg框架与nest框架的对比","keywords":"","body":"egg框架与nest框架的对比 nest 官网 中文文档 egg 官网 midway 官网 midway 对比 egg要解决的是多个上层业务框架之间的共建和维护问题。 nest要比也是找egg的上层框架如midway对比（上层框架其实你可以理解为把egg+预制插件+预制配置打包发一个npm，拥有egg的所有特性，有扩展了研发模式。如midway就是增加了TS的能力） midway本身就是egg的上层框架 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/egg常见问题笔.html":{"url":"Node/egg常见问题笔.html","title":"egg常见问题笔记","keywords":"","body":"egg常见问题笔记 egg日志的时区问题 问题：服务器中打印date时间是正常的，但是日志里的时间少了8小时 方法：（待亲自验证）试下加 TZ 这个环境变量来启动看看。或者自定义 formatter，看下 egg-logger 源码 node后台快速开发框架（TypeScript） 基于egg.js后台快速开发框架，极速编码 码云：https://gitee.com/tjp0515/cool-admin-api github：https://github.com/apgzs/cool-admin-api bilibili：https://www.bilibili.com/video/av69398358 定时任务多次执行 app.js 里做了一些初始化的定时任务的逻辑，没有使用官方的schedule，用的是node-cron，每次重启的时候，为啥会执行好几次呀？这个要怎么解决呢？ 方案一：放到agent里执行 方案二：放到schedule里，设置worker执行一次 方案三：在应用启动前调用脚本执行 ctx.ip总是拿到127.0.0.1应该怎么处理 nginx配置 location / { proxy_set_header Host $host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:7071; } 缺少exports.proxy = true; By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/VScode调试Egg.html":{"url":"Node/VScode调试Egg.html","title":"VScode调试Egg","keywords":"","body":"VScode调试Egg 转载 egg-bin egg-bin debug把proxy功能内置了，实现原理参见当时的RFC提案，缤切提供了vscode-eggjs扩展来方便配置。 解决了： 自动attach重启后的worker新端口 自动生成launch.json 对于一般应用开发者基本上非常易用了，但还存在以下问题： vscode的launch.json对同时attach多个的支持不是很友好，虽然有compounds。 默认只attach worker，并且不支持启动期的断点，如果要brk的话要手动attach3次，非常麻烦。 使用vscode进行调试 安装vscode-eggjs((eggjs)),并初始化调试配置（如果之前有则需要删除launch.json） 然后直接F5进入debug 简析 // .vscode/launch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Launch Egg\", \"type\": \"node\", \"request\": \"launch\", \"cwd\": \"${workspaceRoot}\", \"runtimeExecutable\": \"npm\", \"windows\": { \"runtimeExecutable\": \"npm.cmd\" }, // 启动我们的 egg-bin debug 并默认是 brk \"runtimeArgs\": [ \"run\", \"debug\", \"--\", \"--inspect-brk\" ], // 日志输出到 Terminal，否则启动期的日志看不到 \"console\": \"integratedTerminal\", \"protocol\": \"auto\", // 进程重启后自动 attach \"restart\": true, // 因为无需再 proxy，故改回原来的 9229 端口 \"port\": 9229, // 自动 attach 子进程 \"autoAttachChildProcesses\": true } ] } 其他 { \"version\": \"0.2.0\", \"configurations\": [ { \"type\": \"node\", \"request\": \"launch\", \"name\": \"Egg Test\", \"runtimeExecutable\": \"npm\", \"runtimeArgs\": [ \"run\", \"test-local\", \"--\", \"--inspect-brk\" ], \"protocol\": \"auto\", \"port\": 9229, \"autoAttachChildProcesses\": true, \"disableOptimisticBPs\": false, } ] } 补充 Egg 的调试，跟 Node 没啥区别，因此一定要了解 Node 的基础调试知识。 其中，--inspect-brk 是指在进程第一行就暂停，等待 attach，因此： master 先启动，在第一行会停住，需要你 attach master，才会往下走 接着 master 启动 agent，也是在第一行停住，需要 attach agent 才会往下走 最后 agent 启动完成后，worker 才开始启动，一样也是在第一行停住，需要 attach agent 才会往下走 上面这几个 attach，由于上面我们提到的 VSCode 的支持，只需要开启配置，即可无感知的一键 attach。 虽然如此，但作为开发者，大家还是需要理解 Node 的调试原理。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Node/Make命令教程.html":{"url":"Node/Make命令教程.html","title":"Make命令教程","keywords":"","body":"Make命令教程 代码编程可执行文件，叫做编译（compile）； 先编译这个还是那个（即编译的安排），叫做构建（build） Make是最常用的构建工具，诞生于1977年，主要用于C语言的项目。但是实际上，任何只要某个文件有变化，就要重新构建的项目，都可以用Make构建 一、Make的概念 Make这个词，就是要做出某个文件。比如，要做出文件a.txt,就可以执行下面的命令 $ make a.txt 但是，如果你真的输入这条命令，它并不会其作用。因为Make命令本身并不知道，如何作出a.txt，需要有人告诉它，如何调用其他命令完成这个目标 比如，假设文件a.txt依赖于b.txt和c.txt,是后面两个文件连接（cat命令）的产物。那么，make需要知道下面的规则。 a.txt: b.txt c.txt cat b.txt c.txt > a.txt 也就是说，make a.txt这条命令的背后，实际上分为两步：第一步，确认b.txt和c.txt必须已经存在，第二步使用cat命令 将两个文件合并，输出为新文件 像这样的规则，都写在一个Makefile文件中，Make命令依赖这个文件进行构建。Makefile文件也可以写为makefile，或者用命令行参数指定为掐文件名 $ make -f rules.txt # 或者 $ make --file=rules.txt 总之，make只是一个根据制定的Shell命令进行构建的工具。它的规则很简单，你规定要构建哪个文件、它依赖哪些源文件，当那些文件有变动时，如何重新构建它。 二、Makefile文件的格式 构建规则都写在Makefile文件李买呢，要学会如何如何Make命令，就必须学会如何编写Makefile文件 2.1 概述 Makefile文件由一系列规则（rules）构成。每条规则的形式如下： : [tab] 上面第一行冒号前面的部分，叫做\"目标\"（target），冒号后面的部分叫做\"前置条件\"（prerequisites）；第二行必须由一个tab键起首，后面跟着\"命令\"（commands）。 \"目标\"是必需的，不可省略；\"前置条件\"和\"命令\"都是可选的，但是两者之中必须至少存在一个。 每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。下面就详细讲解，每条规则的这三个组成部分。 例子： test: lint yarn ci 2.2 目标（target） 一个目标就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上下文的a.txt。目标可以时一个文件名，指明Make命令所要构建的对象，比如上文的a.txt。目标可以时一个文件名，也可以时多个文件名，之间用空格分割。 除了文件名，目标还可以是某个操作的名字，这称为“伪目标” clean: rm *.o 上面代码的目标是clean，它不是文件名，而是一个操作的名字，属于\"伪目标 \"，作用是删除对象文件。 $ make clean 但是，如果当前目录中，正好有一个文件叫做clean，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。 为了避免这种情况，可以明确声明clean是\"伪目标\"，写法如下。 .PHONY: clean clean: rm *.o temp 声明clean是\"伪目标\"之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。像.PHONY这样的内置目标名还有不少，可以查看手册。 如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。 $ make 上面代码执行Makefile文件的第一个目标。 2.3 前置条件 前置条件通常是一组文件名，之间用空格分隔。它指定了\"目标\"是否重新构建的判断标准：只要有一个前置文件不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），\"目标\"就需要重新构建。 result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt 。 source.txt: echo \"this is the source\" > source.txt 上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。 $ make result.txt $ make result.txt 上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成。 如果需要生成多个文件，往往采用下面的写法。 source: file1 file2 file3 上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令。 $ make source 执行make source命令后，就会一次性生成file1、file2、 file3三个文件 2.4 命令 命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建\"目标\"的具体指令，它的运行结果通常就是生成目标文件。 每行命令之前必须有一个tab键。如果想用其他键，可以用内置变量.RECIPEPREFIX声明。 .RECIPEPREFIX = > all: > echo Hello, world 上面代码用.RECIPEPREFIX指定，大于号（>）替代tab键。所以，每一行命令的起首变成了大于号，而不是tab键。 需要注意的是，每行命令在一个单独的shell中执行。这些Shell之间没有继承关系。 var-lost: export foo=bar echo \"foo=[$$foo]\" 上面代码执行后（make var-lost），取不到foo的值。因为两行命令在两个不同的进程执行。一个解决办法是将两行命令写在一行，中间用分号分隔。 三、Makefile文件的语法 3.1 注释 井号（#）在Makefile表示注释 # 这是注释 result.txt: source.txt # 这是注释 cp source.txt result.txt # 这也是注释 3.2 回声（echoing） 正常情况下，make会打印每条命令，然后再执行，这就叫做回声（echoing） test: # 这是测试 执行上面的规则，会得到下面的结果。 $ make test # 这是测试 在命令的前面加上@，就可以关闭回声。 test: @# 这是测试 现在再执行make test，就不会有任何输出。 由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@。 test: @# 这是测试 @echo TODO 3.3 通配符 通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（*）、问号（？）和 [...] 。比如， *.o 表示所有后缀名为o的文件。 clean: rm -f *.o 3.4 模式匹配 Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。 %.o: %.c 等同于下面的写法。 f1.o: f1.c f2.o: f2.c 使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。 3.5 变量和赋值符 Makefile 允许使用等号自定义变量。 txt = Hello World test: @echo $(txt) 上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。 调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。 test: @echo $$HOME 有时，变量的值可能指向另一个变量。 v1 = ${v2} 上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。 为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看StackOverflow。 VARIABLE = value # 在执行时扩展，允许递归扩展。 VARIABLE := value # 在定义时扩展。 VARIABLE ?= value # 只有在该变量为空时才设置值。 VARIABLE += value # 将值追加到变量的尾端 3.6 内置变量 Make命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 output: $(CC) -o output input.c 3.7 自动变量 Make命令还提供一些自动变量，它们的值与当前规则有关。主要有以下几个。 （1）$@ $@指代当前目标，就是Make命令当前构建的那个目标。比如，make foo的 $@ 就指代foo。 a.txt b.txt: touch $@ 等同于下面的写法。 a.txt: touch a.txt b.txt: touch b.txt （2）$ $ 指代第一个前置条件。比如，规则为 t: p1 p2，那么$ 就指代p1。 a.txt: b.txt c.txt cp $等同于下面的写法。 a.txt: b.txt c.txt cp b.txt a.txt 3.8 判断和循环 Makefile使用Bash语法，完成判断和循环 ifeq ($(CC),gcc) libs=$(libs_for_gcc) else libs=$(normal_libs) endif 上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。 LIST = one two three all: for i in $(LIST); do \\ echo $$i; \\ done # 等同于 all: for i in one two three; do \\ echo $i; \\ done 上面代码的运行结果。 one two three By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/":{"url":"Frame/","title":"架构","keywords":"","body":"服务框架 BFF 前端应用层框架 前端UI组件库 跨平台 工程智能化 WebAssembly完全入门：了解wasm的前世今生 Next与create-react-app应用程序性能更高 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/BFF.html":{"url":"Frame/BFF.html","title":"BFF","keywords":"","body":"BFF Backend for Fronted BFF是目前一种前后顿啊分离的常用研发模式，通常BFF作为胶水层，解决了终端对于数据多样性的问题，通过对后端为服务进行聚合，从而提供各种定制化的数据给到终端应用。 在BFF技术选型上，选用Node是为了技术栈的统一，从而可以让前端同学从前端UI实现到聚合层接口实现都通过JavaScript完成。这样服务端同学只需要按照领域设计原则暴露各个领域的标准化接口，其他部分前端同学可以通过灵活组合满足各种页面的数据服务需求，达到前后端的分离和研发效率提升。 前端服务层 基于Node.js和Koa实现了egg框架，egg.js为企业级框架和应用而生，有Egg.js孕育出更多上层框架，帮助开发团队和开发人员降低开发和维护成本。Egg提供了一个完善灵活的插件机制，并且奉行约定优于配置。在Egg上层，各个业务团队又各自封装了不同的服务层框架，例如蚂蚁Chair、淘宝Midway、UC Nut等等。 Serverless 是阿里内部一个非常重要的方向，目前阿里云已经提供云函数的能力，然后再基于现有的Baas（通信、用户、存储、运维、通知），可以实现BFF层使用云函数来实现，从而大大减少服务器资源的消耗，也极大的减少了前端开发同学对于服务器运维的要求。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/前端应用层框架.html":{"url":"Frame/前端应用层框架.html","title":"前端应用层框架","keywords":"","body":"前端应用层框架 TWA TWA是期望能实现技术无感化的应用，通过全栈研发框架，将前端客户端代码和服务端代码整合在一个代码仓库。通过一站式的研发运维平台，提供几件研发流程和自助式的运维体验，让研发更加关注业务员实现，不用太关心“应用”、“构建”、“部署”、“流程”等细节。 Bigfish/Umi Bigfish是蚂蚁金服前端开发框架，从上图可以看到Bigfish是一个前端研发全流程的研发框架，涵盖设计师（设计师工具、资产市场、文档、培训）、开发者(部署、插件市场、基础开源框架)，同时也包含外部服务（体验、监控等等）。 Umi似一个可插拔企业级的React应用框架，它通过实现沉淀大量最佳时间，极大地实现了React应用框架的统一性，同时它也具有非常强的插件扩展能力。它又几大特点： 插件化：umi的整个生命周期都是插件话的，甚至其内部实现就是由大量插件组成，比如pwa、按需加载、一键切换preact、一键兼容IE9等等，都是由插件实现。 开箱即用：你只需一个umi依赖就可以启动开发，无需安装react、preact、webpack、react-router、Babel、jest等等。 约定式路由：类next.js的约定式路由，无需再维护一份冗余的路由配置，支持权限、动态路由、嵌套路由等等。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/前端UI组件库.html":{"url":"Frame/前端UI组件库.html","title":"前端 UI 组件库","keywords":"","body":"前端UI组件库 阿里前端UI组件库包括： Ant Design-前端中后台React UI组件库。 Element - 前端中后台Vue UI组件库。 AntV - 数据可视化组件库。 ... Ant Design Ant Design服务于企业级产品的设计体系，基于确定和自然的设计价值观上的模块化解决方案，让设计者和开发者专注于更好的用户体验。 Ant Design不仅仅是一套组件库，而且还是一种设计语言，基于「确定」和「自然」的设计价值观，通过模块化的解决方案，降低冗余的生产成本，让设计者专注于更好的用户体验。 Ant Design基于React框架，提供了总共63个UI组件，涵盖各种基本交互元素，例如按钮、布局、数据录入、数据展示等等。 Element Element是饿了么团队基于Vue打造的一套UI组件库，基于一致、反馈、原则。和Ant Design类似，它也提供了丰富的组件，并且提供了央视主体配置化以及国际化等功能 Element和Ant Design几乎已经成为中后台前端的标准UI组件库，往往根据不同的技术栈React配合使用，而Element则会配合Vue进行使用。 AntV AntV 3.0已全新升级，主要包含G2、G6、F2、L7以及一套完整的图表使用和设计规范。得益于丰富的业务场景和用户需求挑战，AntV经历多年积累与不断打磨，已支撑阿里集团内外6000+业务系统。 AntV作为数据可视化的组件库，从简单的线图到流程图，再到地理空间图应有尽有。下面给大家看几个简单例子： By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/跨平台.html":{"url":"Frame/跨平台.html","title":"跨平台","keywords":"","body":"跨平台 移动端跨平台、动态化始终是一个永恒不变的挑战，阿里巴巴作为电商平台也拥有众多App，因此其在跨平台方面有着非常肺腑的尝试。首先就是基于Vue的Weex跨端解决方案。这是一个完全对标React Native的方案，在阿里淘系内部得到了大量的实践。最近一年，闲鱼技术团队大量采用Flutter，同时也沉淀了大量实践，开源了不少Flutter相关项目 Weex 和RN的设计理念非常类似，通过Vue进行UI代码的编写，然后通过Virtual Dom转换成原生组件进行渲染，从而达到Web开发的体验和原生的渲染体验，而且也实现了跨iOS、Android、Web三段，极大地提升了研发效率。 但是，Weex一度外界认为被阿里废弃，开源社区相对RN也不够活跃，所以让很多人望而却步。不过，最近Weex被Apache社区接纳，似乎重新焕发了青春。 Flutter 作为过去一年的当红，Fluter的出现让跨平台的技术方案又出现了一个完全不一样的思路。 Flutter完全摒弃了iOS/Android的UI层，基于C/C++ziji实现了一套UI渲染引擎，在引擎上，基于Dart语言实现了完整的UI框架。由于Flutter自己完全实现了一整套UI框架和底层渲染引擎，所以开发者基于这套框架可以完全实现跨端能力，并且也能获得非常良好的渲染体验。 闲鱼团队在其App中大量实践Flutter并且开源了Flutter Boost、Fish Redux等项目。 Flutter Boost FlutterBoost是一个Flutter插件，它可以轻松地为现有原生应用程序提供Flutter混合继承方案。FlutterBoost的理念是将Flutter像Webview那样来使用。在现有应用程序中同时管理Native页面并非易事。FlutterBoost帮你处理页面的映射和跳转，你只需要关心页面的名字和参数即可（通常可以是URL）。 Flutter Redux Fish Redux通过Redux做集中化、可观察的数据管理。然而不仅如此，对于传统Redux在使用层面上的缺点，在面向端侧flutter页面唯独开发的场景中，我们通过更好更高的抽象，做了改良。 State、Action、Reducer、Store、Middleware以上概念和社区的ReduxJS是完全一致的，我们将原汁原味地保留所有的Redux的优势。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/工程智能化.html":{"url":"Frame/工程智能化.html","title":"工程智能化","keywords":"","body":"工程智能化 前端未来发展的趋势：人工化-> 工具化-> 工程化-> 智能化。 首先，聊聊阿里在前端工程化的发展。随着前端的系统架构越来越复杂，技术栈也越来越多样，对于大型研发团队而言，工程化可以在技术栈标准化、研发效率、质量提升等方面起到极大的作用。 WebIDE 所谓WebIDE就是只需要一个浏览器，就能够让你编写代码、运行代码、甚至发布代码。 WebIDE有几个好处： 无需本地安装IDE 无需关注本地环境，例如环境变量、npm设置等等。 多人协同编辑，结对编程变得更加有趣。 可以大同现有工程化的能力，深入继承脚手架、工程模版、可视化组件编程、编译、打包、部署等等。 云构建 本地构建存在许多问题，例如依赖本地机器性能效率地下、构建工具不统一、本地环境不统一等等。因袭，阿里基于Docker虚拟机搭建前端系统构建环境，解决了环境不一致和构建性能低的问题，同时还提供了完备的灰度管理、实时日志的能力。 在阿里内部，云构建目前日活跃用户有1500+，日狗见谅5W+，物理机器20+，使用量还是非常大的，而且覆盖研发团队也非常广泛。 智能化 - imgcook imgcook 可以实现从设计（design）到代码（code）的转化，目前可以支持sketch、psd和JPG文件。全链路采用计算机视觉、深度学习等智能化手段依此去除对设计稿的约束，通过对Font字体识别、Iconfont图标识别、Layout布局识别、只智能生成代码，保证代码和视觉的高度还原。同时支持多种DSL代码生成，支持小程序、H5、Rax、Weex等框架。 Ant Design Next Ant Design的设计规范和组件库，阿里对中后台前端研发实现了一套low code的方式。 可视化、低代码：通过可视化拖拽界面生成页面代码，自动生成CSS文件完成布局，并且通过封装实现数据绑定、发送请求等操作，简化JS的编写。 提高效率：基于Rest API接口定义，自动生成CRUD页面，完成80%业务场景。 提升体验：通过设计规范提高页面布局规范，同时介绍页面组件渲染提升页面性能，通过也会对构建进行大量优化，默认城店大量最佳实践。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/Next与create-react-app应用程序性能更高.html":{"url":"Frame/Next与create-react-app应用程序性能更高.html","title":"Next与create-react-app 应用程序性能更高","keywords":"","body":"Next VS Gatsby VS create-react-app应用程序性能更高 Next VS Gatsby VS create-react-app是用于构建React应用程序的系统，而无需通过Webpack自己处理应用程序的捆绑。它们可以帮助您快速设置和运行React应用程序 Next.js还是Gastby？为什么选择他们而不是create-react-app create-react-app不能帮助您轻松生成服务端渲染的应用，它随附的所有内容（SEO，速度。。）仅由Next.js和Gatsby之类。 Next.js是一种可以利用React支持服务端渲染（SSR）的方式，同样，create-react-app是您可以利用React支持客户端渲染（CSR）的一种方法 沃尔玛实验室（Walmart Labs）发表了一片很棒的文章，标题为“服务端渲染相对于客户端渲染的好处” 什么时候Next.js比Gatsby更好？ 它们都可以帮助服务器端呈现，但是有2中不同的方式。‘ 使用Gatsby的最终结果是没有服务器的静态站点生成器。您生成站点，然后Netify或另一个静态托管站点上静态部署生成过程的结果。 Next.js提供了一个后端，服务端可以呈现对请求的响应，从而允许您创建动态网站，这意味着您将其部署在可以运行Node.js的平台上。 Next.js也可以生成一个静态站点，但是我不会说这是其主要用例。 如果我的目标是建立一个静态站点，那么我将很难选择，也许Gatsby拥有一个更好的插件生态系统，其中包括许多特别用于博客的插件 Gatsby很大程度上也基于GraphQL By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/基于NodeWeb框架Chair.html":{"url":"Frame/基于NodeWeb框架Chair.html","title":"基于NodeWeb框架Chair","keywords":"","body":"基于NodeWeb框架Chair 转载 Chair是支付宝前端团队推出的，基于Node.js的web框架，适用于大部分的Web应用。 本文简要介绍Chair的设计思想、功能架构和开发状况。 一、Chair的由来和设计思想 历史上，支付宝前端项目都是直接基于Java后端开发的。这种架构下，前端工程师做出网页模板（基于velocity模板引擎的vm文件），交给后端的Java引擎渲染。支付宝采用的Java引擎是名为Sofa的MVC框架。 对于前端工程师来说，这种架构有很多不方便的地方。首先，需要了解后端的实现，并且依赖开发环境中的dev服务器进行调试开发；其次，开发细节需要与后端的Java工程师沟通，交流成本相当大；最后，难以发起技术创新，因为只要涉及后端的调整，推动起来非常困难。在前端技术日新月异的今天，这已经越来越成为前端工程师心中的痛。 Chair框架就是在这种背景下诞生的，我们希望通过加入一个Node层，加速前端开发，提升研发效率，提高网站整体性能和系统的可维护性。 作为Sofa的替代，Chair直接与底层的Java服务通信，而客户端浏览器则与Chair通信，这样就不使用Sofa了。前端工程师因此可以完全不碰Java，使用熟悉的JavaScript语言，同时在浏览器和服务器两端进行快速迭代。 事实上，Chair这个名字就是来自跟Sofa的对比，因为两者都能坐人，但是椅子（Chair）比沙发（Sofa）轻多了。支付宝已经有了沙发，我们想再为它添一把椅子。 Chair为前端开发，带来了很多便利。 提高了研发效率，前端工程师直接可以改动服务器，避免了与Java后端不必要的沟通成本。 更清晰的职责划分，前端针对表现层（View）开发，后端针对业务和数据（Controller和Model）开发。 更好的⼯程化，前端自己就能完成单元测试、集成测试和自动发布。 节省人工，同样的组件（比如模板和路由）只需要写一次，不用再为浏览器和服务器各写一遍了。 预期的性能提升，Node作为服务器端时，有很强的HTTP请求处理能力。 目前，Chair已经投入了生产环境，与Sofa各自支持着不同的支付宝Web应用。预计不远的将来，会出现更多基于Chair的Web应用。 二、Chair的结构 Chair的基础代码，是基于Koa框架的再开发，使用的语言是下一代JavaScript——ECMAScript 6，模板引擎是Nunjucks，但也可选用其他引擎。同时兼容Velocity模板，现有绝⼤部分模板⽂件⽆需修改也能正常渲染。 整个框架从浏览器到服务器，一共分成五层： 路由层（routers）：适配不同路径的HTTP请求 中间件层（middlewares）：加工HTTP请求 控制器层（controllers）：部署业务逻辑 服务层（services）：提供内部的统一API，供不同业务调用 代理层（proxy）：负责与Java服务通信，提供统一格式的数据 除了模板引擎以外，Chair还部署了一些功能组件，比如mock（数据模拟）和logger（日志器）。 Chair根据业务实际需求和现有架构高度定制。虽然从结构上看，Chair可以提供完整的后端功能，但目前主要用于模板渲染和路由。真正的业务逻辑和数据处理，还是要交给后端的Java服务。 三、性能提升 Node的加入，为很多功能提供了很大的性能改进。根据压测的结果，使用Chair（下图的web）比使用原来的方案（下图的portal），响应时间和系统负载能力至少提高一倍以上。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/淘宝网的后台架构发展.html":{"url":"Frame/淘宝网的后台架构发展.html","title":"淘宝网的后台架构发展","keywords":"","body":"淘宝网的后台架构发展 转载 文章最后汇总了一些架构设计的原则。 基本概念 在介绍架构之前，为了避免部分读者对架构设计中的一些概念不了解，下面对几个最基础的概念进行介绍。 1）什么是分布式？ 系统中的多个模块在不同服务器上部署，即可称为分布式系统，如Tomcat和数据库分别部署在不同的服务器上，或两个相同功能的Tomcat分别部署在不同服务器上。 2）什么是高可用？ 系统中部分节点失效时，其他节点能够接替它继续提供服务，则可认为系统具有高可用性。 3）什么是集群？ 一个特定领域的软件部署在多台服务器上并作为一个整体提供一类服务，这个整体称为集群。 如Zookeeper中的Master和Slave分别部署在多台服务器上，共同组成一个整体提供集中配置服务。 在常见的集群中，客户端往往能够连接任意一个节点获得服务，并且当集群中一个节点掉线时，其他节点往往能够自动的接替它继续提供服务，这时候说明集群具有高可用性。 4）什么是负载均衡？ 请求发送到系统时，通过某些方式把请求均匀分发到多个节点上，使系统中每个节点能够均匀的处理请求负载，则可认为系统是负载均衡的。 5）什么是正向代理和反向代理？ 系统内部要访问外部网络时，统一通过一个代理服务器把请求转发出去，在外部网络看来就是代理服务器发起的访问，此时代理服务器实现的是正向代理； 当外部请求进入系统时，代理服务器把该请求转发到系统中的某台服务器上，对外部请求来说，与之交互的只有代理服务器，此时代理服务器实现的是反向代理。 简单来说，正向代理是代理服务器代替系统内部来访问外部网络的过程，反向代理是外部请求访问系统时通过代理服务器转发到内部服务器的过程。 纯真年代：单机架构 以淘宝作为例子：在网站最初时，应用数量与用户数都较少，可以把Tomcat和数据库部署在同一台服务器上。浏览器往www.taobao.com发起请求时，首先经过DNS服务器（域名系统）把域名转换为实际IP地址10.102.4.1，浏览器转而访问该IP对应的Tomcat。 架构瓶颈：随着用户数的增长，Tomcat和数据库之间竞争资源，单机性能不足以支撑业务。 第一次演进：Tomcat与数据库分开部署 Tomcat和数据库分别独占服务器资源，显著提高两者各自性能。 架构瓶颈：随着用户数的增长，并发读写数据库成为瓶颈。 Tips：欢迎关注微信公众号：架构之路，获取更多技术博文推送。 第二次演进：引入本地缓存和分布式缓存 在Tomcat同服务器上或同JVM中增加本地缓存，并在外部增加分布式缓存，缓存热门商品信息或热门商品的html页面等。通过缓存能把绝大多数请求在读写数据库前拦截掉，大大降低数据库压力。 其中涉及的技术包括：使用memcached作为本地缓存，使用Redis作为分布式缓存，还会涉及缓存一致性、缓存穿透/击穿、缓存雪崩、热点数据集中失效等问题。 架构瓶颈：缓存抗住了大部分的访问请求，随着用户数的增长，并发压力主要落在单机的Tomcat上，响应逐渐变慢。 第三次演进：引入反向代理实现负载均衡 在多台服务器上分别部署Tomcat，使用反向代理软件（Nginx）把请求均匀分发到每个Tomcat中。此处假设Tomcat最多支持100个并发，Nginx最多支持50000个并发，那么理论上Nginx把请求分发到500个Tomcat上，就能抗住50000个并发。 其中涉及的技术包括：Nginx、HAProxy，两者都是工作在网络第七层的反向代理软件，主要支持http协议，还会涉及session共享、文件上传下载的问题。 架构瓶颈：反向代理使应用服务器可支持的并发量大大增加，但并发量的增长也意味着更多请求穿透到数据库，单机的数据库最终成为瓶颈。 第四次演进：数据库读写分离 把数据库划分为读库和写库，读库可以有多个，通过同步机制把写库的数据同步到读库，对于需要查询最新写入数据场景，可通过在缓存中多写一份，通过缓存获得最新数据。 其中涉及的技术包括：Mycat，它是数据库中间件，可通过它来组织数据库的分离读写和分库分表，客户端通过它来访问下层数据库，还会涉及数据同步，数据一致性的问题。 架构瓶颈：业务逐渐变多，不同业务之间的访问量差距较大，不同业务直接竞争数据库，相互影响性能。 第五次演进：数据库按业务分库 把不同业务的数据保存到不同的数据库中，使业务之间的资源竞争降低，对于访问量大的业务，可以部署更多的服务器来支撑。这样同时导致跨业务的表无法直接做关联分析，需要通过其他途径来解决，但这不是本文讨论的重点，有兴趣的可以自行搜索解决方案。 架构瓶颈：随着用户数的增长，单机的写库会逐渐会达到性能瓶颈。 第六次演进：把大表拆分为小表 比如针对评论数据，可按照商品ID进行hash，路由到对应的表中存储；针对支付记录，可按照小时创建表，每个小时表继续拆分成小表，使用用户ID或记录编号来路由数据。只要实时操作的表的数据量足够小，请求能够足够均匀的分发到多台服务器上的小表，拿数据库就能通过水平扩展的方式来提高性能。其中前面提到的Mycat也支持在大表拆分为小表情况下的访问控制。 这种做法显著的增加了数据库运维的难度，对DBA的要求较高。数据库设计到这种结构时，已经可以称为分布式数据库，但是这只是一个逻辑的数据库整体，数据库里不同的组成部分是由不同的组件单独来实现的，如分库分表的管理和请求分发，由Mycat实现，SQL的解析由单机的数据库实现，读写分离可能由网关和消息队列来实现，查询结果的汇总可能由数据库接口层来实现等等，这种架构其实是MPP（大规模并行处理）架构的一类实现 目前开源和商用都已经有不少MPP数据库，开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通的GBase、睿凡科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据哭基本都提供了类似Postgresql】Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分布到每台机器上并执行，最终数据库本身汇总数据进行返回，也提供了注入权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库韵味的成本，并且使数据库也能够实现水平扩展。 架构瓶颈：数据库和Tomcat能够水平扩展，可支撑的并发大幅提高，随着用户数的增长，最终单机的Nginx会成为瓶颈 第七次演进：使用LVS或F5来使多个Nginx负载均衡 由于瓶颈在Nginx，因袭无法通过两层的Nginx来实现多个Nginx的负载均衡。图中的LVS和F5时工作在网络第四层的负载均衡解决方案，其中LVS是软件，运行在操作系统内核态，可对TCP请求或更更高层级的网络协议进行转发，因袭支持的协议更丰富，冰球性能也远高于Nginx，可假设单机的LVS可支持几十万个并发的请求转发；F5是一种负载均衡硬件，与LVS提供的能力类似，性能比LVS更高，但价格昂贵。由于LVS是单机版的软件，若LVS所在服务器宕机则会导致整个后端系统都无法发文，因袭需要有备用节点。可食用keepalived软件模拟出虚拟IP，然后把虚拟IP绑定到多台LVS服务器上，浏览器访问虚拟IP时，会被路由器重定向到真实的LVS服务器，当主LVS服务器宕机时，keepalived软件会自动更新路由器中的路由表，把虚拟IP重定向到另外一台正常的LVS服务器，从而达到LVS服务器高可用的效果。 此处需要注意的是，上图中从Nginx层到Tomcat层这样画并不代表全部Nginx都转发请求到全部的Tomcat，在实际使用时，可能会是几个Nginx下面接一部分的Tomcat，这些Nginx之间通过keepalived实现高可用，其他的Nginx接另外的Tomcat，这样可接入的Tomcat数量就能成倍的增加。 架构瓶颈：由于LVS也是单机的，随着并发数增长到几十万时，LVS服务器最终会达到瓶颈，此时用户数达到千万甚至上亿级别，用户分布在不同的地区，与服务器机房距离不同，导致了访问的延迟会明显不同。 第八次演进：通过DNS轮询实现机房间的负载均衡 在DNS服务器中可配置一个域名对应多个IP地址，每个IP地址对应到不同的机房里的虚拟IP。当用户访问www.taobao.com时，DNS服务器会使用轮训策略或其他策略，来选择某个IP供用户访问，此方式能实现机房间的负载均衡，至此，系统可做到机房级别的水平扩展，千万级别到亿级的兵法量都可以通过增加机房来解决，系统入口处的请求并发量不再是问题 架构瓶颈：随着数据的丰富成都和业务的发展，检索、分析等需求越来越丰富，单单依靠数据库无法解决如此丰富的需求。 第九次演进：引入NoSQL数据库和搜索引擎等技术 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询编码，对于全文检索、可变数据结构等场景，数据库天生不适用。因此需要针对特定的场景，引入合适的解决方案。如对于海量文件存储，可通过分布式文件系统HDFS解决，对于key value类型的数据，可通过HBase和Redis等方案解决，对于全文检索场景，可通过搜索引擎如ElasticSearch解决，对于多维分析场景，可通过Kylin或Druid等方案解决。 当然，引入更多组件同时会提高系统的复杂度，不同的组件保存的数据需要同步，需要考虑一致性的问题，需要有更多的运维手段来管理这些组件等。 架构瓶颈：引入更多组件解决了丰富的需求，业务维度能够极大扩充，随之而来的是一个应用中包含了太多的业务代码，业务的升级迭代变得困难。 第十次演进：大应用拆分为小应用 按照业务板块来划分应用代码，使单个应用的职责更清晰，相互之间可以做到独立升级迭代。这时候应用之间可能会涉及到一些公共配置，可以通过分布式配置中心Zookeeper来解决。 架构瓶颈：不同应用之间存在共用的模块，由应用单独管理会导致相同代码存在多份，导致公共功能升级时全部应用代码都要跟着升级。 第十一次演进：复用的功能抽离成微服务 如用户管理、订单、支付、鉴权等功能在多个应用中都存在，那么可以把这些功能的代码单独抽取出来形成一个单独的服务来管理，这样的服务就是所谓的微服务，应用和服务之间通过HTTP、TCP或RPC请求等多种方式来访问公共服务，每个单独的服务都可以由单独的团队来管理。此外，可以通过Dubbo、SpringCloud等狂简实现服务治理、限流、熔断、降级等功能，提高服务的稳定性和可用性。 架构瓶颈：不同服务的接口访问方式不同，应用代码需要适配多种访问方式才能使用服务，此外，应用访问服务，服务之间也可能相互访问，调用链将会变得非常复杂，逻辑变得混乱 第十二次演进：引入企业服务总线ESB屏蔽服务接口的访问差异 通过ESB统一进行访问协议转换，应用同意通过ESB来访问后端服务，服务与服务指甲吗也通过ESB来相互调用，以此降低系统的耦合程度 这种单个应用拆分为多个应用，公共服务单独抽取出来来管理，并使用企业消息总线来解除服务之间耦合问题的架构，就是所谓的SOA（面向服务）架构，这种架构与微服务架构容易混淆，因为表现形式十分相似。 个人理解，微服务架构更多是指把系统里的公共服务抽取出来单独运维管理的思想，而SOA架构则是指一种拆分服务并使服务接口访问变得统一的架构思想，SOA架构中包含了微服务的思想。 架构瓶颈：业务不断发展，应用和服务都会不断变多，应用和服务的部署变得复杂，同一台服务其上部署多个服务还要解决运行环境冲突的问题，此外，对于如大促这类需要动态扩缩容的场景，需要水平扩展服务的型嫩高，就需要在新增的服务上准备运行环境，部署服务等，运维将变得十分困难。 第十三次演进：引入容器化技术实现运行环境隔离与动态服务管理 目前最流行的容器化技术是Docker，最流行的容器管理服务是Kubernetes(k8s)，应用/服务可以打包为Docker镜像，通过K8S来动态分发和部署镜像。Docker镜像可理解为一个能运行你的应用/服务最小的操作系统，里面放着应用/服务的运行代码，运行环境根据实际的需要设置好。把这个“操作系统”打包为一个镜像后，就可以分发到 需要部署相关服务的机器上，直接启动Docker京瓷昂就可以把服务起起来，使服务的部署和运维变得简单。 在大促的之前，可以在现有的机器集群上划分出服务器来启动Docker镜像，增强服务的性能，大促过后就可以关闭镜像，对机器上的其他服务不造城影响（在第18节之前，服务运行在新增机器上需要修改系统配置来适配服务，这会导致机器上其他服务需要的运行环境被破坏）。 架构瓶颈：使用容器化技术后服务动态扩缩容问题得以解决，但是机器还是需要公司自身来管理，在非大促的时候，还是需要闲置着大量的机器资源来应对大促，机器自身成本和运维成本都极高，资源利用率低。 第十四次演进：以云平台承载系统 系统可部署到公有云上，利用公有云的海量及其资源，解决动态硬件资源的问题，在大促的时间段里，在云平台中临时申请更多的资源，结合Docker和K8S来快速部署服务，在大促结束后释放资源，真正做到按需付费，资源利用率大大提高，同时大大降低了运维成本 所谓的云平台，就是把海量机器资源，通过统一的资源管理，抽象为一个资源整体，在智商可按需动态申请硬件资源（如CPU、内存、网络等），并且之上提供通用的操作系统，提供常用的技术组件（如Hadoop技术栈，MPP数据库等）供用户使用，甚至提供开发好的应用，用户不需要关心应用内部使用了什么技术，就能够解决（如音视频转码服务、邮件服务、个人博客等）。 在云平台会涉及如下几个概念： 1）laaS：基础设施即服务。对应于上面所说的及其资源统一为资源整体，可动态申请硬件资源的层面 2）PaaS：平台即服务。对应于上面所说的提供常用的技术组件方便系统的开发和维护； 3）SaaS：软件即服务。对应于上面所说的提供开发好的应用或服务，安功能或性能要求付费 至此：以上所提到的从高并发访问问题，到服务的架构和系统实施的层面都有了各自的解决方案。但同时也应该意识到，在上面的介绍中，其实是有意忽略了诸如跨机房数据同步、分布式事务实现等等的实际问题，这些问题以后有机会再拿出来单独讨论。 架构设计经验小结 1） 结构的调整是否必须按照上述演变路进行？ 不是的，以上所说的架构演变顺序只是针对某个侧面进行单独的改进，在实际场景中，可能同一时间会有几个问题需要解决，或者可能先达到瓶颈的是另外的方面，这时候就应该按照实际问题实际解决。如在政府类的并发量可能不大，但业务可能很丰富的场景，高并发就不是重点解决的问题，此时优先需要的可能会是丰富需求的解决方案。 2）对于将要实施的系统，架构应该设计到什么程度？ 对于单次实施并且性能指标明确的系统，架构设计到能够支持系统的性能指标要求就足够了，但要留有扩展架构的接口以便不备之需。对于不断发展的系统，如电商平台，应设计到能满足下一阶段用户量和性能指标要求的程度，并根据业务的增长不断的迭代升级架构，以支持更高的并发和更丰富的业务。 3）服务端架构和大数据架构有什么区别？ 所谓的“大数据”其实是海量数据采集清晰转换、数据存储、数据分析、数据服务等场景解决方案的一个统称，在每一个场景都包含了多种可选的技术，如数据采集有Flume、Sqoop、Kettle等，数据存储有分布式文件系统HDFS、FastDFS、NoSQL数据库HBase、MongoDB等，数据分析有Spark技术栈、机器学习算法等 总的来说大数据架构就是根据业务的需求，整合各种大数据组件组合而成的架构，一般会提供分布式存储、分布式计算、多维分析、数据仓库、机器学习算法等能力。而服务端架构更多指的是应用组织层面的架构，底层能力往往是由大数据架构来提供。 4）有没有一些架构设计的原则？ N+1设计：系统中的每个组件都应做到没有单点故障； 回滚设计：确保系统可以向前兼容，在系统升级时应能有办法回滚版本； 禁用设计：应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能； 监控设计：在设计阶段就要考虑监控的手段； 多活数据中心设计：若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用； 采用成熟的技术：刚开发的或开源的技术往往存在很多隐藏的bug，出了问题没有商业支持可能会是一个灾难； 资源隔离设计：应避免单一业务占用全部资源； 架构应能水平扩展：系统只有做到能水平扩展，才能有效避免瓶颈问题； 非核心则购买：非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品； 使用商用硬件：商用硬件能有效降低硬件故障的机率； 快速迭代：系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险； 无状态设计：服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Frame/应用场景数据库选型.html":{"url":"Frame/应用场景数据库选型.html","title":"应用场景数据库选型","keywords":"","body":"应用场景 当数据库中的数据多到一定规模时，数据库就不适用于复杂的查询了，往往只能满足普通查询的场景。对于统计报表场景，在数据量大时不一定能跑出结果，而且在跑复杂查询时会导致其他查询变慢，对于全文检索、可变数据结构等场景，数据库天生不适用。 因此需要哦针对特定的场景，引入合适的解决方案。如: 对于海量文件存储，可通过分布式文件系统HDFS解决 对于key value类型的数据，可通过HBase和Redis 对于全文检索场景，可通过搜索引擎如ElasticSearch解决， 对于多维分析场景，可通过Kylin或Druid等方案解决 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/":{"url":"Git/","title":"Git发布部署相关","keywords":"","body":"Git以及代码规范 Mac下配置多个Git账户 删除远程仓库的某次错误提交 使用Github Actions进行版本发布 使用Github Actions进行版本发布(一) Git subtree教程 git分支开发规范 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/Mac下配置多个Git账户.html":{"url":"Git/Mac下配置多个Git账户.html","title":"Mac下配置多个Git账户","keywords":"","body":"应用场景 我们经常将代码托管到gitlab、github这样的网站，为了避免每次push代码都要输入用户名和 密码，通常回选择ssh协议，将公约保存到托管网站上。在实际开发中，往往要将代码托管到多个不同的网站上。比如公司的代码需要托管到gitlab，自己的开源代码托管到github上，每个托管站上都对应一个git账户。默认情况下，一台电脑的Git对应一个账户，只能网一个网站上push代码，非常不方便。这篇博客将介绍如何在一个终端中配置多个账户。同时管理多个托管网站的代码。 准备工作 首先，需要准备好对git的全局用户进行配置。在初次安装Git时，往往会使用如下命令配置全局用户名和邮箱。 git config --global user.name 'xxxx' //配置全局的用户名，比如Github上注册的yonghuming git config --global user.email 'xxxx' // 配置全局邮箱，比如GitHub上配置的邮箱 这个--global选项，是只这里配置的user.name和user.email是像杜预全局进行配置的，即不同的Git仓库默认的用户名和邮箱都是这个值。由于需要管理多个账户，所以仅仅使用这个全局全局值是不够的，需要在每个仓库中单独配置。对此，有两种处理方法： 如果之前已经使用该命令进行配置，则先使用如下命令进行清楚 git config --global --unset user.name git config --global --unset user.email 如果不确定是否已经配置过，可以使用下面的命令查看 git config --global user.name git config --global user.email 配置步骤： 1. 对每个账户生成一对密钥 首先进入保护密钥的目录： cd ~/.ssh // 进入目录，该目录下保存生成的密钥 然后，根据账户邮箱生成密钥。例如我在GitHub上的邮箱是1352118502@qq.com，则命令为 ssh-keygen -t rsa -C \"1352118502@qq.com\" 输入完成后，会有如下提示： Generating public/private rsa key pair. Enter file in while to save the key (/Users/coco/.ssh/id_rsa): 这里要求对密钥进行命名，默认的文件是id_rsa。为了方便区分，我这里命名为id_rsa_github。接下来的提示都直接进行enter，直到密钥生成。通过ls命令，可以看到刚刚生成的密钥对id_rsa_github和id_rsa_github.pub。其中id_rsa_github.pub是公钥。 同样，对于GitLab上的账户，我是用另一个邮箱注册的，按照同样的步骤生成id_rsa_github的密钥对。接下来的步骤，除额外说明外，两个账户的操作完全相同。 2. 私钥添加到本地 SSH协议的原理，就是托管网站上使用的公钥，在本地使用私钥，这样本地仓库就可以和远程仓库进行通信。在上一步已经生成了密钥晚间，接下来需要私钥文件，首先是在本地使用密钥文件： ssh-add ~/.ssh/id_rsa_github // 将GitHub私钥添加到本地 ssh-add ~/.ssh/id_rsa_gitlab // 将GitLab私钥添加到本地 为了检验本地是否添加成功，可以使用ssh-add -l命令查看 3. 对本地密钥进行配置 由于添加了多个密钥文件，所以需要对这多个密钥进行管理。在.ssh目录下新建一个config文件： touch config 文件的内容如下： Host github HostName github.com User CoCoyh IdentityFile ~/.ssh/id_rsa Host gitlab HostName gitlab.com User CoCoyh IdentityFile ~/.ssh/id_rsa_gitlab 4. 公钥添加到托管网站 以GitHub为例，先在本地复制公钥。进入ssh目录，使用cat id_rsa_github.pub查看生成的GitHub公钥，全选进行复制。 登陆GitHub，点击右上角头像选择settings，在打开的页面中选择SSH and GPG keys，至此，托管网站的公钥添加完成。总结来说，就是针对每个托管网站分别生成一对密钥，然后分别添加到本地和托管网站。这时候，可以测试一下是否成功，测试命令使用别名。例如，对于GitHub，本来应该使用的测试命令是: ssh -T git@github.com 在config文件中，给GitHub网站配置的别名就是gitHub，所以直接使用别名，就是 ssh -T git@github By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/删除远程仓库的某次错误提交.html":{"url":"Git/删除远程仓库的某次错误提交.html","title":"删除远程仓库的某次错误提交","keywords":"","body":"加入你只是想修改上次提交的代码，做一次更完美的commit，可以这样 查看commit idgit log 到上个版本git reset commitId 暂存修改git stash 强制push，远程的最新的一次commit被删除git push --force 释放暂存的修改，开始修改代码 git stash pop By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/使用GithubActions进行版本发布.html":{"url":"Git/使用GithubActions进行版本发布.html","title":"使用Github Actions进行版本发布","keywords":"","body":"使用Github Actions进行版本发布 Github Actions: Github提供的Workflow工具，基于Events提供了一个容器运行环境。 发布流程 主仓库发布 GitHub Config .gitHub/main.workflow ## workflow workflow \"Push\" { on = \"push\" resolves = [\"workman release\"] } workflow \"Pull Request\" { on = \"pull_request\" resolves = [\"workman check\"] } ## actions action \"npm install\" { uses = \"docker://thonatos/github-actions-nodejs\" args = \"npm install\" } action \"npm ci\" { uses = \"docker://thonatos/github-actions-nodejs\" needs = [\"npm install\"] env = { YUQUE_GROUP = \"eggjs-dev\" YUQUE_ENDPOINT = \"https://www.yuque.com/api/v2/\" } secrets = [ \"YUQUE_TOKEN\" ] args = \"npm run ci\" } action \"npm build\" { uses = \"docker://thonatos/github-actions-nodejs\" needs = [\"npm ci\"] args = \"npm run build\" } ## target action \"workman check\" { uses = \"thonatos/github-actions-workman@1.6.0-Marketplace\" needs = [\"npm ci\"] args = \"workman check\" secrets = [ \"GITHUB_TOKEN\", \"NPM_TOKEN\" ] } action \"workman release\" { uses = \"thonatos/github-actions-workman@1.6.0-Marketplace\" needs = [\"npm build\"] args = \"workman release --releaseBranch master\" secrets = [ \"GITHUB_TOKEN\", \"NPM_TOKEN\" ] } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/使用GithubActions进行版本发布（一）.html":{"url":"Git/使用GithubActions进行版本发布（一）.html","title":"使用Github Actions进行版本发布(一)","keywords":"","body":"使用Github Actions进行版本发布(一) By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/gitsubtree教程.html":{"url":"Git/gitsubtree教程.html","title":"Git subtree教程","keywords":"","body":"git subtree教程 关于子仓库或者说是仓库公用，git官方推荐的工具是git subtree。我自己也用了一段时间的git subtree，感觉比git submodule好用，到那时也有一些缺点，在可接受的范围内。 所以对于仓库的公用，在git subtree与git submodule之中选择的话，我推荐git subtree。 git subtree是什么？为什么使用git subtree git subtree可以实现一个仓库作为其他仓库的子仓库。 使用git subtree有以下几个原因： 旧版本的git也支持（最老版本可以到v1.5.2）. git subtree与git submodule不同，它不增加任何像.gitmodule这样的新的源数据文件 git subtree对于项目中的其他成员透明，意味着可以不知道git subtree的存在。 当然，git subtree也有它的缺点，到那时这些缺点还在可以 接受的范围内： 必须学习新的指令（如： git subtree） 子仓库的更新与推送指令相对复杂。 git subtree的使用 git subtree的主要命令有： git subtree add --prefix= git subtree add --prefix= git subtree pull --prefix= git subtree push --prefix= git subtree merge --prefix= git subtree split --prefix= [OPTIONS] [] 准备 我嫩先准备一个仓库叫photoshop，一个仓库叫libpng,然后我们希望把libpng作为photoshop的子仓库。photoshop的路径为https://github.com/test/photoshp.git，仓库里的文件有： photoshop | |-- photoshop.c |-- photoshop.h |-- main.c \\-- README.md libPNG的路径为https://github.com/test/libpng.git，仓库里的文件有： libpng | |-- libpng.c |-- libpng.h \\-- README.md 以下操作均位于附仓库的根目录中。 父仓库中新增子仓库 我们执行以下inkling把libpng添加到photoshop中： git subtree add --prefix=sub/libpng https://github.com/test/libpng.git master --squash (--squash参数表示不拉取历史信息，而只生成一条commit信息) 执行git status可以看到提示新增两条commit： 执行git push把修改推送到远端phtotshop仓库，现在本地仓库与远端仓库的目录结构为： photoshop | |-- sub/ | | | \\--libpng/ | | | |-- libpng.c | |-- libpng.h | \\-- README.md | |-- photoshop.c |-- photoshop.h |-- main.c \\-- README.md 注意，现在的photoshop仓库对于其他项目人员来说，可以不需要知道libpng是一个子仓库。什么意思呢？ 当你git clone或者git pull的时候，你拉取的是整个photoshop（包括libpng在内，libpng就相当于 photoshop里的一个普通目录）；当你修改了libpng里的内容后执行git push，你讲会把修改push到photoshop上。 也就是说photoshop仓库下的libpng与其他文件无异。 从源仓库拉取更新 如果源libpng仓库更新了，photoshop里的libpng如何如何拉取更新？使用git subtree pull，例如： git subtree pull --prefix=sub/libpng https://github.com/test/libpng.git master --squash 推送修改到源仓库 如果在photoshop仓库里修改了libpng，然后想把这个修改推送到源libpng仓库呢？使用git subtree push，例如： git subtree push --prefix=sub/libpng https://github.com/test/libpng.git master 简化git subtree命令 我们已经知道了git subtree的命令的基本用法，但是上述几个命令还是显得有点复杂，特别是子仓库的源仓库地址，特别不方便记忆。 这里我们把子仓库的地址作为一个remote，方便记忆： git remote add -f libpng https://github.com/test/libpng.git 然后可以这样来使用git subtree命令： git remote add -f libpng https://github.com/test/libpng.git By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/git分支开发规范.html":{"url":"Git/git分支开发规范.html","title":"git分支开发规范","keywords":"","body":"git分支开发规范 分支管理 分支命名 master分支 master为主分支，也是用于部署生产环境的恩智，确保master分支稳定性。 master分支一般由develop以及hotfix分支合并，任何时间都不能直接修改代码 develop分支 develop为开发分支，始终保持最新完成以及bug修复后的代码 一般开发新的功能是，feature分支都是基于develop分支创建的。 feature分支 开发新功能时，以develop为基础创建feature分支 分支命名：feature/开头的为特性分支，命名规则：feature/user—module、feature/cart—module release分支 release为预上线分支，发布提测阶段，会release分支代码为基准提测 当有一组feature开发完成，首先会合并到develop分支，进入提测，会创建release分支。如果测试过程中若存在bug需要修复，则直接由开发者在release分支修复并提交。当测试完成之后，合并release分支到master和develop分支，此时master为最新代码，用作上线。 hotfix分支 分支命名：hotfix/开头的为修复分支，它的命名规则与feature分支类似 线上出现紧急问题时，需要即使修复，以master分支为基线，创建hotfix分支，修复完成后，需要合并大master和develop分支 常见任务 增加新功能 (dev)$: git checkout -b feature/xxx # 从dev建立特性分支 (feature/xxx)$: blabla # 开发 (feature/xxx)$: git add xxx (feature/xxx)$: git commit -m 'commit comment' (dev)$: git merge feature/xxx --no-ff 修复紧急bug (master)$: git checkout -b hotfix/xxx # 从master建立hotfix分支 (hotfix/xxx)$: blabla # 开发 (hotfix/xxx)$: git add xxx (hotfix/xxx)$: git commit -m 'commit comment' (master)$: git merge hotfix/xxx --no-ff # 把hotfix分支合并到master，并上线到生产环境 (dev)$: git merge hotfix/xxx --no-ff # 把hotfix分支合并到dev，同步代码 测试环境代码 (release)$: git merge dev --no-ff # 把dev分支合并到release，然后在测试环境拉取并测试 生产环境上线 (master)$: git merge release --no-ff # 把release测试好的代码合并到master，运维人员操作 (master)$: git tag -a v0.1 -m '部署包版本名' #给版本命名，打Tag 日志规范 在一个团队协作的项目中，开发人员需要经常提交一些代码去修复bug或者实现新的feature。而项目中的文件和实现什么功能、解决什么问题都会渐渐淡忘，最后需要浪费之间去阅读代码。但是好的日志规范commit messages编写有帮助到我们，它也反映了一个开发人员是否是良好的协作者。 编写良好的commit messages可以达到3个重要的目的： 加快review的流程 帮助我们编写良好的版本发布日志 让之后的维护者了解代码里特定变化和feature被添加的原因 目前，社区有多种commit messages的写法规范。来自Angular规范是目前使用最广的写法，比较合理和系统化。如下图： Commit messages的基本语法 当前业界应用的比较广泛的是 具体格式为： : type: 本次commit的类型，诸如bugfix docs style等 scope：本次commit波及的范围 subject：简明扼要的阐述下本次commit的主旨，在原文中特意强调了几点1.使用祈使句，是不是很熟悉又陌生的一个词，来传送在此祈使句 2.首字母不要大写 3.结尾无需添加标点 body：同样使用祈使句，在主题内容中我们需要把本次commit详细的描述以下，比如此次变更的动机，如需换行，则使用！ footer：描述下与之惯量的issue或break change，相见案例 type的类别说明： feat：添加新特性 fix：修复bug docs：仅仅修改了文档 style：仅仅修改空格、格式缩进、都好等等，不改变代码逻辑 refactor：代码重构，没有加新功能或者修复bug perf: 增加代码进行性能测试 test：增加测试用例 chore：改变构建流程、或者增加依赖库、工具等 Commit messages格式要求 # 标题行：50个字符以内，描述主要变更内容 # # 主体内容：更详细的说明文本，建议72个字符以内。 需要描述的信息包括: # # * 为什么这个变更是必须的? 它可能是用来修复一个bug，增加一个feature，提升性能、可靠性、稳定性等等 # * 他如何解决这个问题? 具体描述解决问题的步骤 # * 是否存在副作用、风险? # # 如果需要的化可以添加一个链接到issue地址或者其它文档 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/Git规范和Changelog生成.html":{"url":"Git/Git规范和Changelog生成.html","title":"Git规范和Changelog生成","keywords":"","body":"Git规范和Changelog生成 良好的Git commit规范优势： 加快Code Review的流程 根据Git Commit的元数据生成Changelog 后续维护者可以知道Feature被修改的原因 技术方案 Git提交格式： 统一团队Git commit日志标准，便于后续代码review和版本发布 使用angular的Git commit日志作为基本规范 提交类型限制为：feature、fix、docs、style、refactor、pref、test、chore、revert等。 提交信息分为两部分，标题（首字母不大写，末尾不要标点）、主题内（正常描述信息即可） 日志提交时友好的类型选择提示 使用commitize工具 不符合要求格式的日志拒绝提交的保障机制 使用validate-commit-msg工具 需要同时在客户端、gitlab server hook做 统一changelog文档信息生成 使用conventional-changelog-cli工具 提交格式要求 (scope): 对格式的说明如下： type：代表某次提交的类型，比如修复一个bug还是增加一个新的feature，所有的type类型如下： feat： 新增feature； fix：修复bug docs：仅仅修改了文档，比如README，CHANGELOG，CONTRIBURTE等等 style：仅仅修改了空格，格式缩进，逗号等等，不改变代码的逻辑 refactor：代码重构，没有价新功能或者修复bug pref：优化相关，比如提升性能、体验 test：测试用例，包括单元测试、集成测试等等 chore： 改变构建流程、或者增加依赖库、工具等 revert：回滚到上一个版本 本地开发阶段增加precommit钩子 安装husky npm install husky --save-dev 通过commitmsg钩子校验信息 \"script\": { \"commitmsg\": \"validate-commit-msg\", \"changelog\": \"conventional-changelog -p angular -i CHANGELOG.md -s -r 0\" }, { \"devDependencies\": { \"validate-commit-msg\": \"^2.11.1\", \"conventional-changelog\": \"^1.2.0\", \"husky\": \"^0.13.1\" } } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/使用AppVeyor和Travis，自动构建和发布Electron应用.html":{"url":"Git/使用AppVeyor和Travis，自动构建和发布Electron应用.html","title":"使用AppVeyor和Travis，自动构建和发布Electron应用","keywords":"","body":" 前沿 Electron应用在开发以后，本地运行build只能打包相对于的环境。如在Mac下运行只能打包dmg不能兼顾其他平台。为了解决这个痛点，就有了这篇文章。 简单说一下构建和发布流程：主要是配置工具elertron-builder，配置Travis一构建Linux和Mac应用，配置appveyor以构建Windows应用，当提交到GitHub代码后，CI自动拉去代码，运行 electron-builder相关命令，生成一个平台的安装包，并将安装包推送到GitHub Release中。 在使用git提交代码后，CI自动构建并发布。 准备条件 Github账户 Appveyor账户 Travis账户 基于Electron-vue脚手架构建的项目（非必须） 项目基于electron-builder打包（非必须） 配置项目 编辑本地的package.json { \"name\": \"electron-v2er\", \"version\": \"0.0.1\", \"author\": \"ruicky \", \"homepage\":\"https://github.com/ruicky/electron-v2ex\", \"description\": \"An electron-vue project\", \"license\": \"MIT\", \"main\": \"./dist/electron/main.js\", \"scripts\": { \"build\": \"node .electron-vue/build.js && electron-builder --publish always\" .. } ... \"mac\": { \"icon\": \"build/icons/v2ex.icns\", \"category\": \"public.app-category.utilities\" }, \"win\": { \"icon\": \"build/icons/v2ex.ico\", \"target\": \"nsis\" }, \"linux\": { \"icon\": \"build/icons\", \"category\": \"Utility\", \"target\": [ \"deb\", \"AppImage\" ] } ... 说明： 其中的name（项目名称）version（项目版本）author（作者信息）description（x项目描述）license（开源协议）都需要填写，否则在编译Linux版本的时候回报错。填写或修改成自己的参数即可。可参考[Metadata]（https://www.electron.build/configuration/configuration#metadata）参数解释 再scripts中的build添加--publish always可参考How to Publish里的参数解释 生成GitHub的GH_TOKEN electron-builder需要GH_TOKEN（GitHub Personal access token）才有权限上传文件到GitHub Release中。可在Personal access tokens页面进行生成，由于只会显示一次，注意保存好。再创建的时候只需要勾选repo->public_repo即可。 配置CI AppVeyor 在本地项目根目录中添加文件appveyor.yml，具体可以参考一下示例配置，几乎不用修改。 version: 0.1.{build} branches: only: - master image: Visual Studio 2017 platform: - x64 cache: - node_modules - '%APPDATA%\\npm-cache' - '%USERPROFILE%\\.electron' - '%USERPROFILE%\\AppData\\Local\\Yarn\\cache' init: - git config --global core.autocrlf input install: - ps: Install-Product node 8 x64 - git reset --hard HEAD - yarn - node --version build_script: #- yarn test - yarn build test: off 创建项目 然后打开Appveyor项目页，点击左边的NEW PROJECT按钮，然后选择你要自动化的仓库。 设置 回到首页选择创建好的项目 点击SettingTab 在Environment填写GH_TOKEN的值，点击页面下面的“保存” 在 Deployment 中配置部署的结果页，选择 GitHub Releases 并填写之前生成好的 GH_TOKEN，点击页面最下面的“保存” Tavis 在项目的根目录创建文件 .travis.yml 然后参考下面的配置写入，大部分不用修改。 # Commented sections below can be used to run tests on the CI server # https://simulatedgreg.gitbooks.io/electron-vue/content/en/testing.html#on-the-subject-of-ci-testing osx_image: xcode8.3 sudo: required dist: trusty language: c matrix: include: - os: osx - os: linux env: CC=clang CXX=clang++ npm_config_clang=1 compiler: clang cache: directories: - node_modules - \"$HOME/.electron\" - \"$HOME/.cache\" addons: apt: packages: - libgnome-keyring-dev - icnsutils #- xvfb before_install: - mkdir -p /tmp/git-lfs && curl -L https://github.com/github/git-lfs/releases/download/v1.2.1/git-lfs-$([ \"$TRAVIS_OS_NAME\" == \"linux\" ] && echo \"linux\" || echo \"darwin\")-amd64-1.2.1.tar.gz | tar -xz -C /tmp/git-lfs --strip-components 1 && /tmp/git-lfs/git-lfs pull - if [[ \"$TRAVIS_OS_NAME\" == \"linux\" ]]; then sudo apt-get install --no-install-recommends -y icnsutils graphicsmagick xz-utils; fi install: #- export DISPLAY=':99.0' #- Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 & - nvm install 9 - curl -o- -L https://yarnpkg.com/install.sh | bash - source ~/.bashrc - npm install -g xvfb-maybe - yarn script: #- xvfb-maybe node_modules/.bin/karma start test/unit/karma.conf.js #- yarn run pack && xvfb-maybe node_modules/.bin/mocha test/e2e - yarn run build branches: only: - master 进入 travis官网， 点击 GitHub 登录 可自动同步项目到 travis 选择要 配置的项目 选择项目右边的 setting 填写 GB_TOEN的值 总结 按照上述的配置，就能够自动化的部署了。在配置的过程中由于是 electron-vue 脚手架生成的项目，在 package.json 中有些节点没有，导致在 build linux 环境的时候报错。所以建议，按照上面的说明都配置上。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/跨平台桌面应用Electron.html":{"url":"Git/跨平台桌面应用Electron.html","title":"跨平台桌面应用Electron","keywords":"","body":"官网 发现一篇不错的文章：Electron构建跨平台应用Mac/Windows/Linux By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Git/企业级分布式，EDAS模式.html":{"url":"Git/企业级分布式，EDAS模式.html","title":"企业级分布式，EDAS模式","keywords":"","body":"企业级分布式，EDAS模式 部署在 ECS 实例后应用仅仅处于「可用」的状态，还面临着如下问题和隐患： 迭代能力不足，无法优雅终止和快速启动应用 —— 每次重新部署都将会造成应用不可用； 健壮性不足，物理或系统出现的问题将可能导致应用不可用； 并发性不足，虽然在网站前期不会是瓶颈，但单机的模式扩容有其上限。 很容易就能想到使用容器化技术和集群模式。 这就可以应用 Docker 和 Kubernetes（以下简称 k8s），阿里云提供了容器服务 Kubernetes 版。此时应用的架构是这样： 首次部署： 创建 k8s 集群，将集群控制权授予 EDAS； 创建镜像仓库，通过发布 git 标签触发构建镜像； 创建 EDAS 应用，关联镜像，部署应用：； 添加负载均衡，允许公网访问 EDAS 应用。 往后迭代： 通过发布 git 标签触发构建镜像； 访问 EDAS 应用，访问新版本镜像进行重新部署。 参考： 什么是 Docker？ 什么是 Kubernetes？ 什么是阿里云容器服务 Kubernetes 版？ 以下是通过 EDAS 部署 NodeBB 的指引教程。 2.1 创建 Kubernetes 集群 首先我们需要一个 k8s 集群用于部署我们的应用。访问创建 Kubernetes 集群： 地域选择“华东 1”； 可用区选择“可用区 G”； 专有网络选择和上一步“ECS 部署”的一致（因为接下来将需要连接 MongoDB）。 参考： 如何创建 Kubernetes 集群？： 如何创建 Kubernetes 托管版集群？ > 相比于默认的 Kubernetes 集群，托管版本会主动替您运维一套高可用的 Master 组件，免去了默认版本集群中三个 Master ECS 节点，从而节约所需的资金成本及维护时的人力成本。-- 《使用 Terraform 创建托管版 Kubernetes》 如何创建多可用区 Kubernetes 集群？ > 为了保证业务应用的高可用，有些客户会要求关键应用部署到多个机房，一个机房一旦出问题，其他机房正常工作，从而让应用保持不间断连续运行。阿里云支持多 Region（地域），每个 Region 下又有不同的可用区。可用区是指在同一地域内，电力和网络互相独立的物理区域。多可用区能够实现跨区域的容灾能力。同时也会带来额外的网络延时。 2.2 创建 Docker 镜像 紧接着，我们需要构建出应用的 Docker 镜像。访问容器镜像服务控制台： 创建镜像仓库： 代码仓库选择上一步“ECS 部署”中的仓库； 勾选“代码变更时自动构建镜像”。 构建镜像： 在代码仓库的根目录下创建配置文件 config.json： { \"url\": \"http://xxx.taobao.com\", \"secret\": \"xxx\", \"database\": \"mongo\", \"mongo\": { \"uri\": \"mongodb://xxxx\" }, \"port\": \"4567\" } Dockerfile 中使用配置文件启动 NodeBB： FROM node:8.15.0 RUN mkdir -p /usr/src/app WORKDIR /usr/src/app ARG NODE_ENV ENV NODE_ENV $NODE_ENV COPY install/package.json /usr/src/app/package.json RUN npm install && npm cache clean --force COPY . /usr/src/app ENV NODE_ENV=production \\ daemon=false \\ silent=false RUN ./nobebb build CMD ./nodebb start EXPOSE 4567 通过发布标签触发镜像构建：release-v$version 2.3 创建 EDAS 应用 然后我们创建一个 EDAS 应用，并使用刚创建的镜像部署该应用。访问 EDAS 控制台： 访问“EDAS - 资源管理 - 集群”：导入刚创建的 K8S 集群； 访问“EDAS - 应用管理 - 应用列表”：创建新应用： 集群类型：选择“容器服务K8S集群” - 选择刚导入的 K8S 集群； 镜像：选择上一步构建的镜像和版本。 配置应用： 添加负载均衡（公网）： 通过负载均衡IP验证应用部署是否成功。 2.4 配置负载均衡 最后，我们在负载均衡层强制启用HTTPS，并将域名解析道负载均衡的公网IP。 访问负载均衡控制台操作： 创建证书： 访问“负载均衡-实例-证书管理” 点击创建证书 选择“上传第三方签发证书” 上传“ECS部署”章节中下载的证书 负载均衡配置 通过IP找到对应的负载均衡实例 添加HTTPS监听（参考《如何添加HTTPS监听》） 协议选择HTTPS 监听端口输入“443” 高级配置中，开启会话保持 SSL证书选择上一步创建的证书 后端服务器选择“虚拟服务器组” 删除原TCP 80的监听； 添加HTTP监听 协议选择“HTTP” 监听端口输入“80” 高级配置开启“监听转发” 目的监听选择“HTTPS 443” 通过HTTPS访问IP验证负载均衡配置成功； 域名映射：通过IDNS将xxx.taobao.comA到负载均衡公网IP By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Linux/":{"url":"Linux/","title":"Linux","keywords":"","body":"Linux Linux上部署Node服务-外网无法访问 服务器中启动服务的时候的IP选择 怎样修改CentOS 7 SSH端口 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Linux/Linux上部署Node服务-外网无法访问.html":{"url":"Linux/Linux上部署Node服务-外网无法访问.html","title":"Linux上部署Node服务-外网无法访问","keywords":"","body":"最近在一台新的Centos 7服务器上部署了Node服务，服务启动成功后，在Mac上访问这个网站，无法访问 解决思路： linux上的Node服务是否正常运行，端口是否正常监听 查看Linux自己的IP地址 mac是否ping通这台Linux Linux是否开启了防火墙 外面电脑telnet这个Linux，看这个node服务的端口是否连接上 查看Linux自己的IP地址 ifconfig 外面电脑是否能ping通这台Linux ping 192.168.1.1 Linux上的node.js的网站是否正常运行，端口是否正常监听 curl localhost:8072 Linux 是否开启了防火墙 systemctl status firewalld 将Linux启动时，防火墙策略改为关闭 systemctl disable firewalld 将当前系统的防火墙服务停止 systemctl stop firewalld 外面电脑telnet这个Linux，看这个node的端口是否连的上 telnet ip port 在进行上面调整之后，测试还是无法访问，继续在网上查找相关资料 将Linux上的127.0.0.1改为 0.0.0.0，外网就可以访问了 下一片文章会讲到 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Linux/服务器中启动服务的时候的IP选择.html":{"url":"Linux/服务器中启动服务的时候的IP选择.html","title":"服务器中启动服务的时候的IP选择","keywords":"","body":"场景 在centos的服务器中，启动了一个node服务，对于IP选择，之前时127.0.0.1，结果外网无法访问，改用0.0.0.0 外网就可以访问了，要去搞清楚两者的区别 127.0.0.1 VS 0.0.0.0 127.0.0.1是回环地址 0.0.0.0是一个不可路由的元地址，用于指定无效，未知或不使用的目标（‘无特定地址’占位符） 在路由目的的上下问中它通常表示默认路由 在服务器的上下文中，0.0.0.0考试本地计算机上的所有IPv4地址。如果主机有两个IP地址192.168.1.1和10.1.2.1，并且服务器上运行的服务器侦听0.0.0.0，则可以在这两个Ip上访问它。 什么是IP地址127.0.0.1？ 127.0.0.1是回环Internet协议（IP）地址，也称为localhost。该地址用于建立与最终用户使用的同一台机器或计算机的IP连接 对于使用:: 1的内涵支持IPv6寻址的计算机，定义了相同的约定。使用地址127.0.01建立连接是最常见的做法；但是看，使用127 ...*范围内的任何IP地址将以相同或相似的方式运行。会还构造使计算机或设备能够联网在机器上验证火箭里IP栈的能力。 特别地址 A类网络号127被分配了换回功能，即，由更高级协议发送到网络127地址的数据报文应该在主机内回送。发送到网络127地址的数据报文不应该出现在任何网络上的任何地方 如果是全A级，那么最后三个八位的其他任意值的重点是什么？ 回环范围的目的是测试主机上的TCP/IP协议实现。由于较低层是短路的，因此发送到回环地址允许有效地测试叫高层（IP和以上），而不会在较低层出现问题。 127.0.0.1是最常用于测试目的地地址 什么是IP地址0.0.0.0？ 0.0.0.0是有效的地址语法。因此，无论在传统的点分十进制表示法中使用IP地址，它都应该解析为有效。解析并转换为可操作的数字形式后，其值将决定接下来会发生什么 全零值确实具有特殊含义。因此它是有效的，但对于特定情况，其含义可能不合适（因此被视为无效）。它基本上是'无特定地址'的占位符。对于诸如网络连接的地址绑定之类的事情，结果可以是为连接分配适当的接口地址。如果您使用它来配置接口，则可以从接口中删除地址。这取决于使用的上下文来确定“没有特定地址”真正做什么。 在路由条目的上下文中，它通常表示默认路由。结果发生了更多的地址掩码，它选择要比较的位。掩码0.0.0.0不选择任何位，因此比较将始终成功。因此，当配置这样的路由时，总会有一些数据包要去（如果配置了有效的目的地）。 在某些情况下，仅“0”也可以起作用并具有相同的效果。但这不能保证。0.0.0.0表单是说“没有特定地址”的标准方式（在IPv6中是:: 0或只是::）。 在Internet协议版本4中，地址0.0.0.0是一个不可路由的元地址，用于指定无效，未知或不适用的目标。对于无效的数据片段赋予特殊含义的是带内信令的应用。 在服务器的上下文中，0.0.0.0表示本地计算机上的所有IPv4地址。如果主机有两个IP地址，192.168.1.1和10.1.2.1，并且主机上运行的服务器侦听0.0.0.0，则可以在这两个IP上访问它 在路由环境中，0.0.0.0通常表示默认路由，即通向Internet的“其余部分”而不是本地网络某处的路由。 用途包括： 主机在尚未分配地址时声明为自己的地址。例如在使用DHCP时发送初始DHCPDISCOVER数据包。 如果主机的IP堆栈支持，则当通过DHCP的地址请求失败时，主机为自己分配的地址。在现代操作系统中，这种用法已被替换为APIPA机制。 一种指定任何IPv4主机的方法。在指定默认路由时以此方式使用它。 一种明确指定目标不可用的方法。资料来源： 127.0.0.1 - 它的用途是什么，为什么重要？ 一种指定任何IPv4地址的方法。在配置服务器时（即绑定侦听套接字时）以此方式使用它。TCP程序员将其称为INADDR_ANY。[ bind（2）绑定到地址，而不是接口。] 在IPv6中，全零地址写为:: 总结 127.0.0.1: 是个（特殊的）IP地址，往往被分配给了loopback或仅局域网可以访问的接口local-only interface 这是一个伪造的，假的，网络适配器，其只能与同主机host内通信 常用于：让一个可以支持网络的程序，仅仅响应与同主机host内的客户端 一个程序侦听127.0.0.1的话，则只能接受来自本地的访问 localhost：往往是127.0.0.1这个IP地址的主机名hostname Linux中是/etc/hosts设置的 试试ping localhost，则回输出：127.0.0.1 0.0.0.0: 本身包含很多方面的含义，但是此处指的是： 当一个服务器监听这个IP地址的话，意味着：监听所有的网络请求 （对应这IP地址为127.0.0.1的）loopback这个适配器adapter的请求，就像其他本机中的其他的网络适配器一样。 127.0.0.1: loopback地址，和localhost一样， localhost: 是127.0.0.1的主机名 0.0.0.0: 是个不可被路由的元地址，用于指定无效的，未知的，不可使用的目标，相当于： 没有特定的IP地址的占位符 对于路由入口来说：往往指的是默认的路由 对于服务器来说：意味着本机中的所有IPv4的地址 如果一个服务器有两个IP地址： 192.168.1.1和10.1.2.1，则本机中的一个服务监听0.0.0.0的话，则两个IP地址都可以访问该服务 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Linux/怎样修改CentOS7SSH端口.html":{"url":"Linux/怎样修改CentOS7SSH端口.html","title":"怎样修改CentOS 7 SSH端口","keywords":"","body":"场景 一般centos默认的ssh登陆端口是22，很容易受到攻击登陆，为了安全型考虑，最好修改一下 修改sshd_config端口 vi /etc/ssh/sshd_config 取消#PORT 22的注释，在下一行添加你需要修改的新端口Port 2048。（这里不删除22端口是为了防止修改后新端口无法访问，造成无法用 ssh 连接服务器）。 Port 22 Port 2048 修改保存sshd_config文件后重启sshd服务 systemctl restart ssh 退出ssh会话后，再用新的端口连接： $ ssh -p 2048 root@example.com ssh: conntect to host 0.0.0.0 port 2048: Connection refused 好吧，native了...对于CentOS 7这一套修改端口的方法已经不能生效了。 打开SELinux端口 SELinux全程Security Enhanced Linux(安全强化Linux)，是MAC（Mandatory Access Control，强制访问控制系统）的一个实现，目的在于明确的知名某个进程可以访问那些资源（文件、网络端口等） 对于ssh，SELinux默认只允许22端口，我们可以用SELinux管理配置工具semanage，来修改ssh可访问的端口 安装semanage工具 $ yum provides semanage $ yum -y install policycoreutils-python 为ssh打开2048端口 # 为 ssh 添加新的允许端口 $ semanage port -a -t ssh_port_t -p tcp 2048 # 查看当前 SELinux 允许的端口 $ semanage port -l | grep ssh ssh_port_t tcp 2048, 22 错误处理 当SELINUX配置为禁用状态时，使用semanage回报错提示无法读取policy文件： SELinux: Could not downgrade policy file /etc/selinux/targeted/policy/policy.30, searching for an older version. SELinux: Could not open policy file 修改/etc/selinux/config $ vi /etc/selinux/config SELINUX=permissive # 重启服务器 $ init 6 # 重启后查看 SELinux 状态 $ sestatus # if it shows disable, you can run $ load_policy -qi 检查配置 $ semanage port -a -t ssh_port_t -p tcp 2048 $ semanage port -l | grep ssh ssh_port_t tcp 2048, 22 # 重启 ssh 服务 systemctl restart sshd 注：semanage不能禁用ssh的22端口 $ semanage port -d -t ssh_port_t -p tcp 22 ValueError: 在策略中定义了端口 tcp/22，无法删除。 配置防火墙firewalld 启用防火墙 && 产看防火墙状态： $ systemctl enable firewalld $ systemctl start firewalld $ systemctl status firewalld ● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled) Active: active (running) since 二 2016-12-20 02:12:59 CST; 1 day 13h ago Main PID: 10379 (firewalld) CGroup: /system.slice/firewalld.service └─10379 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid $ firewall-cmd --state running 查看防火墙当前默认和激活zone： $ firewall-cmd --get-default-zone public $ firewall-cmd --get-active-zones public interfaces: eth0 eth1 若没有激活区域的话，要执行下面的命令 激活public区域，增加网卡接口 $ firewall-cmd --set-default-zone=public $ firewall-cmd --zone=public --add-interface=eth0 success $ firewall-cmd --zone=public --add-interface=eth1 success 为public zone永久开放2048/TCP端口 # 以防新端口不生效，先把 22 端口暴露 $ firewall-cmd --permanent --zone=public --add-port=22/tcp $ firewall-cmd --permanent --zone=public --add-port=2048/tcp success # 重载防火墙 $ firewall-cmd --reload # 查看暴露端口规则 $ firewall-cmd --permanent --list-port 443/tcp 80/tcp 22/tcp 2048/tcp $ firewall-cmd --zone=public --list-all public (default, active) interfaces: eth0 eth1 sources: services: dhcpv6-client ssh ports: 443/tcp 80/tcp 22/tcp 2048/tcp masquerade: no forward-ports: icmp-blocks: rich rules: 退出ssh后，尝试连接新端口 $ ssh -p 2048 root@example.com 成功登陆的话，就可以做收尾工作了。 禁用22端口 删除ssh允许端口 $ vi /etc/ssh/sshd_config #Port 22 Port 2048 $ systemctl restart sshd # 用 ss 命令检查 ssh 监听的端口，没有 22 证明修改成功 $ ss -tnlp | grep ssh LISTEN 0 128 *:2048 *:* users:((\"sshd\",18233,3)) 防火墙移除22端口 $ firewall-cmd --permanent --zone=public --remove-port=22/tcp success $ firewall-cmd --reload $ firewall-cmd --permanent --list-port 443/tcp 80/tcp 2048/tcp ssh取消监听22端口，就已经配置好了，防火墙只不过是在ssh外多一层访问控制。如果要做的更好还可以将22端口的访问流量专项访问者本地： $ firewall-cmd --permanen --zone=public --add-forward-port=port=22:proto=tcp:toport=22:toaddr=127.0.0.1 # 配置后重载防火墙，用 ssh -p 22 root@example.com 就会访问到自己本地的 22 端口。 若要删除forward配置，可以： $ firewall-cmd --permanen --zone=public --remove-forward-port=port=22:proto=tcp:toport=22:toaddr=127.0.0.1 检验修改ssh端口是否成功： $ ssh -p 22 root@example.com # 无响应，因为转到了本地的 22 端口 # 若防火墙未 forward 连接，则会回显 \"ssh: connect to host {ip} port 22: Connection refused\" $ ssh -p 2048 root@example.com # 成功 success By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/":{"url":"Design/","title":"设计模式","keywords":"","body":"设计模式 javaScript设计模式笔记一 javaScript设计模式笔记二 javaScript设计模式笔记三 javaScript设计模式笔记四 javascript设计模式笔记五 javaScript设计模式笔记六 javaScript设计模式笔记七 javaScript设计模式笔记八 javaScript设计模式笔记九 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记一.html":{"url":"Design/javaScript设计模式笔记一.html","title":"javaScript设计模式笔记一","keywords":"","body":"javaScript单例模式 在GoF（Gang of Four）的书中提出的设计模式为面向对象的软件设计中遇到的一些普遍问题提供了解决方案。它们已经诞生很久了，而且被证实在很多情况下是很有效的。这正是你需要熟悉它的原因，也是我们要讨它的原因。 尽管这些设计模式跟语言和具体的实现方式无关，但他们多年来被关注到的方面仍然主要是强类型静态语言比如C++和Java中的应用。 JavaScript作为一种基于原型的弱类型动态语言，是的有些时候实现某些模式时相当简单，甚至不菲吹灰之力。 让我们从第一个例子--单例模式--来看一下在JavaScript中和静态的基于类的语言有什么不同。 单例 单例模式的核心思想是让指定的类只存在 唯一一个实例。这意味着当你第二次 使用相同的类的去创建对象的时候，你得到的应该和第一次创建的是同一个对象。 这如何应用到JavaScript中呢？在JavaScript中没有类，只有对象。当你创建一个对象时，事实上根本没有另一个对象和它一样，这个对象其实已经是一个单例。使用对象字面量创建一个简单的对象也是一种单例的例子： const obj = { myprop: 'my value' } 在JavaScript中，对象永远不会相等，除非它们是同一个对象，所以即使你创建一个看起来完全一样的对象，它也不会和前面的对象相等： const obj2 = { myprop: 'my value' } obj === obj2; // false obj == obj2; // false 所以你可以说当你每次使用对象字面量创建一个对象的时候就是在创建 一个单例，并没有特别的预发牵涉进来。 需要注意的是，有的时候当人们在JavaScript中提出“单例”的时候，它们可能是指第五章讨论的“模块模式”。 使用new JavaScript没有类，所以一字一句地说单例的定义并没有什么意义。但是 JavaScript有使用new、通过构造函数来创建对象的语法，有时候你可能需要这种语法下的一个单例实现。这也就是说当你使用new、通过同一个构造函数来创建多个对象的时候，你应该只是得到同一个对象的不同引用。 温馨提示：从一个使用模式的角度来说，下面的讨论并不是那么有用，只是更多地在实践模拟一些语言中关于这个模式的一些问题的解决方案。这些语言主要是（静态强类型的）基于类的语言，在这些语言汇总，函数并不是“一等公民”。 下面的代码片段展示了期望的结果（假设你忽略了多元宇宙的设想，接受了只有一个宇宙的观点）： const uni = new Universe(); const uni2 = new Universe(); uni === uni2; // true 在这个例子中，uni只在构造函数第一次被调用时创建。第二次（以及后续更多次）调用时，同一个uni对象被返回。这就是为什么uni === uni2的原因--因为它们实际上是同一个对象的两个引用。那么怎么在JavaScript达到这个效果呢？ 当对象实例this被创建时，你需要在Universe构造函数中缓存它，以便在第二次调用的时候返回。有几种选择可以达到这种效果： 你可以使用一个全局变量来存储实例。不推荐使用这种方法，因为通常我们认为是易用全局变量是不好的。而且，任何人都可以改写全局变量的值，甚至可能是无意中改写。所以我们不再讨论这种方案。 你也可以将对象实例缓存在构造函数的属性中。在JavaScript中，函数也是对象，所以它们也可以有属性。你可以写一些类似Universe。instance的属性来缓存对象。这是一种漂亮干净的解决方案，不足之处是instance属性仍然是可以被公开访问的，别人写代码可能修改它，这样就会失去这个实例。 你可以将实例包裹在闭包中。这可以保持实例是私有的，不会在构造函数之外被修改，代价是一个额外的闭包。 让我们来看一下第二种和第三种方案的实现示例。 将实例放到静态属性中 下面是将唯一的实例放入Universe构造函数的一个静态属性中的例子： function Universe() { // do we hanve an existing instance? if (typeof Universe.instance === 'object') { return Universe.instance; } // proceed as normal this.start_time = 0; this.bang = \"Big\"; // cache Universe.instance = this; // implicit return; // return this; } // testing const uni = new Universe(); const uni2 = new Universe(); uni === uni2; // true 如你所见，这是一种直接有效的解决方案，唯一的缺陷是instance是可以被公开访问的。一般来说它被其他代码误删改的可能是很小的（起码比全局变量instance要小得多），但是仍然是有可能的。 将实例放到闭包中 另一种实现基于类的单例模式的方法是使用一个闭包来保护这个唯一的实例。你可以通过第五章讨论过的”私有静态成员模式“来实现。唯一的秘密就是重写构造函数： function Universe() { // the chched instance const instance = this; // proceed as normal this.start_time = 0; this.bang = \"Big\"; // rewrite the constructor Universe = function () { return instance; }; } // testing const uni = new Universe(); const uni2 = new Universe(); uni === uni2; // true 第一次调用时，原始的构造函数被调用并且正常返回this。在后续的调用中，被重写的构造函数被调用。被重写怕这个构造函数可以通过闭包访问私有的instance变量并且将它返回。 这个实例实际上也是第四章讨论的自定义函数的又一个例子。如我们讨论过的一样，这种模式的缺点是被重写的函数（在这个例子中就是构造函数Universe（））将丢失那些在出事定义和重新定义之间添加的属性。在这个例子中，任何添加到Universe()的原型上的属性将不会被链接到使用原来的实现创建的实例上（注：这里的“原来的实现”是指实例是由未被重写的构造函数创建的，而Universe()则是被重写的构造函数。） 下面我们通过一些测试来展示这个问题： // adding to the prototype Universe.prototype.noting = true; const uni = new Universe(); // again adding to the prototype // after the initial object is created Universe.prototype.everything = true; const uni2 = new Universe(); // testing // only the original prototype was linkes to the objects uni.noting; // true; uni2.noting; // true, uni.everything; // undefined uni2.everything; // undefined // that sounds right: uni.constructor.name; // \"Universe\" // but that's odd: uni.constructor === Universe; // false uni.constructor不再和Universe（）相同的原因是uni.constructor仍然是指向原来的构造函数，而不是被重新定义的那个。 如果一定被要求让prototype和constructor的指向像我们期望的那样，可以通过一些调整来做到： function Universe() { // the cached instance let instance; // rewrite the constructor Universe = function Universe() { return instance; } // carry over the prototype properties Universe.prototype = this; // the instance instance = new Universe(); // reset the constuctor pointer instance.constuctor = Universe; // all the functionality instance.start_time = 0; instance.bang = \"Big\"; return instance; } 现在所有的测试结果都可以像我们期望的那样了： // update prototype and create instance Universe.prototype.noting = true; // true const uni = new Universe(); Universe.prototype.everything = true; // true cosnt uni2 = new Universe(); // it's the same single instance uni === uni2; // true // all prototype properties work // no matter when they were defined uni.noting && uni.everything && uni2.noting && uni2.everything; // true // the normal properties work uni.bang; // \"Big\" // the constuctor points correctly uni.constuctor === Universe; // true 另一种可选的解决发难是将构造函数和实例包在一个立即执行的函数中。当构造函数第一次被调用的时候，它返回一个对象并且将私有的instance指向它。在后续调用时，构造函数是指简单地返回这个私有变量。在这种新的实现下，前面所有的测试代码也会和期望的一样： let Universe (function () { let instance; Universe = function Universe() { if (instance) { return instance; } instance = this; // all the functionality this.start_time = 0; this.bang = \"Big\"; }; }()); By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记二.html":{"url":"Design/javaScript设计模式笔记二.html","title":"javaScript设计模式笔记二","keywords":"","body":"JavaScript设计模式-工厂模式 使用工厂模式的目的就是创建对象。它通常被在类或者类的静态方法中实现，目的是： 执行在建立相似的对象时进行的一些重复的操作 让工厂的使用者在编译阶段创建对象时不必知道它的特定类型（类） 第二点在静态的基于类的语言中更重要，因为在（编译阶段）提前不知道类的情况下，创建类的实例是不普遍的行为。但在JavaScript中，这部分的实现却相当容易。 使用工厂的方法（或类）创建的对象被设计为从同一个副对象继承；它们是特定的实现一些特殊功能的子类。有些时候这个共同的父对象就是包含工厂方法的同一个类。 我们来看一个示例实现，我们有： 一个共同的父构造函数CarMaker。 CarMaker的一个静态方法叫factory()， 用来创建car对象。 特定的从CarMaker继承而来的构造函数CarMaker.Compact，CarMaker.SUV，CarMaker.Convertible。它们都被定义为父构造函数的静态属性以便保持全局空间干净，同时在需要的时候我们也知道在哪里找到它们。 我们来看一下已经完成的实现会怎么被使用： const corolla = CarMaker.factory('Compact'); const solstice = CarMaker.factory('Convertile'); const cherokee = CarMaker.factory('SUV'); corolla.drive(); // \"Vroom, I have 4 doors\" solstice.drive(); // \"Vroom, I have 2 doors\" cherokee.drive(); // \"Vroom, I have 17 doors\" 这一段： const corolla = CarMaker.factory('Compact'); 可能是工厂模式中知名的。你有一个方法可以在运行时接收一个表示类型的字符串，然后它创建并返回一个和请求的类型一样的对象。这里没有使用new的构造函数，也没有看到任何对象字面量，仅仅只有一个函数根据一个字符串指定的类型创建了对象。 这里是一个工厂模式的示例实现，它能让上面的代码片段工作： // parent constructor function CarMaker() {} // a method of the parent CarMaker.prototype.drive = function() { return \"Vroom, I have \" + this.doors + \" doors\"; }; // the static factory method CarMaker.factory = function (type) { var constr = type, newcar; // error if the constructor doesn't exist if (typeof CarMaker[constr] !== \"function\") { throw { name: \"Error\", message: constr + \" doesn't exist\" }; } // at this point the constructor is known to exist // let's have it inherit the parent but only once if (typeof CarMaker[constr].prototype.drive !== \"function\") { CarMaker[constr].prototype = new CarMaker(); } // create a new instance newcar = new CarMaker[constr](); // optionally call some methods and then return... return newcar; }; // define specific car makers CarMaker.Compact = function () { this.door = 4; } CarMaker.Convertible = function () { this.door = 2; } CarMaker.SUV = function () { this.door = 24; } 工厂模式的实现中没有什么是特别困难的。你需要做的仅仅是寻找请求类型的对象的构造函数。在这个例子中，使用了一个简单的名字转换以便映射对象类型和创建对象的构造函数。继承的部分只是一个公共的重复代码片段的示例，它可以被放到工厂方法中而不是被每个构造函数的类习惯重复。 内置对象工厂 作为一个“野生的工厂”的例子，我们来看一下内置的全局构造函数Object（）。它的行为很像工厂，因为它根据不同的输入创建不同的对象。如果传入一个数字，它会使用Number（）构造函数创建一个对象。在传入字符串和布尔值的时候也会发生同样的事情。任何其他的值（包括空值）将会创建一个正常的对象。 下面是这种行为的例子和测试。注意Object调用时可以不用加new： const o = new Object(), n = new Object(1), s = Object('1'), b = Object(true); // test o.constructor === Object; // true n.constructor === Number; // true s.constructor === String; // true b.constructor === Boolean; // true Object()也是一个工厂这一事实可能没有太多实际用处，仅仅是觉得值得作为一个例子提一下，告诉我们工厂模式是随处可见的。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记三.html":{"url":"Design/javaScript设计模式笔记三.html","title":"javaScript设计模式笔记三","keywords":"","body":"JavaScript设计模式- 迭代器 在迭代器模式中，你有一些含有有序聚合数据的对象，这些数据可能在内部用一种复杂的结构存储着，但是你希望提供一种简单的方法来访问这种结构中的每个元素。数据的使用者不需要知道你是怎样组织你的数据的，他们只需要操作一个个独立的元素。 在迭代器模式中，你的对象需要提供一个next()方法。按顺序调用next（）方法必须返回序列中的下一个元素，但是“下一个”在你的特定的数据结构中指什么是由你自己决定的。 假设你的对象叫agg，你可以通过简单地在循环中调用next（）来访问每个数据元素，像这样： const element; while(element = agg.next()) { // do someting with the element... console.log(element); } 在迭代器模式中，聚合对象通常也会提供一个方便的方法hasNext()，这样对象的使用者就可以知道他们已经获取到你数据的最后一个元素。当使用另一种方法--hasNext()--来按顺序访问所有元素时，是像这样的： while(agg.hasNext()) { // do something with the next element... console.log(agg.next()); } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记四.html":{"url":"Design/javaScript设计模式笔记四.html","title":"javaScript设计模式笔记四","keywords":"","body":"JavaScript设计模式-装饰器 在装饰器模式中，一些额外的功能可以在运行时被动态地添加到一个对象中。在静态的基于类的语言中，处理这个问题可能是个挑战，但是在JavaScript中，对象本来就是可变的，所以给一个对象添加额外的功能本身并不是什么问题 装饰器模式的一个很方便的特性是可以对我们需要的特性进行定制和配置。刚开始的时，我们由一个拥有基本功能的对象，然后可以从可用的装饰器中去挑选一些需要用到的去增加这个对象，甚至如果顺序很重要的化，还可以指定增强的顺序。 用法 我们来看一下这个模式的示例用法。假设你正在做一个买东西的web应用，每个新交易是一个新的sale对象。这个对象“知道”交易的价格并且可以通过调用sale.getPrice（）方法返回。根据环境的不同，你可以开始用一些额外的功能来装饰这个对象。假设一个场景是这笔交易是发生在加拿大的一个省Quebec，在这种情况下，购买者需要付联邦税和Quebec省税。根据装饰器模式的用法，你需要指明使用联邦税装饰器和Quebec省税装饰器来装饰这个对象。然后你还可以给这个对象装饰一些价格式的功能。这个场景的使用凡事可能是这样： const sale = new Sale(100); // the price is 100 dollars sale = sale.decorate('fedtax); // add federal tax sale = sale.decorate('quebec'); // add provincial tax sale = sale.decorate('money'); // format like money sale.getPrice(); // \"$112.88\" 在另一种场景下，购买者在一个不需要交省税的省，并且你想用加拿大元的格式来显示价格，你可以这样做： const sale = new Sale(100); // the price is 100 dollars sale = sale.decorate('fedtax'); // add federal tax sale = sale.decorate('cdn'); // format using CDN sale.getPrice(); // \"CDN$105.00\" 如你所见，这是一种在运行时很灵活的方法来添加功能和调整对象。我们来看一下如何来实现这种模式。 实现 一种实现装饰器模式的方法是让每个装饰器成为一个拥有应该被重写的方法的对象。每个装饰器实际上是继承自己已经被前一个装饰器增强过的对象。装饰器的每个方法都会调用父对象（继承自的对象）的同名方法并取得值，然后做一些额外的处理。 最终的效果就是当你在第一个例子中调用sale.getPrice()时，实际上是在调用money装饰器的方法 但是因为每个装饰器会先调用父对象的方法，money的get Price()先调用quebec的getPrice()，而它又会去调用fedtax的getPrice()方法，以此类推。这个链会一直走到原始的未经装饰的由Sale()构造函数实现的getPrice(). 这个实现可以以一个构造函数和一个原型方法开始： function Sale(price) { this.price = price || 100; } Sale.prototype.getPrice = function() { return this.price; } 装饰器对象将被作为构造函数的属性实现： Sale.decorators = {}; 我们来看一个装饰器的例子。这是一个对象，实现了一个自定义的getPrice()方法。注意这个方法首先从父对象的方法中取值然后修改这个值： Sale.decorators.festax = { getPrice: function() { let price = this.uber.getPrice(); price += price * 5 /100; return price; } }; 使用类似的方法我们可以实现任意多个需要的其他装饰器。他们的实现方式像插件一样来扩展核心的Sale（）的功能。他们甚至可以被放到额外的文件中，被第三方的开发者来开发和共享： Sale.decorators.quebec = { getPrice: function () { var price = this.uber.getPrice(); price += price * 7.5 / 100; return price; } }; Sale.decorators.money = { getPrice: function () { return \"$\" + this.uber.getPrice().toFixed(2); } }; Sale.decorators.cdn = { getPrice: function () { return \"CDN$ \" + this.uber.getPrice().toFixed(2); } }; 最后我们来看decorate()这个神奇的方法，它把所有上面说的片段都串起来了。记得它是这样被调用的： sale = sale.decorate('fedtax'); 字符串‘fedtax’对应在Sale.decorators.fedtax中实现的对象。被装饰过的最新的对象newobj将从现有的对象（也就是this对象，它要么是原始的对象，要么是经过最后一个装饰器装饰过的对象）中继承。实现这一部分需要用到前面张杰中提到的临时构造函数模式，我们也设置一个uber属性给newObj以便自对象可以访问到父对象。然后我们从装饰器种肤质所有额外的属性到被装饰的对象newobj中。最后，在我们的例子中，newobj被返回并且成为被更新过的sale对象。 Sale.prototype.decorate = function(decorator) { let F = function() {}, overrides = this.constructor.decorators[decorator], i, newobj; F.prototype = this; newobj = new F(); newobj.uber = F.prototype; for (i in overrides) { if (overrides.hasOwnProperty(i)) { newobj[i] = overrides[i]; } } return newobj; }; 使用列表实现 我们来看另一个明显不同的实现方法，受益于JavaScript的动态特性，它完全不需要使用继承，。同时我们也可以简单地将前一个方法的结果作为参数传给下一个方法，而不需要每一个方法都去调用前一个方法。 这样的实现方法还允许很容易地反装饰（undecorating）或者撤销一个装饰，这仅仅需要从一个装饰器列表中移除一个条目。 用法示例也会迷宫内出现简单一些，因为我们不需要将decorate（）的返回值赋值给对象。在这个实现中，decorator不对对象做任何事情，它只是简单地将装饰器加入到一个列表中： var sale = new Sale(100); // the price is 100 dollars sale.decorate('fedtax'); // add federal tax sale.decorate('quebec'); // add provincial tax sale.decorate('money'); // format like money sale.getPrice(); // \"$112.88\" Sale()构造函数现在有了一个作为自己属性的装饰器列表： function Sale(price) { this.price = (price > 0) || 100; this.decorators_list = []; } 可用的装饰器荏苒被实现为Sale.decorators的属性。注意getPrice()方法现在更简单了，因为他们不需要调用父对象的getPrice()来获取结果，结果已经作为参数传递给它们了： Sale.decorators = {}; Sale.decorators.fedtax = { getPrice: function (price) { return price + price * 5 / 100; } }; Sale.decorators.quebec = { getPrice: function (price) { return price + price * 7.5 / 100; } }; Sale.decorators.money = { getPrice: function (price) { return \"$\" + price.toFixed(2); } }; 最有趣的部分发生在父对象的decorate()会让getPrice()。在前一种实现方式中，decorate()还是多少有些复杂，而getPrice()十分简单。在这种实现方式中事情反过来了： decorate()只需要往列表中添加条目而getPrice()做了所有的工作。这些工作包括遍历现在添加的装饰器的列表，然后调用他们的getPrice()方法，并将结果传递给前一个： Sale.prototype.decorate = function (decorator) { this.decorators_list.push(decorator); }; Sale.prototype.getPrice = function () { var price = this.price, i, max = this.decorators_list.length, name; for (i = 0; i 装饰器模式的第二种实现方式更简单一点，并且没有引入进程。装饰的方法也会简单。所有的工作都由“同意”被装饰的方法来做。在这个示例实现中，getPrice()是唯一被允许装饰的方法。如果你想有更多可以被装饰的方法，那遍历装饰器列表的工作就需要由每个方法重复去做。但是，这可以很容易地被抽象到一个辅助的方法中，给它穿一个方法然后使这个方法“可被装饰”。如果这样实现的化，decorators_list属性就应该是一个对象，它的属性名字是方法名，值是装饰器对象的数组。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javascript设计模式笔记五.html":{"url":"Design/javascript设计模式笔记五.html","title":"javascript设计模式笔记五","keywords":"","body":"JavaScript设计模式-策略模式 策略模式允许在运行的时候选择算法。你的代码的使用者可以在处理特定任务的时候根据即将要做的事情的上下文来从一些可用的算法中选择一个。 使用策略模式的一个例子是解决表单验证的问题。你可以创建 一个validator对象，又一个validate()方法。这个方法被调用时不用区分具体的表单类型，它总是会返回同样的结果--一个 但是根据具体的徐阿哟验证的表单和数据，你代码的使用者可以选择进行不同类别的检查。你的validator选择 最佳的策略来出来这个任务，然后将具体的数据检查工作交给合适的算法去做。 数据验证示例 假设你有一个下面这样的数据，它可能来自页面上的一个表单，你希望验证它是不是有效的数据： const data = { first_name: \"Super\", last_name: \"Man\", age: \"unknown\", username: \"o_O\" }; 对这个例子中的validator，它需要知道哪个是最佳策略，因此你需要先配置它，给它设定好规则以确定哪些是有效的数据/ 假设你不需要姓，名字可以接收任何内容，但要求年龄是一个数字，并且用户名只允许包含字母和数字，配置可能是这样的： validator.config = { first_name: 'isNonEmpty', age: 'isNumber', userName: 'isAlphaNum' } 现在validator对象已经有了用来处理数据的配置，你可以调用validate（）方法，然后将任何验证错误打印到控制台上。 validator.validate(data); if (validator.hasError()) { console.log(validator.messages.join(\"\\n\")); } 它可能会打印出这样的信息： Invalid value for *age*, the value can only be a valid number, e.g. 1, 3.14 or 2010 Invalid value for *username*, the value can only contain characters and numbers, no special symbols 现在我们来看一下这个validator是如何实现的。所有可用的用来检查的逻辑都是拥有一个validate（）方法的对象，他们还有一行辅助信息用来显示错误信息： // checks for non-empty values validator.types.isNonEmpty = { validate: function(value) { return value !== \"\"; }, instructions: \"the value cannot be empty\" }; // checks if a value is a number validator.types.isNumber = { validate: function (value) { return !isNaN(value); }, instructions: \"the value can only be a valid number, e.g. 1, 3.14 or 2010\" }; // checks if the value contains only letters and numbers validator.types.isAlphaNum = { validate: function (value) { return !/[^a-z0-9]/i.test(value); }, instructions: \"the value can only contain characters and numbers, no special symbols\" } 最后，validator对象的核心是这样的： const validator = { // all avaliable checks types: {}, // error messages in the current // validation session messages: [], config: {}, // the interface method // `data` is key => value pairs validate: function(data) { let i, msg, type, checker, result_ok; // reset all messages this.messages = []; for (i in data) { if (data.hasOwnProperty(i)) { type = this.config[i]; checker = this.types[type]; if (!type) { continue; // no need to validate } if (!checker) { throw { name: \"ValidationError\", message: \"No handler to validate type\" + type, }; } result_ok = checker.validate(data[i]); if (!result_ok) { msg = \"Invalid value for * \" + i + \"*, \" + checker.instructions; this.messages.push(msg); } } } return this.hasErrors(); }, // helper hasErrors: function() { return this.messages.length ! == 0; } }; 如你所见，validator对象是通用的，在所有的需要验证的场景下都可以保持这个样子。改进它的办法就是增加更多类型的检查。如果你讲它用在很多页面上，每块你就会有一个非常好的验证类型的集合。然后在每个新的使用场景下你需要做的仅仅是配置validator然后调用validate（）方法。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记六.html":{"url":"Design/javaScript设计模式笔记六.html","title":"javaScript设计模式笔记六","keywords":"","body":"JavaScript设计模式-外观模式 外观模式是一种很简单的模式，它只是为对象提供了更多选择的接口。是方法保持短小而不是处理太多的工作是一种很好的实践。在这种实践的指导下，你会有一对的方法，而不是一个有着非常多参数的Uber方法。有写时候，两个或者更所的方法会经常被一起调用。在这种情况下，创建另一个将这些重复调用包裹起来的方法就变得有意义了。 例如，在处理浏览器实践的时候，有一下的事件： stopPropagation（） 阻止实践冒泡到父节点 preventDefault（） 阻止浏览器执行默认动作（如打开连接或者提交表单） 这是两个有不同目的的相互独立的方法，他们也应该保持独立，但于此同时，他们也经常被一起调用。所以为了不再应用中到处重复调用这两个方法，你可以创建你一个外观方法来调用它们： const myevent = { // .. stop: function(e) { e.preventDefault(); e.stopPropagation(); } // ... }; 外观模式也适用于一些浏览器脚本的场景，即将浏览器的差异隐藏在一个外观方法的下面。继续前面的例子，你可以添加一些处理IE中实践API的代码： const myevent = { // ... stop: function(e) { // others if (typeof e.preventDefault === \"function\"{ e.preventDefault(); } if (typeof e.stopPropagation ===\"function\") { e.stopPropagation(); } // IE if (typeof e.returnValue === \"boolean\") { e.returnValue = false; } if (typeof e.cancelBubble === \"boolean\") { e.cancelBubble = true; } } // ... } 外观模式在做一些重新设计和重构工作时也很有用。当你想用一个不同的实现来替换某个对象的时候，你可能需要工作相当长时间（一个复杂的对象），于此同时，一些使用这个新对象的代码在被同步编写。你可以先想好新对象的API。然后使用新的API创建一个外观方法在旧的对象前面。使用这种方式，当你完全替换到旧的对象的时候，你只需要修改少量客户代码，因为新的客户代码已经是在使用新的API了。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记七.html":{"url":"Design/javaScript设计模式笔记七.html","title":"javaScript设计模式笔记七","keywords":"","body":"JavaScript设计模式-代理模式 在代理设计模式中，一个对象充当了另一个对象的接口的角色。它和外观模式不一样，外观模式的方便仅限于将几个方法调用联合起来。而代理对象位于某个对象和它的客户之间，可以保护对象的访问。 这个模式看起来开销有点大，但在出于性能考虑是非常有用。代理对象可以作为对象（也叫“真正的主体”）的保护者，让真正的主体对象做尽量少的工作。 一种示例用法是我们称之为“懒初始化”（延迟初始化）的东西。假设初始化真正的主体是开销很大的，并且正好可以作为客户代码将它初始化后并不真正使用它。在这种情况下，代理对象可以作为真正的主体的接口起到帮助作用。代理对象接收到初始化请求，但在真正的主体真正被使用之前都不会将它传递过去。 图展示了这个场景，当客户代码发出初始化请求时，代理对象回复一切就绪，但并没有将请求传递过去，只有在客户代码真正需要真正的主体多做些工作的时候才将两个请求一起传递过去。 一个例子 在真正的主体做某件工作开销很大是，代理模式很有用处。在web应用中，开销最大的操作之一就是网络请求，此时尽可能地合并HTTP请求是有意义的。我们来看一个这种场景下应用代理模式的实例。 一个视屏列表（expando） 我们假设又一个用来播放选中视频的应用。你可以在这里看到真实的例子http://www.jspatterns.com/book/7/proxy.html 页面上又一个视频标题的列表，当用户点击视频标题的时候，标题下方的区域会展开并显示视屏的更多信息，同时也是的视频可被播放。视频的详细信息和用来播放的URL并不是页面的一部分，它们需要通过网络请求来获取。服务端可以接收多个视频ID，这样我们就可以在合适的时候通过一次请求多个视频信息来减少HTTP请求以加快应用的速度。 我们的应用允许一次展开好几个（或全部）视频，所以这是一个合并网络请求的绝好机会。 没有代理对象的情况 这个应用中最主要的角色是两个对象： videos 负责对信息区域展开/收起（videos.getInfo()方法）和播放视屏的响应（videos.getPlayer()方法） http 负责通过http.makeRequest（）方法与服务端通讯 当没有代理对象的时候，videos.getInfo()会为每个视频调用一次http.makeRequest()方法。当我们添加代理对象proxy后，它将位于videos和http中间，接收对makeRequest()的调用，并在可能的时候合并请求。 我们首先看一下没有代理对象的代码，然后添加嗲里对象来提升应用的响应速度。 HTML HTML代码仅仅是一个链接列表： Toggle Checked Grave digger Save Me Crus h Don't Drink The Water Fun ny the Way It Is What Would You Say 事件处理 现在我们来看一下事件处理的逻辑。首先我们定义一个方便的快捷函数$: var $ = function (id) { return document.getElementById(id); }; 使用事件代理，我们将所有id =\"vids\"的条目是那个的点击事件同意放到一个函数中处理： $('vids').onclick = function (e) { var src, id; e = e || window.event; src = e.target || e.srcElement; if (src.nodeName !== \"A\") { return; } if (typeof e.preventDefault === \"function\") { e.preventDefault(); } e.returnValue = false; id = src.href.split('--')[1]; if (src.className === \"play\") { src.parentNode.innerHTML = videos.getPlayer(id); return; } src.parentNode.id = \"v\" + id; videos.getInfo(id); }; videos对象 videos对象有三个方法： getPlayer（） 返回播放视频需要的HTML代码 updateList() 网络请求的回调函数，接收从服务器返回的数据，然后生成用于视频详细信息的HTML代码。这一部分也没有什么太有趣的事情。 getInfo() 这个方法切换视屏信息的可视状态，同时也调用http对象的方法，并传递updateList（）作为回调函数。 下面是这个对象的代码片段： var videos = { getPlayer: function (id) {...}, updateList: function (data) {...}, getInfo: function (id) { var info = $('info' + id); if (!info) { http.makeRequest([id], \"videos.updateList\"); return; } if (info.style.display === \"none\") { info.style.display = ''; } else { info.style.display = 'none'; } } }; http对象 http对象只有一个方法，它像Yahoo！的YQL服务发起一个JSONP请求： var http = { makeRequest: function (ids, callback) { var url = 'http://query.yahooapis.com/v1/public/yql?q=', sql = 'select * from music.video.id where ids IN (\"%ID%\")', format = \"format=json\", handler = \"callback=\" + callback, script = document.createElement('script'); sql = sql.replace('%ID%', ids.join('\",\"')); sql = encodeURIComponent(sql); url += sql + '&' + format + '&' + handler; script.src = url; document.body.appendChild(script); } }; YQL(Yahoo! Query Language)是一种web service，它提供了使用类似SQL的语法来调用很多其他web service的能力，使得使用者不需要学习每个service的API。 当所有的六个视频都被选中后，将会向服务端发起六个独立的像这样的YQL请求： select * from music.video.id where ids IN (\"2158073\") 代理对象 前面的代码工作的很正常，但我们可以让它工作的更好。proxy对象就在这样的场景中出现，并接管了http和videos对象之间的通讯。它将使用一个简单的逻辑来尝试合并请求：50ms的延迟。videos对象并不直接调用后台接口，而是调用proxy对象的方法。proxy对象在转发这个请求前将会等待一段时间，如果在等待的50ms内有另一个来自videos的调用，则他们将被合并请求以提升点击“toggle”时的体验，一次展开多个视频。它也可以显著降低服务的负载，因为web服务器只需要处理更少亮的请求。 合并后查询两个视频信息的YQL大概是这样： select * from music.video.id where ids IN (\"2158073\", \"123456\") 在修改后的代码中，唯一变化是videos。getInfo()现在调用的是proxy。makeRequest()而不是http.makeRequest()，像这样： proxy.makeRequest(id, videos.updateList, videos); proxy对象创建了一个队列来收集50ms之内接收到的视频ID，然后将这个队列传递给http对象，并提供回调函数，因为videos.updateList()只能处理一次接收到的数据。 下面是proxy对象的代码： var proxy = { ids: [], delay: 50, timeout: null, callback: null, context: null, makeRequest: function (id, callback, context) { // add to the queue this.ids.push(id); this.callback = callback; this.context = context; // set up timeout if (!this.timeout) { this.timeout = setTimeout(function () { proxy.flush(); }, this.delay); } }, flush: function () { http.makeRequest(this.ids, \"proxy.handler\"); // clear timeout and queue this.timeout = null; this.ids = []; }, handler: function (data) { var i, max; // single video if (parseInt(data.query.count, 10) === 1) { proxy.callback.call(proxy.context, data.query.results.Video); return; } // multiple videos for (i = 0, max = data.query.results.Video.length; i 了解代理模式后就在只简单地改动一下原来的代码的情况下，将多个web serivice请求合并为一个。 了解代理模式后就在只简单地改动一下原来的代码的情况下，将多个 web service 请求合并为一个。 图 7-4 和 7-5 展示了使用代理模式将与服务器三次数据交互(不用代理模式 时)变为一次交互的过程。 使用嗲里对象做缓存 在这个例子中，客户对象（videos）已经可以做到不对同一个对象重复发出请求。但显示情况中并不总是这样。这个代理对象还可以通过缓存之前的请求结果到cache属性中来进一步保护真正的主体http对象（图7-6）。然后当videos对象需要对同一个ID的视频请求第二次时，proxy对象可以直接从缓存中取出，从而避免一次网络交互。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记八.html":{"url":"Design/javaScript设计模式笔记八.html","title":"javaScript设计模式笔记八","keywords":"","body":"JavaScript设计模式-中介者模式 一个应用不论大小，都是由一些彼此独立的对象组成的。所有的对象都需要 一个通讯的方式来保持可维护性，即你可以安全地修改应用的一部分而不破 坏其它 部分。随着应用的开发和维护，会有越来越多的对象。然后，在重构 代码的时候，对象可能会被移除或者被重新安排。当对象知道其它对象的太 多信息并且直接通讯 (直接调用彼此的方法或者修改属性)时，会导致我们 不愿意看到的紧耦合。当对象耦合很紧时，要修改一个对象而不影响其它的 对象是很困难的。此时甚至连一个 最简单的修改都变得不那么容易，甚至连 一个修改需要用多长时间都难以评估。 中介者模式就是一个缓解此问题的办法，它通过解耦来提升代码的可维护性 (见图 7-7)。在这个模式中，各个彼此合作的对象并不直接通讯，而是通 过一 个 mediator(中介者)对象通讯。当一个对象改变了状态后，它就通 知中介者，然后中介者再将这个改变告知给其它应该知道这个变化的对象。 中介者示例 我们来看一个使用中介者模式的实例。这个应用是一个游戏，它的玩法是比 较两位游戏者在半分钟内按下按键的次数，次数多的获胜。玩家 1 需要按的 是 1，玩家 2 需要按的是 0(这样他们的手指不会搅在一起)。当前分数会显 示在一个计分板上。 对象列表如下: Player 1 Player 2 Scoreboard Mediator 中介者 Mediator 知道所有的对象。它与输入设备(键盘)打交道，处理 keypress 事件，决定现在是哪位玩家玩的，然后通知这个 玩家(见图 7-8)。 玩家负责玩(即给自己的分数加一分)，然后通知中介者他这一轮已经玩完。 中介者再告知计分板最新的分数，计分板更新显示。 除了中介者之外，其它的对象都不知道有别的对象存在。这样就使得更新这 个游戏变得很简单，比如要添加一位玩家或者是添加另外一个显示剩余时间 的地方。 你可以在这里看到这个游戏的在线演示 http://jspatterns.com/book/7/mediator.html。 玩家对象是通过 Player()构造函数来创建的，有自己的 points 和 name 属性。 原型上的 play()方法负责给自己加一分然后通知中介者: function Player(name) { this.points = 0; this.name = name; } Player.prototype.play = function () { this.points += 1; mediator.played(); }; scoreboard 对象(计分板)有一个 update()方法，它会在每次玩家玩完后被 中介者调用。计分析根本不知道玩家的任何信息，也不保存分数，它只负责 显示中介者给过来的分数: var scoreboard = { // HTML element to be updated element: document.getElementById('results'), // update the score display update: function (score) { var i, msg = ''; for (i in score) { if (score.hasOwnProperty(i)) { msg += '' + i + ': '; msg += score[i]; msg += ''; } } this.element.innerHTML = msg; } }; 现在我们来看一下 mediator 对象(中介者)。在游戏初始化的时候，在 setup() 方法中创建游戏者，然后放后 players 属性以便后续使 用。played()方法会 被游戏者在每轮玩完后调用，它更新 score 哈希然表然后将它传给 scoreboard 用于显示。最后一个方法是 keypress()，负责处理键盘事件， 决定是哪位玩家玩的，并且通知它: var mediator = { // all the players players: {}, // initialization setup: function () { var players = this.players; players.home = new Player('Home'); players.guest = new Player('Guest'); }, // someone plays, update the score played: function () { var players = this.players, score = { Home: players.home.points, Guest: players.guest.points }; scoreboard.update(score); }, // handle user interactions keypress: function (e) { e = e || window.event; // IE if (e.which === 49) { // key \"1\" mediator.players.home.play(); return; } if (e.which === 48) { // key \"0\" mediator.players.guest.play(); return; } } }; 最后一件事是初始化和结束游戏: // go! mediator.setup(); window.onkeypress = mediator.keypress; // game over in 30 seconds setTimeout(function () { window.onkeypress = null; alert('Game over!'); }, 30000); By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Design/javaScript设计模式笔记九.html":{"url":"Design/javaScript设计模式笔记九.html","title":"javaScript设计模式笔记九","keywords":"","body":"JavaScript设计模式-观察者模式 观察者模式被广泛地应用于 JavaScript 客户端编程中。所有的浏览器事件 (mouseover，keypress 等)都是使用观察者模式的例 子。这种模式的另一 个名字叫“自定义事件”，意思是这些事件是被编写出来的，和浏览器触发 的事件相对。它还有另外一个名字叫“订阅者/发布者”模式。 使用这个模式的最主要目的就是促进代码触解耦。在观察者模式中，一个对 象订阅另一个对象的指定活动并得到通知，而不是调用另一个对象的方法。 订阅者 也被叫作观察者，被观察的对象叫作发布者或者被观察者 35。当一个 特定的事件发生的时候，发布者会通知(调用)所有的订阅者，同时还可能 以事件对象的形式传递一些消息。 例1: 杂志订阅 为了理解观察者模式的实现方式，我们来看一个具体的例子。我们假设有一个发布者paper，它发型一份日报和一份月刊。无论是日报还是月刊发行，有一个名叫joe的订阅者都会收到通知。 paper 对象有一个 subscribers 属性，它是一个数组，用来保存所有的订阅 者。订阅的过程就仅仅是将订阅者放到这个数组中而已。当一个事 件发生时， paper 遍历这个订阅者列表，然后通知它们。通知的意思也就是调用订阅者 对象的一个方法。因此，在订阅过程中，订阅者需要提供一个方法给 paper 对象的 subscribe()。 paper 对象也可以提供 unsubscribe()方法，它可以将订阅者从数组中移除。 paper 对象的最后一个重要的方法是 publish()，它负责调用订阅者的方法。 总结一下，一个发布者对象需要有这些成员: subscribers 一个数组 subscribe() 将订阅者加入数组 unsubscribe() 从数组中移除订阅者 publish() 遍历订阅者并调用它们订阅时提供的方法 所有三个方法都需要一个 type 参数，因为一个发布者可能触发好几种事件 (比如同时发布杂志和报纸)，而订阅者可以选择性地订阅其中的一种或几 种。 因为这些成员对任何对象来说都是通用的，因此将它们作为独立对象的一部 分提取出来是有意义的。然后，我们可以(通过掺元模式)将它们复制到任 何一个对象中，将这些对象转换为订阅者。 下面是这些发布者通用功能的一个示例实现，它定义了上面列出来的所有成 员，还有一个辅助的 visitSubscribers()方法: var publisher = { subscribers: { any: [] // event type: subscribers }, subscribe: function (fn, type) { type = type || 'any'; if (typeof this.subscribers[type] === \"undefined\") { this.subscribers[type] = []; } this.subscribers[type].push(fn); }, unsubscribe: function (fn, type) { this.visitSubscribers('unsubscribe', fn, type); }, publish: function (publication, type) { this.visitSubscribers('publish', publication, type); }, visitSubscribers: function (action, arg, type) { var pubtype = type || 'any', subscribers = this.subscribers[pubtype], i, max = subscribers.length; for (i = 0; i 下面这个函数接受一个对象作为参数，并通过复制通用的发布者的方法将这 个对象墨迹成发布者: function makePublisher(o) { var i; for (i in publisher) { if (publisher.hasOwnProperty(i) && typeof publisher[i] === \"function\") { o[i] = publisher[i]; } } o.subscribers = {any: []}; } 现在我们来实现 paper 对象，它能做的事情就是发布日报和月刊: var paper = { daily: function () { this.publish(\"big news today\"); }, monthly: function () { this.publish(\"interesting analysis\", \"monthly\"); } }; makePublisher(paper); 现在我们有了一个发布者，让我们再来看一下订阅者对象 joe，它有两个方 法: var joe = { drinkCoffee: function (paper) { console.log('Just read ' + paper); }, sundayPreNap: function (monthly) { console.log('About to fall asleep reading this ' + monthly); } }; 现在让 joe 来订阅 paper: paper.subscribe(joe.drinkCoffee); paper.subscribe(joe.sundayPreNap, 'monthly'); 如你所见，joe 提供了一个当默认的 any 事件发生时被调用的方法，还提供 了另一个当 monthly 事件发生时被调用的方法。现在让我们来触发一些事件 paper.daily(); paper.daily(); paper.daily(); paper.monthly(); 这些发布行为都会调用 joe 的对应方法，控制台中输出的结果是: Just read big news today Just read big news today Just read big news today About to fall asleep reading this interesting analysis 这里值得称道的地方局势paper对象并没有硬编码写上joe，而joe也同样没有硬编码写上paper。这里也没有知道所有事情的中介者对象。多有设计到的对象都是松耦合的，而且在不修改代码的前提下，我们可以个paper添加更多的订阅者，同时joi也可以在任何时候取消订阅。 让我们更进一步，将joe也编程一个发布者。（毕竟，在博客和微博上，任何人可以是发布者。）这样，joe变成发布者之后就可以在Twitter上更新状态： makePublisher(joe); joe.tweet = function(msg) { this.publish(msg); } 现在假设paper的公共部门准备通过Twitter收集读者反馈，于是它订阅了joe,提供了一个方法readTweets(): paper.readTweets = function(tweet) { alert('Call big meeting! Someone ' + tweet); }; joe.subscribe(\"hated the paper today\"); 这样每当joe发出消息时，paper就会弹出警告窗口： joe.tweet(\"hated the paper today\"); 结果是一个精工窗口：“Call big meeting！Someone hated the paper today”。 你可以在 http://jspatterns.com/book/7/observer.html 看到完整的源代 码，并且在控制台中运行这个实例。 例 2:按键游戏 我们来看另一个例子。我们将实现一个和中介者模式的示例一样的按钮游戏， 但这次使用观察者模式。为了让它看起来更高档，我们允许接受无限个玩家， 而 不限于 2 个。我们仍然保留用来产生玩家的 Player()构造函数，也保留 scoreboard 对象。只有 mediator 会变成 game 对象。 在中介者模式中，mediator 对象知道所有涉及到的对象，并且调用它们的方 法。而观察者模式中的game对象不是这样，它会让对象来订阅它们感兴趣的 事件。比如，scoreboard 会订阅 game 对象的 scorechange 事件。 首先我们重新看一下通用的 publisher 对象，并且将它的接口做一点小修改 以更贴近浏览器的情况: 将 publish()，subscribe()，unsubscribe()分别改为 fire()，on()，remove() 事件的 type 每次都会被用到，所以把它变成三个方法的第一个参数 可以给订阅者的方法额外加一个 context 参数，以便回调方法可以用 this 指向它 { this.subscribers[type] = []; 自己所属的对象 新的 publisher 对象是这样: var publisher = { subscribers: { any: [] }, on: function (type, fn, context) { type = type || 'any'; fn = typeof fn === \"function\" ? fn : context[fn]; if (typeof this.subscribers[type] === \"undefined\"){ this.subscribers[type] = []; } } this.subscribers[type].push({fn: fn, context: context || this}); }, remove: function (type, fn, context) { this.visitSubscribers('unsubscribe', type, fn, context); }, fire: function (type, publication) { this.visitSubscribers('publish', type, publication); }, visitSubscribers: function (action, type, arg, context) { var pubtype = type || 'any', subscribers = this.subscribers[pubtype], i, max = subscribers ? subscribers.length : 0; for (i = 0; i 新的Player()构造函数是这样： function Player(name, key) { this.points = 0; this.name = name; this.key = key; this.fire('newplayer', this); } Player.prototype.play = function () { this.points += 1; this.fire('play', this); }; 变动的部分是这个构造函数接收key，代表这个玩家在键盘上用来按之后得分的按键。（这些键预先硬编码过。）每次创建一个新玩家的时候，一个newplayer事件也会被触发。类似的，每次有一个玩家玩的时候，会触发play事件。 scoreboard对象和原来一样，它只是简单地将当前分数显示出来。 game对象会关注所有的玩家，这样它九可以给出分数并且触发scorechange事件。它也会订阅浏览中所有的keypress事件，这样它聚会知道按钮对应的玩家： const game = { key: {}, addPlayer: function(player) { const key = player.key.toString().charCOdeAt(0); this.keys[key] = player; handleKeypress: function (e) { e = e || window.event; // IE if (game.keys[e.which]) { game.keys[e.which].play(); } }, handlePlay: function(player) { let i, players = this.keys; score = {}; for (i in players) { if (players.hasOwnProperty(i)) { score[players[i].name] = players[i].points; } } this.fire('scorechange', score); } } } 用于将任意对象转变为订阅者的makePublisher()还是和之前一样。game对象会变成发布者（这样它才可以触发scorechange事件），Player.prototype也会变成发布者，以使得每个玩家对象可以触发play和newplayer事件： makePublisher(Player.prototype); makePublisher(game); game对象订阅play和newplayer事件（以及浏览器的keypress事件），scoreboard订阅scorechange事件： Player.prototype.on(\"newplayer\", \"addPlayer\", game); Player.prototype.on(\"play\", \"handlePlay\", game); game.on(\"scorechange\", scoreboard.update, scoreboard); window.onkeypress = game.handleKeypres; 如你所见，on()方法允许订阅者通过函数（scoreboard.update）或者是字符串（“addPlayer”）来制定回调函数。当有提供context（如game）时，才能通过字符串来指定回调函数。 初始化的最后一点工作就是动态地创建玩家对象（以及他们对象的按键），用户想要多少个就可以创建多少个： var playername, key; while (1) { playername = prompt(\"Add player (name)\"); if (!playername) { break; } while (1) { key = prompt(\"Key for \" + playername + \"?\"); if (key) { break; } } new Player(playername, key); } 这就是游戏的全部。你可以在看到完整的代码并且试玩一下。 值得注意的是，在中介者模式中，mediator对象必须知道所有的对象，然后在适当的时机去调用对应的方法。而这个例子中，game对象会显得笨拙一些，游戏依赖于对象去观察特写事件然后触发相应的动作：如scoreboard观察scoreboard事件。这是的对象之间的耦合更怂了（对象间知道彼此的信息越少越好），而代价则是弄清事件和订阅者之间的对应关系会更困难一些。 在这个例子中，所有的订阅行为都发生在代码中的同一个地方，而随着应用规模的增长，on（）可能会被在各个地方调用（如在每个对象的初始化代码中）。这使得调试更困难一些，因为没有一个几种的地方来看这些代码并理解正在发生什么事情≥在观察者弄湿中，你讲不再能看到那种从开头一直跟到结尾的顺序执行方式。 小结 在这章中你学到了若干种流行的设计模式，并且也知道了如何在JavaScript中实现它们。我们讨论过的设计模式有： 单例模式 只创建了类的唯一一个实例。我们看了好几种可以不通过搞糟函数和类Java语法达成单例的方法。从另一方面来说，JavaScript中所有的对象都是单例。有时候开发者说的单例是指铜鼓模块化创建的对象。 工厂模式 一种在运行时通过指定字符串来创建指定类型对象的方法。 遍历模式 通过提供API来实现复杂的洗定义数据节后中的遍历和导航。 装饰模式 在运行时通过从预先定义好的装饰器对象来给被装饰对象动态添加功能。 策略模式 保持接口一直的情况下选择最好的策略来完成特写类型的任务。 外观模式 通过包装通用的（或设计的很差的）方法来提供一个更方便的 API。 代理模式 包装一个对象以控制对它的访问。通过合并操作或者是只在真正需要使执行来尽量避免开销太大的操作 中介者模式 通过让对象不彼此沟通，只通过一个中介者对象沟通的方法来促进解耦。 观察者模式 通过创建“可被观察的对象“使它在某个事件发生时通知订阅者的方式来接耦。（也叫”订阅者/发布者”或“自定义事件”）。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"Database/":{"url":"Database/","title":"数据库","keywords":"","body":"Mysql 语法笔记一：delete和truncate的区别和联系 语法笔记二：having与where 语法笔记三：pluck 语法笔记四：first 语法笔记五：references 语法笔记六：where1 语法笔记七：in和exists二者之间的区别和性能影响 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"ORM/ORM框架选型.html":{"url":"ORM/ORM框架选型.html","title":"ORM框架选型","keywords":"","body":"ORM框架选型(笔记) 选型标准：实现O/R mapping，基于promise，支持原生SQL语句，支持连接池，支持事物，支持ES6/ES7 name star createtime sequelize 19934 2010-07-18 node-orm2 3001 2012-11-25 typeorm 15230 2016-02-21 bookshelf 5664 2013-03-10 waterline 5099 2012-11-25 knex 10539 2012-12-23 node-orm2 3001 2013-08-03 备注：以上数据截止2019.9.25 node-mysql与knex非ORM框架 sequeize https://github.com/sequelize/sequelize 文档齐全，star最多。 支持数据库PostgreSQL、mysql、sqlite和sqlserver 特性：模型定义 模型同步、删除1：1 1:m n:m关联 穿透模型promise hooks/callbacks/lifecycle 支持原生SQL连接池 事务 迁移CLI node-orm2 https://github.com/dresende/node-orm2 支持数据库： mysql & mariadb postgresql amazon sqlite mongodb(beta版) 特性： 创建模型、同步删除、批量新增】获取、查询、删除、统计、集合创建模型关联 定制有效性验证 模型实例缓存 通过插件支持分页、事务、mysql全文搜索、迁移。支持原生SQL语句。连接池，可通过pool参数设置。promise通过q-rom库实现。 项目近期更新少，与sequelize竞争完全处于劣势，放弃。 bookshelf [https://github.com/tgriesser/bookshelf] (https://github.com/tgriesser/bookshelf) bookshelf是基于knex的ORM框架 支持数据库： mysql & mariadb & postgresql & sqlite3 和 Oracle 特性：事务 连接池 流式查询 promise和callback API 原生SQL语句 waterline https://github.com/balderdashy/waterline waterline是从Salis框架衍生出来的ORM框架 支持数据库： mysql、mongo、PostgreSQL、redis等 文档包含在sail文档中 http://sailsjs.com/documentation/reference/waterline-orm 文档不完善，放弃。 typeorm 基于Decorator的ORM框架，对TypeScript支持比较好，同时支持在JavaScript中通过手动声明使用，以及JSON方式的Entity配置声明 官网：https://github.com/typeorm/typeorm/ 数据库： 支持关系行数据库，beta支持MongoDB 编程风格： 基本上是Hibernate的js版本 支持Promise/async/await 支持基于链式构造的Query Builder查询 支持CLI工具 热度： 周频更新， NPM周下载2.8w 最终对比 从以上分析，我们最终从sequelize和bookshelfl两者之间作出选择。项目数据对比 https://nodejs.libhunt.com/compare-sequelize-vs-bookshelf 依赖于sequelize的项目： https://www.npmjs.com/browse/depended/sequelize 依赖于bookshelf的项目： https://www.npmjs.com/browse/depended/bookshelf 1W star 以上项目依赖： ghost项目使用bookshelf react-starter-kit项目使用sequelize 两者文档都很全面。sequelize的star数量和版本更新频率高于bookshelf，所以推荐使用Sequelize By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"ORM/Knexjs笔记.html":{"url":"ORM/Knexjs笔记.html","title":"Knexjs笔记","keywords":"","body":"Knexjs笔记 knex方法分类： 操作table的方法，属于Schema Builder，对应create、drop、alter等 操作column的方法，属于Schema Builder，如设置键的类型，设置主键，外键等 执行SQL请求的方法，属于Query Builder，对应select、insert、update、delete等 其他方法 knex安装 #以Mysql为例 sudo npm install knex --save sudo npm install mysql --save knex初始化 const knex = require('knex')({ client: 'mysql', //指明数据库类型，还可以是mysql，sqlite3等等 connection: { //指明连接参数 host : '127.0.0.1', user : 'liuyueyi', password : 'password', database : 'example' }, debug: true, //指明是否开启debug模式，默认为true表示开启 pool: { //指明数据库连接池的大小，默认为{min: 2, max: 10} min: 0, max: 7, }, acquireConnectionTimeout: 10000, //指明连接计时器大小，默认为60000ms migrations: { tableName: 'migrations' //数据库迁移，可选 } }); 首先让我们在项目中和全局都安装knex。因为我们用的是mysql，所以mysql的模块也要安装。 npm install knex --save npm install knex -g npm install mysql --save 装好之后，别忘了检查一下。 knex --version Knex CLI version: 0.19.5 Knex Local version: None 初次使用的时候，可以用命令行创建一个配置文件。创建再项目的根目录中。 ->knex init Created .knexfile.js 这个是创建knexfile.js里的默认内容。 // Update with your config settings. module.exports = { development: { client: 'sqlite3', connection: { filename: './dev.sqlite3' } }, staging: { client: 'postgresql', connection: { database: 'my_db', user: 'username', password: 'password' }, pool: { min: 2, max: 10 }, migrations: { tableName: 'knex_migrations' } }, production: { client: 'postgresql', connection: { database: 'my_db', user: 'username', password: 'password' }, pool: { min: 2, max: 10 }, migrations: { tableName: 'knex_migrations' } } }; 我们之前写了一个config.js，里面就包含了数据库的设置。这个时候就需要我们合并一下了，合并之后就是下面这个样子了。 这当中使用了lodash这个库，这个库就是一个工具库。其中有很多的函数可以直接用，很方便。 var config = require('./config'); var _ = require('lodash'); var baseConfig = { migrations: { directory: './db/migrations' } } module.exports = { development: _.extend(config.development.database, baseConfig), staging : _.extend(config.staging.database, baseConfig), production : _.extend(config.production.database, baseConfig) }; 我们第一次做的更新就是创建users表，用于存储用户的信息。我们在knex的配置文件中设定了migrations的目录，所以直接生成在那个目录当中。 -> knex migrate:make add-user Using environment: development Created Migration: /Users/project/demo/db/migrations/20191011122323_add_user.js 刚生成完，里面是什么都没有的。生成表的代码我们就写再这个地方，里面定义了更新和回滚的操作，分别放在up和down中。我们顺便在up中给users表插入了一条数据。 这里面用到了Promise，如果你不了解的话可以参考以下： http://caniuse.com/#search=promise https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise https://www.promisejs.org/ http://caniuse.com/#search=promise exports.up = function(knex, Promise) { return Promise.all([ // 创建表 knex.schema.createTableIfNotExists('users', function (table) { table.increments('id').primary().notNullable(); table.string('name').notNullable(); table.string('password').notNullable(); table.string('email').notNullable(); table.string('uuid').notNullable(); table.string('status').notNullable().defaultTo('active'); table.dateTime('created_at').notNullable(); table.dateTime('updated_at').nullable(); }), // 插入数据 knex.table('users').insert({ name: 'limichange', email: 'limichange@hotmail.com', password: 'asdfasdf', uuid: '1', created_at: new Date() }) ]); }; exports.down = function(knex, Promise) { return Promise.all([ // 删除表 knex.schema.dropTable('users') ]); }; 接下来用命令行工具来更新数据库： ➜ knex migrate:latest Using environment: development Batch 1 run: 1 migrations /Users/limi/website/db/migrations/20160719121550_add-user.js 只要执行撤销的操作，就可以让数据库回滚 ➜ knex migrate:rollback Using environment: development Batch 1 rolled back: 1 migrations /Users/limi/website/db/migrations/20160719121550_add-user.js By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"ORM/bookshelf笔记.html":{"url":"ORM/bookshelf笔记.html","title":"bookshelf笔记","keywords":"","body":"bookshelf笔记 https://blog.csdn.net/liuyueyi1995/article/details/53760577 https://github.com/arden/egg-bookshelf By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/":{"url":"TypeScript/","title":"TypeScript","keywords":"","body":"TypeScript 基础笔记一 基础笔记二 函数笔记 泛型笔记 枚举笔记 类型兼容性笔记 高级类型笔记 .d.ts和源文件.ts有什么区别? 使用TypeScript装饰器装饰你的代码 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/基础笔记一.html":{"url":"TypeScript/基础笔记一.html","title":"基础笔记一","keywords":"","body":"基础笔记一 基础类型易错点 元组Tuple let x: [string, number] // 当访问一个越界的元素，会使用联合类型代替 console.log(x[5].toString()); // OK, 'string' 和 'number' 都有 toString x[6] = true; // Error,布尔不是（string | number）类型 Never never类型是任何类型的子类型，也可以赋值给任何类型；然而，没有类型是never的子类型或可以赋值给never类型（除了never本身之外）。即使any也不可以赋值给never // 返回never的函数必须存在无法达到的终点 function error(message: string): never { throw new Error(message); } Object object表示非原始类型，也就是除number，string，boolean，symbol，null或undefined之外的类型。 使用object类型，就可以更好的表示像Object.create这样的API。例如： declare function create(o: object | null): void; create({ prop: 0 }); // OK create(null); // OK create(42); // Error create(\"string\"); // Error create(false); // Error create(undefined); // Error 变量易错点 对象展开比数组复杂的多 let defaults = { food: \"spicy\", price: \"$$\", ambiance: \"noisy\" }; let search = { food: \"rich\", ...defaults }; name，defaults里的food属性会重写food: \"rich\"，再这里并不是我们呢想要的结果 对象展开还有一些意想不到的限制。首先，它仅包含对象自身的可枚举属性。大体上是说当你展开一个对象实例是，你会丢失其方法 class C { p = 12; m() { } } let c = new C(); let clone = { ...c }; clone.p; // ok clone.m(); // error! 在结构属性上基于一个默认或可选的属性用来替换主初始化列表，要知道C的定义又一个b是可选属性 function f({ a, b = 0 } = { a: \"\" }): void { // ... } f({ a: \"yes\" }); // ok, default b = 0 f(); // ok, default to {a: \"\"}, which then defaults b = 0 f({}); // error, 'a' is required if you supply an argument 接口 只读属性 TypeScript具有ReadonlyArray类型，它与Array相似，只是把所有可变方法去掉了，因此可以确保数组创建后再也不能被修改： let a: number[] = [1, 2, 3, 4]; let ro: ReadonlyArray = a; ro[0] = 12; // error! ro.push(5); // error! ro.length = 100; // error! a = ro; // error! 上面代码的最后一行，可以看到就算把整个ReadonlyArray赋值到一个普通数组也是不可以的。但是你可以用类型断言重写 a = ro as number[]; readonly vs const 最简单判断该用readonly还是const的方法是看把它作为变量使用还是作为一个属性，作为变量使用的话用const，若作为属性则使用readonly 额外的属性检查 interface SquareConfig { color?: string; width?: number; } function createSquare(config: SquareConfig): { color: string; area: number } { // ... } // error: 'colour' not expected in type 'SquareConfig' let mySquare = createSquare({ colour: \"red\", width: 100 }); 如果一个对象字面量存在任何“目标类型”不包含的属性时，你会得到一个错误 绕开这些检查非常简单 方法一：最简单的方法就是适用类型断言 let mySquare = createSquare({ width: 100, opacity: 0.5 } as SquareConfig); 方法二：最佳的方式时能够添加一个字符串索引签名，前提时你能够确定这个兑现更可能具有某些作为特殊用途使用的额外属性，如果SequareConfig带有上面定义的类型的color和width属性，并且还会带有任意数量的其他属性，那么我们可以这样定义它： interface SquareConfig { color?: string; width?: number; [propName: string]: any; } 方法三：它就是将这个对象赋值给另一个变量：因为squareOptions不会经过额外属性检查，所以编译器不会报错。 let squareOptions = { colour: \"red\", width: 100 }; let mySquare = createSquare(squareOptions); 可索引的类型 TypeScript支持两种索引签名：字符串和数字。可以同时使用两种类型的索引，但是数字索引的返回值必须是字符串索引返回值类型的子类型。这是因为当使用number来索引时，JavaScript会将它转换成string然后再去索引对象。也就是说用100（一个number）去索引等同于使用“100”(一个string)去索引，因此两者需要保持一致/ class Animal { name: string; } class Dog extends Animal { breed: string; } // 错误：使用数值型的字符串索引，有时会得到完全不同的Animal! interface NotOkay { [x: number]: Animal; [x: string]: Dog; } 字符串索引签名能够 很好的描述dictionary模式，并且他们也会确保所有属性与其返回值类型匹配。因为字符串索引声明了obj.property和obj[\"property\"]两种形式都可以。下面的例子里，name的类型与字符串索引类型不匹配，所以类型检查器给出一个错误提示： interface NumberDictionary { [index: string]: number; length: number; // 可以，length是number类型 name: string // 错误，`name`的类型与索引类型返回值的类型不匹配 } 类静态部分和实例部分的区别 当你操作类和接口的时候，你要知道累屎具有两个类型的：静态部分的类型和实例的类型。你会注意到，当你用构造器签名去定义一个接口并试图定义一个类去实现这个接口时会得到一个错误： interface ClockConstructor { new (hour: number, minute: number); } class Clock implements ClockConstructor { currentTime: Date; constructor(h: number, m: number) { } } 这里因为当一个类实现了一个接口时，只对其实例部分进行类型检查。constructor存在于类的静态部分，所以不再检查的范围内。 接口继承类 当接口继承了一个类类型时，它会继承类的成员但不包括其实现。九号嫌犯个借口声明了所有类中存在的成员，但并没有提供实现一样。这意味着当你创建了一个接口继承了一个拥有私有或受保护的成员的类时，这个接口类型只能被这个类或其子类所实现（implement） class Control { private state: any; } interface SelectableControl extends Control { select(): void; } class Button extends Control implements SelectableControl { select() { } } class TextBox extends Control { select() { } } // 错误：“Image”类型缺少“state”属性。 class Image implements SelectableControl { select() { } } class Location { } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/基础笔记二.html":{"url":"TypeScript/基础笔记二.html","title":"基础笔记二","keywords":"","body":"基础笔记二 类 继承 class Animal { name: string; constructor(theName: string) { this.name = theName; } move(distanceInMeters: number = 0) { console.log(`${this.name} moved ${distanceInMeters}m.`); } } class Snake extends Animal { constructor(name: string) { super(name); } move(distanceInMeters = 5) { console.log(\"Slithering...\"); super.move(distanceInMeters); } } class Horse extends Animal { constructor(name: string) { super(name); } move(distanceInMeters = 45) { console.log(\"Galloping...\"); super.move(distanceInMeters); } } let sam = new Snake(\"Sammy the Python\"); let tom: Animal = new Horse(\"Tommy the Palomino\"); sam.move(); tom.move(34); 派生类包含了一个构造函数，它必须调用super(),它会执行基类的构造函数。而且。在构造函数里访问this的属性之前，我们一定要调用super()。这个时TypeScript强制执行的一条重要规则。 结果： Slithering... Sammy the Python moved 5m. Galloping... Tommy the Palomino moved 34m. 公共，私有与受保护的修饰符 当我们比较带有private或protected成员的类型的时候，情况就不同了。如果其中一个类型了里包含一个private成员，那么只有当另外一个类型中也存在这样一个private成员，并且他们来自同一处声明时，我们才认为这两个类型时兼容的。对于protected成员也适用这个规则。 class Animal { private name: string; constructor(theName: string) { this.name = theName; } } class Rhino extends Animal { constructor() { super(\"Rhino\"); } } class Employee { private name: string; constructor(theName: string) { this.name = theName; } } let animal = new Animal(\"Goat\"); let rhino = new Rhino(); let employee = new Employee(\"Bob\"); animal = rhino; animal = employee; // 错误: Animal 与 Employee 不兼容. protected修饰符与private修饰符的行为很相似，但有一点不同，protected成员再派生类中仍然可以访问 构造函数也可以被标记成protected，这意味着这个类不能在包含它的类外部被实例化，但是能被继承。比如： class Person { protected name: string; protected constructor(theName: string) { this.name = theName; } } // Employee 能够继承 Person class Employee extends Person { private department: string; constructor(name: string, department: string) { super(name); this.department = department; } public getElevatorPitch() { return `Hello, my name is ${this.name} and I work in ${this.department}.`; } } let howard = new Employee(\"Howard\", \"Sales\"); let john = new Person(\"John\"); // 错误: 'Person' 的构造函数是被保护的. 存取器 只带有get不带有set的存取器自动被推断为readonly。这在代码生成.d.ts文件时是有帮助的，因为利用这个属性的用户会看到不允许够改变它的值。 抽象类 抽象类作为其他派生类的基类使用。他们一般不会直接被实例化。抽象类中的抽象方法不包含具体实现并且必须在派生类中实现。抽象方法的语法与接口相似。两者都是定义方法签名但不包含方法提。 abstract class Department { constructor(public name: string) { } printName(): void { console.log('Department name: ' + this.name); } abstract printMeeting(): void; // 必须在派生类中实现 } class AccountingDepartment extends Department { constructor() { super('Accounting and Auditing'); // 在派生类的构造函数中必须调用 super() } printMeeting(): void { console.log('The Accounting Department meets each Monday at 10am.'); } generateReports(): void { console.log('Generating accounting reports...'); } } let department: Department; // 允许创建一个对抽象类型的引用 department = new Department(); // 错误: 不能创建一个抽象类的实例 department = new AccountingDepartment(); // 允许对一个抽象子类进行实例化和赋值 department.printName(); department.printMeeting(); department.generateReports(); // 错误: 方法在声明的抽象类中不存在 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/函数笔记.html":{"url":"TypeScript/函数笔记.html","title":"函数笔记","keywords":"","body":"函数笔记 函数的类型 参数 TypeScript能够根据返回语句自动推断出返回值类型，因此我们通常省略他它。 可选参数必须跟在必须参数后面。 如果上例我们想让first name是可选的，那么就必须调整它们的位置，把first name放在后面。 在TypeScript里，我们也可以为参数提供一个默认值当用户没有传递这个参数或传递的值是undefined时。 它们叫做有默认初始化值的参数。 让我们修改上例，把last name的默认值设置为\"Smith\"。 function buildName(firstName: string, lastName = \"Smith\") { return firstName + \" \" + lastName; } let result1 = buildName(\"Bob\"); // works correctly now, returns \"Bob Smith\" let result2 = buildName(\"Bob\", undefined); // still works, also returns \"Bob Smith\" let result3 = buildName(\"Bob\", \"Adams\", \"Sr.\"); // error, too many parameters let result4 = buildName(\"Bob\", \"Adams\"); // ah, just right 在所有必须参数后面的带默认初始化的参数都是可选的，与可选参数一样，在调用函数的时候可以省略。 也就是说可选参数与末尾的默认参数共享参数类型。 function buildName(firstName: string, lastName?: string) { // ... } 和 function buildName(firstName: string, lastName = \"Smith\") { // ... } 共享同样的类型(firstName: string, lastName?: string) => string。 默认参数的默认值消失了，只保留了它是一个可选参数的信息。 与普通可选参数不同的是，带默认的参数不需要放在必须参数的后面。如果带默认值的参数必须出现在必须参数面前，用户必须明确的传入undefined值来获得默认值。 function buildName(firstName = \"Will\", lastName: string) { return firstName + \" \" + lastName; } let result1 = buildName(\"Bob\"); // error, too few parameters let result2 = buildName(\"Bob\", \"Adams\", \"Sr.\"); // error, too many parameters let result3 = buildName(\"Bob\", \"Adams\"); // okay and returns \"Bob Adams\" let result4 = buildName(undefined, \"Adams\"); // okay and returns \"Will Adams\" this let deck = { suits: [\"hearts\", \"spades\", \"clubs\", \"diamonds\"], cards: Array(52), createCardPicker: function() { return function() { let pickedCard = Math.floor(Math.random() * 52); let pickedSuit = Math.floor(pickedCard / 13); return {suit: this.suits[pickedSuit], card: pickedCard % 13}; } } } let cardPicker = deck.createCardPicker(); let pickedCard = cardPicker(); alert(\"card: \" + pickedCard.card + \" of \" + pickedCard.suit); 可以看到createCardPicker是个函数，并且它有返回一个函数。如果我们尝试运行这个程序，会发现它并没有弹出对话框而是报错了，因为createCardPicker返回的饿函数里的this被设置成了window顶级的非方法式调用会将this视为window。（注意：在严格模式下，this为undefined而不是window） 为了解决这个问题，我们可以在函数被返回时就绑定好正确的this。使用箭头函数，箭头函数能保存函数创建时的this值，而不是调用时的值： let deck = { suits: [\"hearts\", \"spades\", \"clubs\", \"diamonds\"], cards: Array(52), createCardPicker: function() { // NOTE: the line below is now an arrow function, allowing us to capture 'this' right here return () => { let pickedCard = Math.floor(Math.random() * 52); let pickedSuit = Math.floor(pickedCard / 13); return {suit: this.suits[pickedSuit], card: pickedCard % 13}; } } } let cardPicker = deck.createCardPicker(); let pickedCard = cardPicker(); alert(\"card: \" + pickedCard.card + \" of \" + pickedCard.suit); this参数在毁掉函数里 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/泛型笔记.html":{"url":"TypeScript/泛型笔记.html","title":"泛型笔记","keywords":"","body":"泛型笔记 可以使用泛型来创建可重用的组件，一个组件可以支持多种类型的数据 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/枚举笔记.html":{"url":"TypeScript/枚举笔记.html","title":"枚举笔记","keywords":"","body":"枚举笔记 数字枚举 数字枚举可以被混入到计算过的和常量成员。简短地说，不带初始化器的枚举或者被放在第一的位置，或者被放在使用了数字敞亮或其他长量初始化了的枚举后迷啊。换句话说，下面的情况是被允许的： enum E { A = getSomeValue(), B, // error! 'A' is not constant-initialized, so 'B' needs an initializer } By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/类型兼容性笔记.html":{"url":"TypeScript/类型兼容性笔记.html","title":"类型兼容性笔记","keywords":"","body":"类型兼容性笔记 TypeScript结构化类型系统的基本规则是，如果x要兼容y，那么y至少具有与x相同的属性。比如： interface Named { name: string; } let x: Named; // y's inferred type is { name: string; location: string; } let y = { name: 'Alice', location: 'Seattle' }; x = y; 在这个例子中，y必须包含名字是name的string类型成员。y满足条件，因此赋值正确。 比较两个函数 1） let x = (a: number) => 0; let y = (b: number, s: string) => 0; y = x; // OK x = y; // Error 要查看x是否能赋值给y，首先要看他们的参数列表。x的每个参数必须能在y例找到对应类型的参数。注意的是参数的名字相同与否无所谓，只看它们的类型。 这里，x的每个参数在y中都能找到对应的参数，所以允许赋值。 第二个赋值错误，因为y有个必需的第二个参数，但是x并没有，所以不允许赋值。 2）下面来看看如何处理返回值类型，创建两个仅是返回值类型不同的函数： let x = () => ({name: 'Alice'}); let y = () => ({name: 'Alice', location: 'Seattle'}); x = y; // OK y = x; // Error, because x() lacks a location property 类型系统强制源函数的返回值类型必须是目标函数返回值类型的子类型 枚举 枚举类型与数字类型兼容，并且数字类型与枚举类型兼容。不同枚举类型之间是不兼容的。比如， enum Status { Ready, Waiting }; enum Color { Red, Blue, Green }; let status = Status.Ready; status = Color.Green; // Error 类 类与对象字面量和接口差不多，担忧一点不同，类有静态部分和实例部分的类型。比较两个类类型的对象时，只有实例的成员会被比较。静态成员和构造函数不在比较的范围内 class Animal { feet: number; constructor(name: string, numFeet: number) { } } class Size { feet: number; constructor(numFeet: number) { } } let a: Animal; let s: Size; a = s; // OK s = a; // OK By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/高级类型笔记.html":{"url":"TypeScript/高级类型笔记.html","title":"高级类型笔记","keywords":"","body":"高级类型笔记 联合类型 如果一个值是联合类型，我们只能访问此联合类型的所有类型例共有的成员。 interface Bird { fly(); layEggs(); } interface Fish { swim(); layEggs();· } function getSmallPet(): Fish | Bird { // ... } let pet = getSmallPet(); pet.layEggs(); // okay pet.swim(); // errors 可以为null的类型 TypeScript具有两种特殊的类型， null和 undefined，它们分别具有值null和undefined. 我们在基础类型一节里已经做过简要说明。 默认情况下，类型检查器认为 null与 undefined可以赋值给任何类型。 null与 undefined是所有其它类型的一个有效值。 这也意味着，你阻止不了将它们赋值给其它类型，就算是你想要阻止这种情况也不行。 null的发明者，Tony Hoare，称它为 价值亿万美金的错误。 --strictNullChecks标记可以解决此错误：当你声明一个变量时，它不会自动地包含 null或 undefined。 你可以使用联合类型明确的包含它们： let s = \"foo\"; s = null; // 错误, 'null'不能赋值给'string' let sn: string | null = \"bar\"; sn = null; // 可以 sn = undefined; // error, 'undefined'不能赋值给'string | null' 可选参数和可选属性 使用了 --strictNullChecks，可选参数会被自动地加上| undefined： function f(x: number, y?: number) { return x + (y || 0); } f(1, 2); f(1); f(1, undefined); f(1, null); // error, 'null' is not assignable to 'number | undefined' 可选属性也会有同样的处理： class C { a: number; b?: number; } let c = new C(); c.a = 12; c.a = undefined; // error, 'undefined' is not assignable to 'number' c.b = 13; c.b = undefined; // ok c.b = null; // error, 'null' is not assignable to 'number | undefined' By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/.d.ts和源文件.ts有什么区别?.html":{"url":"TypeScript/.d.ts和源文件.ts有什么区别?.html","title":".d.ts和源文件.ts有什么区别?","keywords":"","body":".d.ts和源文件.ts有什么区别? 问题 .ts和.d.ts这两个文件有什么不一样？ 在网上查过，.d.ts是对自己写的JS文件进行类型定义的 那是不是可以认为想要在TS中使用JS文件里面的方法，就一定要写.d.ts这个定义呢？ 如果你是直接写.ts的，就不用写.d.ts这个文件呢？ 解答 1. 答 .d.ts就是TypeDefinition类型定义文件，用来定义类型信息以及接口规范 想要在ts使用js里面的方法，不一定要有类型定义 但如果没有类型定义文件，你在编码的过程中，编辑器不会给你方法的提示，你想找API的时候也没有对应的文档 如果想用早期的JS Library，是没有类型定义信息的，这个时候为了使用方便我们就需要创建一个对应的.d.ts文件 2. 答 .ts表示你的代码就是用ts写的 但这种代码最后会编译成.js的js代码供他人使用。这个时候，类型信息就丢失了。所以 ts 编译器会自动根据 .ts 中的信息，生成对外的 .d.ts 文件，和生成的 js 文件搭配使用。其中，js 文件是给运行引擎用的，而 .d.ts 文件是给 IDE（智能编辑器）写代码时参考用的。 另一种情况是，你的代码不是用 ts 写的，只是希望最后给别人用时能有类型信息作为辅助，那么这种情况下的 .d.ts 文件就需要你手写了 .ts文件生成.d.ts文件和.js文件 全局安装tsc命令，执行 tsc --declaration service.ts .js文件生成.d.ts文件 npm i dtsmake -g npm i tern --save-dev dtsmake -s filename.js By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "},"TypeScript/使用TypeScript装饰器装饰你的代码.html":{"url":"TypeScript/使用TypeScript装饰器装饰你的代码.html","title":"使用TypeScript装饰器装饰你的代码","keywords":"","body":"聊聊TypeScript中的设计模式--装饰器篇（decorators） 转载自 什么是装饰者模式 最近在看nestjs等支持TypeScript的node框架，经常看到这样一种写法 import { Controller, Get } from '@nestjs/common'; @Controller('cats') export class CatsController { @Get() findAll() { return 'This action returns all cats'; } } 上述代码定义了一个处理URL为“/cats”的控制器，该控制器对于URL为“/cats”的get方法执行findAll()函数，返回相应的字符串 在上述的代码中，用@controller('cats')修饰CatsController类，通过@get来修饰类中的findAll方法，这就是典型的装饰着模式。通过@controller('cats')和@get修饰后的类CatsController，简单来说，就是拥有了丰富的“内涵” 下面看看具体装饰者模式的定义： 我们知道继承模式是丰富子元素“内涵”的一种重要方式，不管是继承接口还是子类继承基类。而装饰者模式可以在不改变继承关系的前提下，包装现有的模块，使其内涵更加丰富，并不会影响到原有的功能。与继承相比，更加灵活。 JavaScript中的装饰器处于建议征集的第二阶段，通过babel和TypeScript都可以实现装饰器的语法 二、TypeScript中的装饰器 Typescript中的装饰器与类相关，分别可以装饰类的实例函数和静态函数、类本身、类的属性、类中函数的参数以及类的set/get存取器，下面来一一介绍： 1. 类方法的装饰器 下面来介绍一下用装饰器来修饰函数，首先看一个例子： let temple; function log(target, key, descriptor) { console.log(`${key} was called!`); temple = target; } class P { @log foo() { console.log('Do something'); } } const p = new P() console.log(P.prototype === temple) //true 上述是实例方法foo中我们用log函数修饰，log函数接受三个参数，通过P.prototype === temple(target)可以判断，在类的实例函数的装饰器函数第一个参数为类的原型，第二个参数为函数名本身，第三个参数为该函数的描述属性 具体总结如下： target: 如果装饰的是类的实例函数，那么target就是类的原型。如果修饰的是类的静态函数，那么target就是类本身 key: 该函数的函数名 descriptor：该函数的描述属性，比如configurable、value、enumerable等 从上述的例子中我们可以看到，用来装饰器来修饰相应的类的函数十分方便 @log foo() { } 2. 类的装饰器 装饰函数也可以直接修饰类 let temple function foo(target) { console.log(target) temple = target } @foo class P { constructor() { } } const p = new P(); temple === P // true 当装饰函数直接修饰类的时候，装饰函数接受唯一的参数，这个参数就是被修饰累本身。上述的例子中，输出的target就是类P的本身。 此外，在修饰类的时候，如果装饰函数有返回值，该返回值会重新定义这个类，也就是会所当装饰函数 有返回值时，其实是生成了一个新类，该新类会通过返回值来定义。 举例来说： function foo(target) { return class extends target { name = 'Jony'; sayHello() { console.log('Hello ' + this.name) } } } @foo class P { constructor() {} } const p = new P() p.sayHello(); // 会输出Hello Jony 上面的例子可以看到，当装饰函数foo有返回值时，实际上P类已经被返回值所代表的新类所代替，因此P的实例p拥有sayHello方法 3. 类的属性的装饰器 下面我们来看类的属性的装饰器，装饰函数修饰类的属性时，在类实例化的时候调用属性的装饰函数，举例来说： function foo(target, name) { console.log(\"target is\", target); console.log(\"name is\", name); } class P { @foo name = 'Jony' } const p = new P (); // 会依次输出 target is f P() name is Jony 这里对于类的属性的装饰器函数接受两个参数，对于静态属性而言，第一个参数是类本身，对于实例属性而言，第一个参数是类的原型，第二个参数是指属性的值 4. 类函数参数的装饰器 接着来看类函数参数的装饰器，累函数的参数装饰器可以修饰类的构建函数中的参数，以及类中其它普通函数中的参数。该装饰器在类的方法被调用的时候执行，下面来看实例： function foo(target, key, index) { console.log(\"target is\", target); console.log(\"key is \", key); console.log(\"index is\", index) } class P { test(@foo a) { } } const p = new P (); p.test (\"Hello Jony\") // 依次输出 P()，test， 0 累函数参数的装饰器函数接受三个参数，依次为类本身，类中该修饰的函数本身，以及被修饰的参数在参数列表中的索引值。 targe： 类本身 key：该参数所在的函数的函数名 index：该参数在函数参数列表中的索引值 从上面的Typescript中在基类汇总常用的装饰器后，我们发现 装饰器可以起到分离复杂逻辑的功能，且使用上极其简单方便。与继承相比，也更加灵活，可以从装饰类，到装饰类函数的参数 三、Typescript中的注解 在了解了Typescript 中的装饰器之后，接着我们呢来看TypeSCript中的注解 什么时注解，所谓注解的定义就是： 为相应的类附加元数据支持 所谓元数据可以简单的解释，就是修饰数据的数据，比如一个人有name、age等数据属性，那么name和age这些字段就是为了修饰数据的数据，可以简单的成为元数据。 元数据简单来说就是可以修饰某些数据的字段，下面给出装饰器和注解的解释和区别： 装饰器：定义劫持，可以对类、类的方法，类的属性以及类的方法的入参进行修改，不提供元数据的支持 注解：仅提供给元数据的支持 两者之间的联系 通过注解添加元数据，然后子啊装饰器中获取这些元数据，完成对类、类的方法等等的修改，可以在装饰器中添加元数据的支持，比如可以在装饰器工厂函数以及装饰器函数中添加元数据支持等 1. Typescript中的元数据操作 可以通过reflect-metadata包来实现对于元数据的操作。首先我们来看reflect-metadata的使用，首先定义使用元数据的函数： const formatMetadataKey = Symbol(\"format\") function format(formatString: string) { return reflect.metadata(formatMetadataKey, formatString) } function getFormat(target: any, propertyKey: stirng) { return Reflect.getMetadata(formatMetadataKey, target, propertyKey); } 这里的format可以作为装饰器函数的工厂函数，因为format函数返回的是一个装饰器函数，上述的方法定义了元数据Sysmbol(\"format\"),用Sysmbol的原因是为了防止元数据中的字段重复，而format定义了取元数据中相应字段的功能。 接着我们来在类中使用相应的元数据： class Greeter { @format(\"Hello, %s\") name: string; constructor(name: string) { this.name = message; } sayHello() { let formatString = getFormat(this, \"name\"); return formatString.replace(\"%s\", this.name); } } const g = new Greeter(\"Jony\"); console.log(g.sayHello()); 在上述中，我们在name属性的装饰器工厂函数，执行@Format(\"Hello, %s\")，返回一个装饰器函数，且该装饰器函数修饰了Greeter类的name属性，将“name”属性的值写入为\"Hello, %s\"。 然后再sayHello方法中，通过getFormat(this,\"name\")取到formatString为“Hello,%s”. 四、总结 通过装饰器，可以方便的修饰类，以及类的方法，类的属性等，相比于继承而言更加灵活，此外，通过注解的方法，可以在Typescript中引入元数据，实现元编程等。特别是在angularjs、nestjs中，大量使用了注解，特别是nestjs构建了类似于java springMVC式的web框架。 By guoyanhong，使用知识共享 署名-相同方式共享 4.0协议发布            此页面修订于： 2021-05-14 06:31:05 "}}